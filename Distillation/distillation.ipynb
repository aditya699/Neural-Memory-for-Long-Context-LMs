{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514a6c99-630f-4ddd-9393-9152c894f542",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Library Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd973176-6bb1-4647-a42f-2e28dd06b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets matplotlib accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610ec2e-56c1-4c3f-95c4-1f9582c013e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6ee3c4-1b74-4486-b5be-ccc057009393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA RTX 4000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc426f-1cf2-4386-af7f-72710b6e265e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load the Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea731ee-c246-4109-824a-c201f3f99d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading teacher model (Qwen2.5-0.5B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21939e27979844a1910fe44085345b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125e1b1317274ff9bc6bf8ef54eec026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model loaded!\n",
      "Number of parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading teacher model (Qwen2.5-0.5B)...\")\n",
    "\n",
    "# Load the tokenizer (converts text to numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "# Load the pretrained teacher model\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"cuda\"            # Put it on your GPU\n",
    ")\n",
    "\n",
    "# Set to evaluation mode (no training, just inference)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(f\"Teacher model loaded!\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in teacher_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8210770-88c8-4134-8f98-02397134c788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Eval on Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45379e94-f299-4626-a2e0-ad1403a2e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing teacher model...\n",
      "\n",
      "Input: The capital of France is\n",
      "Generated: The capital of France is Paris. It is the largest city in Europe and\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test Teacher Model\n",
    "print(\"Testing teacher model...\")\n",
    "\n",
    "# Input text\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize (convert text to numbers)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate next tokens\n",
    "with torch.no_grad():  # Don't compute gradients (saves memory)\n",
    "    outputs = teacher_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,      # Generate 10 new tokens\n",
    "        do_sample=False,        # Greedy decoding (pick highest probability)\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode back to text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nInput: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f16979a-e647-441c-976a-cd199da9360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at teacher's probability distribution...\n",
      "\n",
      "Prompt: 'The cat sat on the'\n",
      "\n",
      "Top 10 predictions for next token:\n",
      "--------------------------------------------------\n",
      "1. ' mat' → 84.33%\n",
      "2. ' cat' → 2.35%\n",
      "3. ' fence' → 1.11%\n",
      "4. '\n",
      "' → 0.95%\n",
      "5. ' window' → 0.47%\n",
      "6. ' green' → 0.45%\n",
      "7. ' ' → 0.44%\n",
      "8. ' moon' → 0.42%\n",
      "9. ' l' → 0.32%\n",
      "10. ' ____' → 0.26%\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Look at Teacher's Soft Targets\n",
    "print(\"Looking at teacher's probability distribution...\\n\")\n",
    "\n",
    "# Simple prompt\n",
    "prompt = \"The cat sat on the\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Get model's output (logits = raw scores before softmax)\n",
    "with torch.no_grad():\n",
    "    outputs = teacher_model(**inputs)\n",
    "    logits = outputs.logits  # Shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "# Get the logits for predicting the NEXT token after our prompt\n",
    "next_token_logits = logits[0, -1, :]  # Last position's predictions\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get top 10 most likely next tokens\n",
    "top_probs, top_indices = torch.topk(probs, k=10)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nTop 10 predictions for next token:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1}. '{token}' → {prob.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633eda1e-4ea3-4ac3-992a-4a406bce7338",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setting Up the Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9889330c-a1a6-4aab-9c8d-febb176cdefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating student model configuration...\n",
      "\n",
      "Teacher configuration:\n",
      "  - Hidden size: 896\n",
      "  - Number of layers: 24\n",
      "  - Attention heads: 14\n",
      "  - Vocab size: 151936\n",
      "  - Total params: 494M\n",
      "\n",
      "Student configuration:\n",
      "  - Hidden size: 256\n",
      "  - Number of layers: 6\n",
      "  - Attention heads: 8\n",
      "  - Vocab size: 151936\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Student Model Configuration\n",
    "from transformers import AutoConfig\n",
    "\n",
    "print(\"Creating student model configuration...\\n\")\n",
    "\n",
    "# Load teacher's config\n",
    "teacher_config = teacher_model.config\n",
    "\n",
    "print(\"Teacher configuration:\")\n",
    "print(f\"  - Hidden size: {teacher_config.hidden_size}\")\n",
    "print(f\"  - Number of layers: {teacher_config.num_hidden_layers}\")\n",
    "print(f\"  - Attention heads: {teacher_config.num_attention_heads}\")\n",
    "print(f\"  - Vocab size: {teacher_config.vocab_size}\")\n",
    "print(f\"  - Total params: 494M\")\n",
    "\n",
    "# Create student config (SMALLER)\n",
    "student_config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "student_config.hidden_size = 256           # Teacher: 896, Student: 256\n",
    "student_config.num_hidden_layers = 6       # Teacher: 24, Student: 6  \n",
    "student_config.num_attention_heads = 8     # Teacher: 14, Student: 8\n",
    "student_config.intermediate_size = 1024    # Teacher: 4864, Student: 1024\n",
    "\n",
    "print(\"\\nStudent configuration:\")\n",
    "print(f\"  - Hidden size: {student_config.hidden_size}\")\n",
    "print(f\"  - Number of layers: {student_config.num_hidden_layers}\")\n",
    "print(f\"  - Attention heads: {student_config.num_attention_heads}\")\n",
    "print(f\"  - Vocab size: {student_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87157526-eb79-4662-9df0-ec771049f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating student model from config...\n",
      "\n",
      "Teacher parameters: 494,032,768\n",
      "Student parameters: 44,602,880\n",
      "Reduction: 11.1x smaller\n",
      "\n",
      "Student model created with RANDOM weights (untrained)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Student Model\n",
    "print(\"Creating student model from config...\\n\")\n",
    "\n",
    "# Create model with our custom config\n",
    "student_model = AutoModelForCausalLM.from_config(student_config)\n",
    "\n",
    "# Move to GPU\n",
    "student_model = student_model.to(\"cuda\")\n",
    "\n",
    "# Count parameters\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,}\")\n",
    "print(f\"Reduction: {teacher_params / student_params:.1f}x smaller\")\n",
    "print(f\"\\nStudent model created with RANDOM weights (untrained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9e7c0-26b9-4a6a-9457-bdd2da1b95ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test the Student Model Note:This is an model which hasn't leanred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9b99df-6ec3-41c5-90a7-036598634227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing UNTRAINED student (random weights)...\n",
      "\n",
      "Prompt: 'The capital of France is'\n",
      "\n",
      "Student's top 10 predictions (UNTRAINED):\n",
      "--------------------------------------------------\n",
      "1. '的基础上' → 0.00%\n",
      "2. ' CUT' → 0.00%\n",
      "3. 'act' → 0.00%\n",
      "4. 'Compra' → 0.00%\n",
      "5. '市中心' → 0.00%\n",
      "6. ' PowerPoint' → 0.00%\n",
      "7. ' pdb' → 0.00%\n",
      "8. ' abort' → 0.00%\n",
      "9. ' specification' → 0.00%\n",
      "10. ' kenn' → 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Test Untrained Student\n",
    "print(\"Testing UNTRAINED student (random weights)...\\n\")\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Get student's predictions (with random weights!)\n",
    "with torch.no_grad():\n",
    "    student_outputs = student_model(**inputs)\n",
    "    student_logits = student_outputs.logits[0, -1, :]\n",
    "    \n",
    "# Convert to probabilities\n",
    "student_probs = F.softmax(student_logits, dim=-1)\n",
    "\n",
    "# Get top 10 predictions\n",
    "top_probs, top_indices = torch.topk(student_probs, k=10)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nStudent's top 10 predictions (UNTRAINED):\")\n",
    "print(\"-\" * 50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1}. '{token}' → {prob.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acf7bf-487e-4e63-a0bd-4c7fa69debb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Trainning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5242f7-86cb-42c5-b2cb-b1f59f3e8bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataset - removing empty/short examples...\n",
      "\n",
      "Original dataset size: 36718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1200021cd6154f84a4ea4e27f6fddd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 16018 examples\n",
      "Removed: 20700 empty/bad examples\n",
      "Using: 5000 examples for training\n",
      "\n",
      "Example good text:\n",
      "------------------------------------------------------------\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ p\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Clean the Dataset\n",
    "print(\"Cleaning dataset - removing empty/short examples...\\n\")\n",
    "\n",
    "# Reload fresh dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Filter function - keep only good examples\n",
    "def is_good_example(example):\n",
    "    text = example['text'].strip()\n",
    "    # Keep if: has real content AND is at least 50 characters\n",
    "    return len(text) > 50 and not text.startswith('=')  # Remove headers\n",
    "\n",
    "# Filter dataset\n",
    "dataset = dataset.filter(is_good_example)\n",
    "\n",
    "print(f\"After filtering: {len(dataset)} examples\")\n",
    "print(f\"Removed: {36718 - len(dataset)} empty/bad examples\")\n",
    "\n",
    "# Take more examples now (since we have good data)\n",
    "dataset = dataset.select(range(min(5000, len(dataset))))\n",
    "\n",
    "print(f\"Using: {len(dataset)} examples for training\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample good text:\")\n",
    "print(\"-\" * 60)\n",
    "print(dataset[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102b6d0-9e96-4277-993b-9bdfcf3edba3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b654a5-ab66-4952-8e07-8858fb9e805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation loss function defined!\n",
      "Temperature parameter: Makes distributions 'softer'\n",
      "Higher temp (e.g., 4.0) → more uniform probabilities\n",
      "Lower temp (e.g., 1.0) → sharper probabilities\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Define Distillation Loss Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    \"\"\"\n",
    "    Compute KL divergence loss between student and teacher\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Raw scores from student [batch, seq_len, vocab_size]\n",
    "        teacher_logits: Raw scores from teacher [batch, seq_len, vocab_size]\n",
    "        temperature: Softmax temperature (higher = softer distributions)\n",
    "    \n",
    "    Returns:\n",
    "        loss: KL divergence loss (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply temperature scaling and softmax to get soft targets\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence: sum(teacher_probs * log(teacher_probs / student_probs))\n",
    "    # Equivalent to: sum(teacher_probs * (log(teacher_probs) - log(student_probs)))\n",
    "    kl_div = F.kl_div(\n",
    "        student_log_probs, \n",
    "        teacher_probs, \n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Scale by temperature^2 (standard practice in distillation)\n",
    "    kl_div = kl_div * (temperature ** 2)\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "# Test the function\n",
    "print(\"Distillation loss function defined!\")\n",
    "print(f\"Temperature parameter: Makes distributions 'softer'\")\n",
    "print(f\"Higher temp (e.g., 4.0) → more uniform probabilities\")\n",
    "print(f\"Lower temp (e.g., 1.0) → sharper probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2ce1d-2124-4f42-8fc5-cfad80bde751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bc2354f-4881-4391-9d6e-a2f9f34a4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a89f884d3884bd5bdaf2a5a8463ab53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset tokenized!\n",
      "  Total examples: 5000\n",
      "  Max sequence length: 128 tokens\n",
      "\n",
      "Example tokenized:\n",
      "  Input IDs shape: torch.Size([128])\n",
      "  First 10 token IDs: [5363, 73, 55661, 902, 85162, 88, 4204, 220, 18, 549]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Prepare Training Data\n",
    "print(\"Preparing training data...\\n\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,           # Cut off if too long\n",
    "        max_length=128,            # Maximum 128 tokens per example\n",
    "        padding='max_length',      # Pad shorter sequences\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text, keep only token IDs\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print(f\"✓ Dataset tokenized!\")\n",
    "print(f\"  Total examples: {len(tokenized_dataset)}\")\n",
    "print(f\"  Max sequence length: 128 tokens\")\n",
    "print(f\"\\nExample tokenized:\")\n",
    "print(f\"  Input IDs shape: {tokenized_dataset[0]['input_ids'].shape}\")\n",
    "print(f\"  First 10 token IDs: {tokenized_dataset[0]['input_ids'][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17908b9d-7c36-4793-8ba2-5d641c917e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking tokenized examples...\n",
      "\n",
      "Example 0:\n",
      "  Non-padding tokens: 128\n",
      "  First 20 token IDs: [5363, 73, 55661, 902, 85162, 88, 4204, 220, 18, 549, 1230, 8548, 291, 65316, 320, 10769, 549, 49434, 99, 74167]\n",
      "  Decoded text:  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chron\n"
     ]
    }
   ],
   "source": [
    "# Cell 10.5: Check tokenized data\n",
    "print(\"Checking tokenized examples...\\n\")\n",
    "\n",
    "# Look at a few examples to find real text\n",
    "for i in range(10):\n",
    "    input_ids = tokenized_dataset[i]['input_ids']\n",
    "    # Count non-padding tokens\n",
    "    non_padding = (input_ids != 151643).sum().item()\n",
    "    \n",
    "    if non_padding > 20:  # Find one with at least 20 real tokens\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  Non-padding tokens: {non_padding}\")\n",
    "        print(f\"  First 20 token IDs: {input_ids[:20].tolist()}\")\n",
    "        \n",
    "        # Decode to see the actual text\n",
    "        decoded = tokenizer.decode(input_ids[:50], skip_special_tokens=True)\n",
    "        print(f\"  Decoded text: {decoded[:150]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c4153-76dd-41e7-9c56-dc2670867ef1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Creating the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2dfbcc-b81b-4eff-a948-c2ba7ac58850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoader...\n",
      "\n",
      "✓ DataLoader created!\n",
      "  Batch size: 8\n",
      "  Total batches: 625\n",
      "  Total examples: 5000\n",
      "\n",
      "Testing dataloader...\n",
      "  Batch input_ids shape: torch.Size([8, 128])\n",
      "  Batch attention_mask shape: torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Creating DataLoader...\\n\")\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=8,        # Process 8 examples at once\n",
    "    shuffle=True,        # Randomize order each epoch\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoader created!\")\n",
    "print(f\"  Batch size: 8\")\n",
    "print(f\"  Total batches: {len(train_dataloader)}\")\n",
    "print(f\"  Total examples: {len(tokenized_dataset)}\")\n",
    "\n",
    "# Test the dataloader\n",
    "print(f\"\\nTesting dataloader...\")\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"  Batch input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"  Batch attention_mask shape: {batch['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87218bc-5c70-46e2-9154-5104cd7795e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train the smaller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52ccd4aa-d685-4e46-871e-3e83e53e24b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training...\n",
      "\n",
      "Training configuration:\n",
      "  Epochs: 3\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-5\n",
      "  Temperature: 2.0\n",
      "  Total training steps: 1875\n",
      "\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.44it/s, loss=772.7190] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 859.0203\n",
      "\n",
      "Epoch 2/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.43it/s, loss=751.7917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 791.3633\n",
      "\n",
      "Epoch 3/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:55<00:00, 11.33it/s, loss=785.4382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 773.9486\n",
      "\n",
      "============================================================\n",
      "✓ Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Training Loop\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Setting up training...\\n\")\n",
    "\n",
    "# Optimizer (updates student weights)\n",
    "optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 3\n",
    "temperature = 2.0\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Batch size: 8\")\n",
    "print(f\"  Learning rate: 5e-5\")\n",
    "print(f\"  Temperature: {temperature}\")\n",
    "print(f\"  Total training steps: {len(train_dataloader) * num_epochs}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Put student in training mode\n",
    "student_model.train()\n",
    "\n",
    "# Track losses\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        \n",
    "        # Get teacher predictions (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Get student predictions (we WILL compute gradients)\n",
    "        student_outputs = student_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        optimizer.step()       # Update weights\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd975db-c6ad-4c94-9eb5-e8f100ce62b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test the student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f71e5c2d-73a8-41a4-8a65-a041e57451b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TRAINED student vs UNTRAINED...\n",
      "\n",
      "Prompt: 'The capital of France is'\n",
      "\n",
      "============================================================\n",
      "TEACHER predictions:\n",
      "------------------------------------------------------------\n",
      "1. ' Paris' → 31.57%\n",
      "2. ' ______' → 11.43%\n",
      "3. ' ____' → 6.41%\n",
      "4. ' __' → 5.48%\n",
      "5. ':\n",
      "' → 5.15%\n",
      "6. ' located' → 3.57%\n",
      "7. ' the' → 2.80%\n",
      "8. '\n",
      "' → 2.47%\n",
      "9. ' (' → 1.99%\n",
      "10. ' .\n",
      "' → 1.90%\n",
      "\n",
      "============================================================\n",
      "TRAINED STUDENT predictions:\n",
      "------------------------------------------------------------\n",
      "1. ' .\n",
      "' → 0.09%\n",
      "2. ' .\n",
      "\n",
      "' → 0.08%\n",
      "3. '.' → 0.08%\n",
      "4. ' [' → 0.07%\n",
      "5. ' B' → 0.07%\n",
      "6. ' in' → 0.07%\n",
      "7. ' .' → 0.06%\n",
      "8. ' and' → 0.06%\n",
      "9. '9' → 0.06%\n",
      "10. '\n",
      "\n",
      "' → 0.06%\n",
      "\n",
      "============================================================\n",
      "REMEMBER - Untrained student predicted:\n",
      "'的基础上', 'CUT', 'PowerPoint', etc. (nonsense!)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Test Trained Student\n",
    "print(\"Testing TRAINED student vs UNTRAINED...\\n\")\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Set student to eval mode\n",
    "student_model.eval()\n",
    "\n",
    "# Get trained student's predictions\n",
    "with torch.no_grad():\n",
    "    student_outputs = student_model(**inputs)\n",
    "    student_logits = student_outputs.logits[0, -1, :]\n",
    "    student_probs = F.softmax(student_logits, dim=-1)\n",
    "\n",
    "# Get teacher's predictions (for comparison)\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model(**inputs)\n",
    "    teacher_logits = teacher_outputs.logits[0, -1, :]\n",
    "    teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "\n",
    "# Get top 10 for both\n",
    "student_top_probs, student_top_indices = torch.topk(student_probs, k=10)\n",
    "teacher_top_probs, teacher_top_indices = torch.topk(teacher_probs, k=10)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEACHER predictions:\")\n",
    "print(\"-\"*60)\n",
    "for i, (prob, idx) in enumerate(zip(teacher_top_probs, teacher_top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1}. '{token}' → {prob.item()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINED STUDENT predictions:\")\n",
    "print(\"-\"*60)\n",
    "for i, (prob, idx) in enumerate(zip(student_top_probs, student_top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1}. '{token}' → {prob.item()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REMEMBER - Untrained student predicted:\")\n",
    "print(\"'的基础上', 'CUT', 'PowerPoint', etc. (nonsense!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023cde93-e4f2-4736-b8a4-3ab1b641568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking student's actual learning...\n",
      "\n",
      "Student generates: The capital of France isHumanHumanHumanHumanHumanHumanHumanHumanHumanHuman\n",
      "Teacher generates: The capital of France is Paris. It is the largest city in Europe and\n"
     ]
    }
   ],
   "source": [
    "# Cell 13.5: Check if student is actually updating\n",
    "print(\"Checking student's actual learning...\\n\")\n",
    "\n",
    "# Generate text to see behavior\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "student_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = student_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Student generates: {generated}\")\n",
    "\n",
    "# Also check teacher for comparison\n",
    "with torch.no_grad():\n",
    "    teacher_gen = teacher_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "teacher_text = tokenizer.decode(teacher_gen[0], skip_special_tokens=True)\n",
    "print(f\"Teacher generates: {teacher_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca4051-38db-4e27-84e8-29f18797df86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Better Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c91ab0e-588d-48aa-80d8-5c34937f6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting with fresh student model and better config...\n",
      "\n",
      "Creating NEW student model...\n",
      "✓ Fresh student created with random weights\n",
      "\n",
      "New training configuration:\n",
      "  Epochs: 10 (was 3)\n",
      "  Learning rate: 1e-4 (was 5e-5)\n",
      "  Temperature: 3.0 (was 2.0)\n",
      "  Weight decay: 0.01 (regularization)\n",
      "  Total steps: 6250\n",
      "\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.45it/s, loss=435.6461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 514.9640\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.49it/s, loss=381.9105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 391.7198\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.48it/s, loss=374.4852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 374.8575\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.38it/s, loss=385.5041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 365.0590\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:55<00:00, 11.34it/s, loss=350.7498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 358.5952\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.43it/s, loss=414.4724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 354.7307\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.42it/s, loss=357.2596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 351.9713\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.43it/s, loss=320.4314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg loss: 349.9068\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.44it/s, loss=324.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg loss: 348.2993\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [00:54<00:00, 11.44it/s, loss=400.7427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg loss: 347.0600\n",
      "\n",
      "============================================================\n",
      "✓ Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: RESTART Training Properly\n",
    "print(\"Restarting with fresh student model and better config...\\n\")\n",
    "\n",
    "# 1. CREATE FRESH STUDENT (important!)\n",
    "print(\"Creating NEW student model...\")\n",
    "student_config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "student_config.hidden_size = 256\n",
    "student_config.num_hidden_layers = 6\n",
    "student_config.num_attention_heads = 8\n",
    "student_config.intermediate_size = 1024\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_config(student_config)\n",
    "student_model = student_model.to(\"cuda\")\n",
    "print(\"✓ Fresh student created with random weights\")\n",
    "\n",
    "# 2. BETTER OPTIMIZER CONFIG\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# 3. MORE EPOCHS\n",
    "num_epochs = 10  # Increased from 3 to 10!\n",
    "temperature = 3.0  # Increased temperature for softer targets\n",
    "\n",
    "print(f\"\\nNew training configuration:\")\n",
    "print(f\"  Epochs: {num_epochs} (was 3)\")\n",
    "print(f\"  Learning rate: 1e-4 (was 5e-5)\")\n",
    "print(f\"  Temperature: {temperature} (was 2.0)\")\n",
    "print(f\"  Weight decay: 0.01 (regularization)\")\n",
    "print(f\"  Total steps: {len(train_dataloader) * num_epochs}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 4. TRAIN\n",
    "student_model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        \n",
    "        # Teacher predictions (frozen)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Student predictions\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # Loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b54b8bad-e172-4d43-bb3e-b58b936ddc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the PROPERLY trained student...\n",
      "\n",
      "Prompt: 'The capital of France is'\n",
      "\n",
      "======================================================================\n",
      "TEACHER predictions:\n",
      "----------------------------------------------------------------------\n",
      " 1. ' Paris         ' →  31.57%\n",
      " 2. ' ______        ' →  11.43%\n",
      " 3. ' ____          ' →   6.41%\n",
      " 4. ' __            ' →   5.48%\n",
      " 5. ':\n",
      "             ' →   5.15%\n",
      " 6. ' located       ' →   3.57%\n",
      " 7. ' the           ' →   2.80%\n",
      " 8. '\n",
      "              ' →   2.47%\n",
      " 9. ' (             ' →   1.99%\n",
      "10. ' .\n",
      "            ' →   1.90%\n",
      "\n",
      "======================================================================\n",
      "TRAINED STUDENT predictions (10 epochs, proper config):\n",
      "----------------------------------------------------------------------\n",
      " 1. ',              ' →   0.57%\n",
      " 2. ' .\n",
      "            ' →   0.35% ✓ (matches teacher!)\n",
      " 3. ' -             ' →   0.28%\n",
      " 4. ' .\n",
      "\n",
      "           ' →   0.25%\n",
      " 5. ' that          ' →   0.20%\n",
      " 6. ' [             ' →   0.19%\n",
      " 7. ' as            ' →   0.19%\n",
      " 8. ' in            ' →   0.19%\n",
      " 9. ' more          ' →   0.19%\n",
      "10. ' only          ' →   0.18%\n",
      "\n",
      "======================================================================\n",
      "Let's also try generation:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Teacher: The capital of France is Paris. It is the largest city in Europe and the second largest in the\n",
      "Student: The capital of France is,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Test the PROPERLY Trained Student\n",
    "print(\"Testing the PROPERLY trained student...\\n\")\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Set to eval mode\n",
    "student_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    # Teacher\n",
    "    teacher_outputs = teacher_model(**inputs)\n",
    "    teacher_logits = teacher_outputs.logits[0, -1, :]\n",
    "    teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "    \n",
    "    # Student\n",
    "    student_outputs = student_model(**inputs)\n",
    "    student_logits = student_outputs.logits[0, -1, :]\n",
    "    student_probs = F.softmax(student_logits, dim=-1)\n",
    "\n",
    "# Top 10 for both\n",
    "teacher_top_probs, teacher_top_indices = torch.topk(teacher_probs, k=10)\n",
    "student_top_probs, student_top_indices = torch.topk(student_probs, k=10)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEACHER predictions:\")\n",
    "print(\"-\"*70)\n",
    "for i, (prob, idx) in enumerate(zip(teacher_top_probs, teacher_top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1:2d}. '{token:15s}' → {prob.item()*100:6.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINED STUDENT predictions (10 epochs, proper config):\")\n",
    "print(\"-\"*70)\n",
    "for i, (prob, idx) in enumerate(zip(student_top_probs, student_top_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    is_match = idx in teacher_top_indices\n",
    "    marker = \" ✓ (matches teacher!)\" if is_match else \"\"\n",
    "    print(f\"{i+1:2d}. '{token:15s}' → {prob.item()*100:6.2f}%{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Let's also try generation:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    student_gen = student_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=15,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    teacher_gen = teacher_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=15,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "student_text = tokenizer.decode(student_gen[0], skip_special_tokens=True)\n",
    "teacher_text = tokenizer.decode(teacher_gen[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nTeacher: {teacher_text}\")\n",
    "print(f\"Student: {student_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad241a-e12f-4960-962e-be4595286c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
