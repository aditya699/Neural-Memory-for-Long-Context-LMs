{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e4709f",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d813fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA RTX 4000 Ada Generation\n",
      "GPU memory: 21.01805056 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"No GPU found - will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d88b1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c9bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e9090",
   "metadata": {},
   "source": [
    "# Raw Implementation of Multi Head Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f729a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "Multi-head attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f5042",
   "metadata": {},
   "source": [
    "# Raw Implementation for GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1496a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "✅ Grouped Query Attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Grouped Query Attention (GQA) class.\n",
    "    \"\"\" \n",
    "    def __init__(self, d_model, num_heads, num_kv_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads  # In GQA, we have num_kv_heads for K and V, and num_heads for Q. K and V heads are shared across Q head groups, so their initialization is different\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.group_size = num_heads // num_kv_heads  # How many Q heads will share one K, V head\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model, the weight initialization follows Xavier/Kaiming initialization\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  # This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim) for Q and (batch_size, seq_len, num_kv_heads, head_dim) for K and V\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # GQA: Repeat K and V heads to match Q's number of heads\n",
    "        # repeat_interleave is used to repeat the elements of the tensor along a given dimension\n",
    "        # Each K, V head is shared by group_size Q heads\n",
    "        K = K.repeat_interleave(self.group_size, dim=1)  # (batch, num_heads, seq_len, head_dim)\n",
    "        V = V.repeat_interleave(self.group_size, dim=1)  # (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create GQA model\n",
    "    # 8 Q heads, 2 K/V heads → 4 Q heads share each K/V head\n",
    "    model = GroupedQueryAttention(d_model=512, num_heads=8, num_kv_heads=2, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"✅ Grouped Query Attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349b92b",
   "metadata": {},
   "source": [
    "# MLA from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed86718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:         torch.Size([32, 10, 512])\n",
      "Output shape:        torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):  # Inheritance\n",
    "    def __init__(self, d_model, num_heads, d_latent, dropout=0.1):  # d_model is the dimension of the model, d_latent is the compression vector\n",
    "        \"\"\"\n",
    "        Implementing Multi-Head Latent Attention from scratch\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class constructor\n",
    "        assert d_model % num_heads == 0  # Assert is used to check if the condition is true\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_latent = d_latent  # Added the compression dimension\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Compress: Initialize the weight matrix for shared compression\n",
    "        self.kv_compress = nn.Linear(d_model, d_latent)\n",
    "        \n",
    "        # Expand latent to K and V for all heads\n",
    "        self.k_proj = nn.Linear(d_latent, d_model)\n",
    "        self.v_proj = nn.Linear(d_latent, d_model)\n",
    "\n",
    "        # NOTE: Q remains the same, there is no compression in Q\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection (This is after attention we need to concatenate all heads)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Create a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack the input\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Project Q (NOTE: Projection just means x@W+b)\n",
    "        Q = self.q_proj(x)\n",
    "\n",
    "        # COMPRESS: Compress input to latent space (THE KEY STEP!)\n",
    "        # Here you compress the input into a smaller latent vector\n",
    "        kv_latent = self.kv_compress(x)\n",
    "\n",
    "        # Now we need to expand latent into K and V\n",
    "        K = self.k_proj(kv_latent)\n",
    "        V = self.v_proj(kv_latent)\n",
    "        \n",
    "        # Here we are reshaping so that each head can compute attention independently and in parallel\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to put heads first\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert scores into attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Multiply with V\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Transpose back\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "       \n",
    "        # Concat all heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Final Output\n",
    "        output = self.out_proj(output)\n",
    "         \n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    # d_model=512, d_latent=128 means we compress from 512 → 128 → 512\n",
    "    model = MultiHeadLatentAttention(d_model=512, num_heads=8, d_latent=128, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:         {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape:        {output.shape}\")   # [32, 10, 512]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a87040",
   "metadata": {},
   "source": [
    "# BenchMarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e73cc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f30bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up Config for Benchmarking\n",
    "# Benchmark settings\n",
    "batch_size = 32\n",
    "seq_len = 512  # Longer sequence to see real differences\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_kv_heads = 2  # For GQA\n",
    "d_latent = 128    # For MLA\n",
    "dropout = 0.0     # Turn off for fair benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c7752ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512, 512])\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create input tensor on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1075ab7",
   "metadata": {},
   "source": [
    "# Main Function that would measure memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b600e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(model, x, device):\n",
    "    \"\"\"Measures GPU memory used by a model\"\"\"\n",
    "    # Ensure input is on correct device\n",
    "    x = x.to(device)\n",
    "    \n",
    "    torch.cuda.empty_cache() #This will clear empty cache\n",
    "    #PyTorch tracks the maximum memory used since the last reset. This resets that counter to zero, so we measure ONLY this model's memory.\n",
    "    torch.cuda.reset_peak_memory_stats(device) \n",
    "    \n",
    "    # Run forward pass\n",
    "    _ = model(x)\n",
    "    \n",
    "    # Get peak memory in MB\n",
    "    memory_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "    \n",
    "    return memory_allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f3b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(model, x, num_iterations=100):\n",
    "    \"\"\"Measures average forward pass time\"\"\"\n",
    "    # Ensure input is on correct device\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.to(model.parameters().__next__().device)\n",
    "    \n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Warmup runs (GPU needs to \"warm up\")\n",
    "    for _ in range(10):\n",
    "        _ = model(x)\n",
    "    \n",
    "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    \n",
    "    # Actual timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        _ = model(x)\n",
    "    torch.cuda.synchronize()  # Wait again\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_iterations\n",
    "    return avg_time * 1000  # Convert to milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dba6985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Counts trainable parameters in the model\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params / 1e6  # Return in millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65408927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models on cuda\n",
      "MHA first layer device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Recreate models with explicit GPU placement\n",
    "mha_model = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "mha_model = mha_model.to(device)\n",
    "\n",
    "gqa_model = GroupedQueryAttention(d_model, num_heads, num_kv_heads, dropout)\n",
    "gqa_model = gqa_model.to(device)\n",
    "\n",
    "mla_model = MultiHeadLatentAttention(d_model, num_heads, d_latent, dropout)\n",
    "mla_model = mla_model.to(device)\n",
    "\n",
    "print(f\"✅ All models on {device}\")\n",
    "print(f\"MHA first layer device: {next(mha_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d8ce3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Benchmarks...\n",
      "\n",
      "Benchmarking Multi-Head Attention...\n",
      "Benchmarking Grouped Query Attention...\n",
      "Benchmarking Multi-Head Latent Attention...\n",
      "Benchmarks complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Benchmarks...\\n\")\n",
    "\n",
    "# Benchmark MHA\n",
    "print(\"Benchmarking Multi-Head Attention...\")\n",
    "mha_memory = measure_memory(mha_model, x, device)\n",
    "mha_speed = measure_speed(mha_model, x)\n",
    "mha_params = count_parameters(mha_model)\n",
    "\n",
    "# Benchmark GQA\n",
    "print(\"Benchmarking Grouped Query Attention...\")\n",
    "gqa_memory = measure_memory(gqa_model, x, device)\n",
    "gqa_speed = measure_speed(gqa_model, x)\n",
    "gqa_params = count_parameters(gqa_model)\n",
    "\n",
    "# Benchmark MLA\n",
    "print(\"Benchmarking Multi-Head Latent Attention...\")\n",
    "mla_memory = measure_memory(mla_model, x, device)\n",
    "mla_speed = measure_speed(mla_model, x)\n",
    "mla_params = count_parameters(mla_model)\n",
    "print(\"Benchmarks complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnvif0lxsjr",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tjp7plusb9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK RESULTS\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Batch size: 32\n",
      "  Sequence length: 512\n",
      "  Model dimension: 512\n",
      "  Number of heads: 8\n",
      "  Number of KV heads (GQA): 2\n",
      "  Latent dimension (MLA): 128\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model                          Memory (MB)     Speed (ms)      Parameters (M) \n",
      "--------------------------------------------------------------------------------\n",
      "Multi-Head Attention (MHA)     826.41          9.1889          1.051          \n",
      "Grouped Query Attention (GQA)  762.41          7.9750          0.657          \n",
      "Multi-Head Latent Attention (MLA) 834.41          8.3616          0.723          \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Comparison vs MHA:            \n",
      "--------------------------------------------------------------------------------\n",
      "Metric                         GQA Savings               MLA Savings              \n",
      "--------------------------------------------------------------------------------\n",
      "Memory                           7.74%                    -0.97%\n",
      "Speed                           13.21%                     9.00%\n",
      "Parameters                      37.50%                    31.18%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print benchmark results\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Model dimension: {d_model}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Number of KV heads (GQA): {num_kv_heads}\")\n",
    "print(f\"  Latent dimension (MLA): {d_latent}\")\n",
    "print(f\"\\n{'-' * 80}\")\n",
    "print(f\"{'Model':<30} {'Memory (MB)':<15} {'Speed (ms)':<15} {'Parameters (M)':<15}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "print(f\"{'Multi-Head Attention (MHA)':<30} {mha_memory:<15.2f} {mha_speed:<15.4f} {mha_params:<15.3f}\")\n",
    "print(f\"{'Grouped Query Attention (GQA)':<30} {gqa_memory:<15.2f} {gqa_speed:<15.4f} {gqa_params:<15.3f}\")\n",
    "print(f\"{'Multi-Head Latent Attention (MLA)':<30} {mla_memory:<15.2f} {mla_speed:<15.4f} {mla_params:<15.3f}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "# Calculate savings\n",
    "print(f\"\\n{'Comparison vs MHA:':<30}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "print(f\"{'Metric':<30} {'GQA Savings':<25} {'MLA Savings':<25}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "memory_savings_gqa = ((mha_memory - gqa_memory) / mha_memory) * 100\n",
    "memory_savings_mla = ((mha_memory - mla_memory) / mha_memory) * 100\n",
    "speed_savings_gqa = ((mha_speed - gqa_speed) / mha_speed) * 100\n",
    "speed_savings_mla = ((mha_speed - mla_speed) / mha_speed) * 100\n",
    "param_savings_gqa = ((mha_params - gqa_params) / mha_params) * 100\n",
    "param_savings_mla = ((mha_params - mla_params) / mha_params) * 100\n",
    "\n",
    "print(f\"{'Memory':<30} {memory_savings_gqa:>6.2f}%{'':<18} {memory_savings_mla:>6.2f}%\")\n",
    "print(f\"{'Speed':<30} {speed_savings_gqa:>6.2f}%{'':<18} {speed_savings_mla:>6.2f}%\")\n",
    "print(f\"{'Parameters':<30} {param_savings_gqa:>6.2f}%{'':<18} {param_savings_mla:>6.2f}%\")\n",
    "print(f\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb85d9",
   "metadata": {},
   "source": [
    "# Llama 2 Attention Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fbc22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama_attention(model_size='70B'):\n",
    "    \"\"\"\n",
    "    Creates Llama 2 attention configuration\n",
    "    \n",
    "    Llama 2 70B specs:\n",
    "    - d_model = 8192\n",
    "    - num_heads = 64  \n",
    "    - num_kv_heads = 8 (GQA with group size g=8)\n",
    "    - head_dim = 128\n",
    "    \"\"\"\n",
    "    if model_size == '70B':\n",
    "        d_model = 8192\n",
    "        num_heads = 64\n",
    "        num_kv_heads = 8\n",
    "        dropout = 0.0\n",
    "        \n",
    "        model = GroupedQueryAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_kv_heads=num_kv_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(f\"Model size {model_size} not supported yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ab230c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Llama 2 70B attention...\n",
      "Input shape: torch.Size([1, 4096, 8192])\n",
      "Output shape: torch.Size([1, 4096, 8192])\n",
      "✅ Llama attention works!\n"
     ]
    }
   ],
   "source": [
    "# Test Llama 2 70B configuration\n",
    "print(\"Creating Llama 2 70B attention...\")\n",
    "llama_attn = create_llama_attention('70B')\n",
    "llama_attn = llama_attn.to(device)\n",
    "\n",
    "# Create input (batch=1, seq=4096, dim=8192)\n",
    "x_llama = torch.randn(1, 4096, 8192, device=device)\n",
    "\n",
    "print(f\"Input shape: {x_llama.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output_llama = llama_attn(x_llama)\n",
    "print(f\"Output shape: {output_llama.shape}\")\n",
    "print(\"✅ Llama attention works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1558d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Llama 2 70B KV Cache Analysis:\n",
      "Per layer (4K context): 16.78 MB\n",
      "Total 80 layers (4K context): 1.34 GB\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_cache(model, seq_len, dtype_bytes=2):\n",
    "    \"\"\"\n",
    "    Calculate KV cache memory for a GQA/MHA model\n",
    "    \n",
    "    Args:\n",
    "        model: The attention model (GQA or MHA)\n",
    "        seq_len: Sequence length\n",
    "        dtype_bytes: 2 for FP16, 4 for FP32\n",
    "    \n",
    "    Returns:\n",
    "        Memory in bytes\n",
    "    \"\"\"\n",
    "    d_model = model.d_model\n",
    "    num_kv_heads = model.num_kv_heads\n",
    "    head_dim = model.head_dim\n",
    "    \n",
    "    # KV cache stores: Keys + Values for each KV head\n",
    "    # Shape: [seq_len, num_kv_heads, head_dim]\n",
    "    elements_per_cache = seq_len * num_kv_heads * head_dim\n",
    "    \n",
    "    # Both K and V\n",
    "    total_elements = 2 * elements_per_cache\n",
    "    \n",
    "    # Convert to bytes\n",
    "    memory_bytes = total_elements * dtype_bytes\n",
    "    \n",
    "    return memory_bytes\n",
    "\n",
    "# Calculate for Llama 2 70B\n",
    "kv_cache_per_layer = calculate_kv_cache(llama_attn, seq_len=4096, dtype_bytes=2)\n",
    "total_cache_80_layers = kv_cache_per_layer * 80\n",
    "\n",
    "print(f\"\\nLlama 2 70B KV Cache Analysis:\")\n",
    "print(f\"Per layer (4K context): {kv_cache_per_layer / 1e6:.2f} MB\")\n",
    "print(f\"Total 80 layers (4K context): {total_cache_80_layers / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad1008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
