{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e4709f",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d813fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA RTX 4000 Ada Generation\n",
      "GPU memory: 21.125267456 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"No GPU found - will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d88b1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c9bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e9090",
   "metadata": {},
   "source": [
    "# Raw Implementation of Multi Head Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f729a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  #This runs when you create an object of this class\n",
    "        super().__init__()                                #This is used to call nn.module's init method which which intializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "       #nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "       #It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "       #So it does a y=xw^T+b\n",
    "        \n",
    "        #So we need to create projections for Q, K, V (the parameters are input_dim, output_dim),so self.q_proj will create a weight matrix of size d_model x d_model\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "       #Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        #Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #This is the method which runs when you call the model\n",
    "    def forward(self,x):\n",
    "        #This is tuple unpacking\n",
    "        batch_size,seq_len,d_model=x.size()\n",
    "\n",
    "        #Now we need to project the input matrix into a different matrix\n",
    "        #So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q=self.q_proj(x) # Query= x@W_q^T + b_q\n",
    "        K=self.k_proj(x) # Key= x@W_k^T + b_k\n",
    "        V=self.v_proj(x) # Value= x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q=Q.view(batch_size,seq_len,self.num_heads,self.head_dim)\n",
    "        K=K.view(batch_size,seq_len,self.num_heads,self.head_dim)\n",
    "        V=V.view(batch_size,seq_len,self.num_heads,self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q=Q.transpose(1,2)\n",
    "        K=K.transpose(1,2)\n",
    "        V=V.transpose(1,2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j] :how much token i should attend to token j high score means more attention\n",
    "        scores=torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(self.head_dim)\n",
    "        \n",
    "        #convert to probabilities\n",
    "        attn_weights=torch.softmax(scores,dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f5042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496a0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349b92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
