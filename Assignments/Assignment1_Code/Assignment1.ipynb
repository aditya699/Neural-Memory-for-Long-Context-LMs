{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e4709f",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d813fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA RTX 4000 Ada Generation\n",
      "GPU memory: 21.01805056 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"No GPU found - will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d88b1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c9bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e9090",
   "metadata": {},
   "source": [
    "# Raw Implementation of Multi Head Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f729a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "Multi-head attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f5042",
   "metadata": {},
   "source": [
    "# Raw Implementation for GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1496a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "‚úÖ Grouped Query Attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Grouped Query Attention (GQA) class.\n",
    "    \"\"\" \n",
    "    def __init__(self, d_model, num_heads, num_kv_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads  # In GQA, we have num_kv_heads for K and V, and num_heads for Q. K and V heads are shared across Q head groups, so their initialization is different\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.group_size = num_heads // num_kv_heads  # How many Q heads will share one K, V head\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model, the weight initialization follows Xavier/Kaiming initialization\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  # This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim) for Q and (batch_size, seq_len, num_kv_heads, head_dim) for K and V\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # GQA: Repeat K and V heads to match Q's number of heads\n",
    "        # repeat_interleave is used to repeat the elements of the tensor along a given dimension\n",
    "        # Each K, V head is shared by group_size Q heads\n",
    "        K = K.repeat_interleave(self.group_size, dim=1)  # (batch, num_heads, seq_len, head_dim)\n",
    "        V = V.repeat_interleave(self.group_size, dim=1)  # (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create GQA model\n",
    "    # 8 Q heads, 2 K/V heads ‚Üí 4 Q heads share each K/V head\n",
    "    model = GroupedQueryAttention(d_model=512, num_heads=8, num_kv_heads=2, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"‚úÖ Grouped Query Attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349b92b",
   "metadata": {},
   "source": [
    "# MLA from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed86718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:         torch.Size([32, 10, 512])\n",
      "Output shape:        torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):  # Inheritance\n",
    "    def __init__(self, d_model, num_heads, d_latent, dropout=0.1):  # d_model is the dimension of the model, d_latent is the compression vector\n",
    "        \"\"\"\n",
    "        Implementing Multi-Head Latent Attention from scratch\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class constructor\n",
    "        assert d_model % num_heads == 0  # Assert is used to check if the condition is true\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_latent = d_latent  # Added the compression dimension\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Compress: Initialize the weight matrix for shared compression\n",
    "        self.kv_compress = nn.Linear(d_model, d_latent)\n",
    "        \n",
    "        # Expand latent to K and V for all heads\n",
    "        self.k_proj = nn.Linear(d_latent, d_model)\n",
    "        self.v_proj = nn.Linear(d_latent, d_model)\n",
    "\n",
    "        # NOTE: Q remains the same, there is no compression in Q\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection (This is after attention we need to concatenate all heads)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Create a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack the input\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Project Q (NOTE: Projection just means x@W+b)\n",
    "        Q = self.q_proj(x)\n",
    "\n",
    "        # COMPRESS: Compress input to latent space (THE KEY STEP!)\n",
    "        # Here you compress the input into a smaller latent vector\n",
    "        kv_latent = self.kv_compress(x)\n",
    "\n",
    "        # Now we need to expand latent into K and V\n",
    "        K = self.k_proj(kv_latent)\n",
    "        V = self.v_proj(kv_latent)\n",
    "        \n",
    "        # Here we are reshaping so that each head can compute attention independently and in parallel\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to put heads first\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert scores into attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Multiply with V\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Transpose back\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "       \n",
    "        # Concat all heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Final Output\n",
    "        output = self.out_proj(output)\n",
    "         \n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    # d_model=512, d_latent=128 means we compress from 512 ‚Üí 128 ‚Üí 512\n",
    "    model = MultiHeadLatentAttention(d_model=512, num_heads=8, d_latent=128, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:         {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape:        {output.shape}\")   # [32, 10, 512]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a87040",
   "metadata": {},
   "source": [
    "# BenchMarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc51483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "# from attention import MultiHeadAttention, GroupedQueryAttention, MultiHeadLatentAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f715fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_kv_heads = 2  # For GQA\n",
    "d_latent = 128    # For MLA\n",
    "seq_len = 1024\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5395d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All models created!\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "gqa = GroupedQueryAttention(d_model=d_model, num_heads=num_heads, num_kv_heads=num_kv_heads)\n",
    "mla = MultiHeadLatentAttention(d_model=d_model, num_heads=num_heads, d_latent=d_latent)\n",
    "\n",
    "print(\"‚úÖ All models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a55498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "‚úÖ Models moved to device!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and move models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "mha = mha.to(device)\n",
    "gqa = gqa.to(device)\n",
    "mla = mla.to(device)\n",
    "\n",
    "print(\"‚úÖ Models moved to device!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d69a1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1024, 512])\n",
      "‚úÖ Input created!\n"
     ]
    }
   ],
   "source": [
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(\"‚úÖ Input created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f71d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç KV Cache Size (what matters for inference!):\n",
      "\n",
      "MHA KV cache: 4.00 MB\n",
      "GQA KV cache: 1.00 MB\n",
      "MLA KV cache: 1.00 MB\n",
      "\n",
      "üéâ Real Savings:\n",
      "GQA vs MHA: 4.00x smaller\n",
      "MLA vs MHA: 4.00x smaller\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_cache_size(model, seq_len, batch_size=1):\n",
    "    \"\"\"Calculate theoretical KV cache size in MB\"\"\"\n",
    "    \n",
    "    if isinstance(model, MultiHeadAttention):\n",
    "        # MHA: num_heads K + num_heads V\n",
    "        kv_heads = model.num_heads * 2  # K and V\n",
    "    elif isinstance(model, GroupedQueryAttention):\n",
    "        # GQA: num_kv_heads K + num_kv_heads V\n",
    "        kv_heads = model.num_kv_heads * 2  # K and V\n",
    "    elif isinstance(model, MultiHeadLatentAttention):\n",
    "        # MLA: compressed latent for K and V\n",
    "        # Cache size = 2 * d_latent (for K and V latent)\n",
    "        bytes_per_element = 4  # float32\n",
    "        cache_size = 2 * batch_size * seq_len * model.d_latent * bytes_per_element\n",
    "        return cache_size / (1024 ** 2)  # Convert to MB\n",
    "    \n",
    "    # For MHA and GQA\n",
    "    head_dim = model.head_dim\n",
    "    bytes_per_element = 4  # float32\n",
    "    cache_size = kv_heads * batch_size * seq_len * head_dim * bytes_per_element\n",
    "    return cache_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "print(\"\\nüîç KV Cache Size (what matters for inference!):\\n\")\n",
    "\n",
    "mha_cache = calculate_kv_cache_size(mha, seq_len)\n",
    "gqa_cache = calculate_kv_cache_size(gqa, seq_len)\n",
    "mla_cache = calculate_kv_cache_size(mla, seq_len)\n",
    "\n",
    "print(f\"MHA KV cache: {mha_cache:.2f} MB\")\n",
    "print(f\"GQA KV cache: {gqa_cache:.2f} MB\")\n",
    "print(f\"MLA KV cache: {mla_cache:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüéâ Real Savings:\")\n",
    "print(f\"GQA vs MHA: {mha_cache/gqa_cache:.2f}x smaller\")\n",
    "print(f\"MLA vs MHA: {mha_cache/mla_cache:.2f}x smaller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cc42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
