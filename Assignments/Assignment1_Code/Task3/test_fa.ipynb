{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcce0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FLASH ATTENTION COMPATIBILITY TEST\n",
      "======================================================================\n",
      "\n",
      "[TEST 1] GPU Information:\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU name: NVIDIA RTX 4000 Ada Generation\n",
      "GPU memory: 20.99 GB\n",
      "Compute capability: 8.9\n",
      "\n",
      "[TEST 2] PyTorch Native Flash Attention:\n",
      "✅ scaled_dot_product_attention found (PyTorch 2.0+)\n",
      "\n",
      "[TEST 3] Basic Flash Attention Test:\n",
      "✅ Flash Attention works!\n",
      "   Input shape: torch.Size([2, 8, 512, 64])\n",
      "   Output shape: torch.Size([2, 8, 512, 64])\n",
      "   Memory used: 0.00 GB\n",
      "\n",
      "[TEST 4] Available SDPA Backends:\n",
      "✅ FLASH_ATTENTION available\n",
      "✅ EFFICIENT_ATTENTION available\n",
      "✅ MATH available\n",
      "\n",
      "[TEST 5] Memory Comparison Test:\n",
      "Testing standard attention vs Flash Attention memory usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Attention: 86.11 MB\n",
      "Flash Attention: 20.05 MB\n",
      "Memory savings: 76.7%\n",
      "\n",
      "[TEST 6] Speed Comparison Test:\n",
      "Standard Attention: 1.01 ms/iteration\n",
      "Flash Attention: 0.15 ms/iteration\n",
      "Speedup: 6.63x faster\n",
      "\n",
      "======================================================================\n",
      "TEST COMPLETE\n",
      "======================================================================\n",
      "\n",
      "[VERDICT]\n",
      "✅ Flash Attention WILL WORK on this GPU!\n",
      "   You can proceed with Experiment 6.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Flash Attention Compatibility Test for RTX 4000 Ada\n",
    "Run this BEFORE Experiment 6\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FLASH ATTENTION COMPATIBILITY TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Basic GPU Info\n",
    "print(\"\\n[TEST 1] GPU Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    compute_cap = torch.cuda.get_device_capability(0)\n",
    "    print(f\"Compute capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
    "else:\n",
    "    print(\"❌ CUDA not available! Check your environment.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Test 2: Check if scaled_dot_product_attention exists (PyTorch 2.0+)\n",
    "print(\"\\n[TEST 2] PyTorch Native Flash Attention:\")\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    print(\"✅ scaled_dot_product_attention found (PyTorch 2.0+)\")\n",
    "except ImportError:\n",
    "    print(\"❌ scaled_dot_product_attention NOT found\")\n",
    "    print(\"   You need PyTorch 2.0+\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Test 3: Test basic Flash Attention call\n",
    "print(\"\\n[TEST 3] Basic Flash Attention Test:\")\n",
    "try:\n",
    "    Q = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "    K = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "    V = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Test with is_causal (what you need for language modeling)\n",
    "    out = torch.nn.functional.scaled_dot_product_attention(\n",
    "        Q, K, V, \n",
    "        is_causal=True,\n",
    "        dropout_p=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Flash Attention works!\")\n",
    "    print(f\"   Input shape: {Q.shape}\")\n",
    "    print(f\"   Output shape: {out.shape}\")\n",
    "    print(f\"   Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Flash Attention failed!\")\n",
    "    print(f\"   Error: {str(e)}\")\n",
    "    print(\"\\n   Possible causes:\")\n",
    "    print(\"   - GPU compute capability not supported\")\n",
    "    print(\"   - CUDA/PyTorch version mismatch\")\n",
    "    print(\"   - Driver issue\")\n",
    "\n",
    "# Test 4: Check which backends are available\n",
    "print(\"\\n[TEST 4] Available SDPA Backends:\")\n",
    "try:\n",
    "    from torch.backends.cuda import sdp_kernel, SDPBackend\n",
    "    \n",
    "    # Create dummy tensors\n",
    "    Q = torch.randn(1, 1, 128, 64, device='cuda', dtype=torch.float16)\n",
    "    K = torch.randn(1, 1, 128, 64, device='cuda', dtype=torch.float16)\n",
    "    V = torch.randn(1, 1, 128, 64, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    backends = {\n",
    "        \"FLASH_ATTENTION\": SDPBackend.FLASH_ATTENTION,\n",
    "        \"EFFICIENT_ATTENTION\": SDPBackend.EFFICIENT_ATTENTION,\n",
    "        \"MATH\": SDPBackend.MATH\n",
    "    }\n",
    "    \n",
    "    for name, backend in backends.items():\n",
    "        try:\n",
    "            with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n",
    "                # Enable only this backend\n",
    "                if backend == SDPBackend.FLASH_ATTENTION:\n",
    "                    with sdp_kernel(enable_flash=True):\n",
    "                        torch.nn.functional.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "                elif backend == SDPBackend.EFFICIENT_ATTENTION:\n",
    "                    with sdp_kernel(enable_mem_efficient=True):\n",
    "                        torch.nn.functional.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "                else:\n",
    "                    with sdp_kernel(enable_math=True):\n",
    "                        torch.nn.functional.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "                print(f\"✅ {name} available\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {name} not available: {str(e)}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"⚠️  Cannot check backends (need PyTorch 2.0+)\")\n",
    "\n",
    "# Test 5: Memory comparison test\n",
    "print(\"\\n[TEST 5] Memory Comparison Test:\")\n",
    "print(\"Testing standard attention vs Flash Attention memory usage...\")\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"Your current attention implementation\"\"\"\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "    \n",
    "    # Causal mask\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device='cuda'))\n",
    "    scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "    \n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attn, V)\n",
    "    return out\n",
    "\n",
    "# Standard attention memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "\n",
    "try:\n",
    "    out_standard = standard_attention(Q, K, V)\n",
    "    mem_standard = torch.cuda.max_memory_allocated() / 1e6\n",
    "    print(f\"Standard Attention: {mem_standard:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Standard Attention failed: {e}\")\n",
    "    mem_standard = 0\n",
    "\n",
    "# Flash attention memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "\n",
    "try:\n",
    "    out_flash = torch.nn.functional.scaled_dot_product_attention(\n",
    "        Q, K, V, is_causal=True\n",
    "    )\n",
    "    mem_flash = torch.cuda.max_memory_allocated() / 1e6\n",
    "    print(f\"Flash Attention: {mem_flash:.2f} MB\")\n",
    "    \n",
    "    if mem_standard > 0:\n",
    "        savings = ((mem_standard - mem_flash) / mem_standard) * 100\n",
    "        print(f\"Memory savings: {savings:.1f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"Flash Attention failed: {e}\")\n",
    "\n",
    "# Test 6: Speed comparison\n",
    "print(\"\\n[TEST 6] Speed Comparison Test:\")\n",
    "import time\n",
    "\n",
    "def benchmark(fn, Q, K, V, num_runs=100):\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        fn(Q, K, V)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        fn(Q, K, V)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    \n",
    "    return (end - start) / num_runs * 1000  # ms per run\n",
    "\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "\n",
    "try:\n",
    "    time_standard = benchmark(standard_attention, Q, K, V)\n",
    "    print(f\"Standard Attention: {time_standard:.2f} ms/iteration\")\n",
    "except:\n",
    "    print(\"Standard Attention: Failed\")\n",
    "    time_standard = 0\n",
    "\n",
    "try:\n",
    "    def flash_fn(Q, K, V):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "    \n",
    "    time_flash = benchmark(flash_fn, Q, K, V)\n",
    "    print(f\"Flash Attention: {time_flash:.2f} ms/iteration\")\n",
    "    \n",
    "    if time_standard > 0:\n",
    "        speedup = time_standard / time_flash\n",
    "        print(f\"Speedup: {speedup:.2f}x faster\")\n",
    "except:\n",
    "    print(\"Flash Attention: Failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n[VERDICT]\")\n",
    "if torch.cuda.is_available() and hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
    "    try:\n",
    "        Q = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "        K = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "        V = torch.randn(2, 8, 512, 64, device='cuda', dtype=torch.float16)\n",
    "        torch.nn.functional.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "        print(\"✅ Flash Attention WILL WORK on this GPU!\")\n",
    "        print(\"   You can proceed with Experiment 6.\")\n",
    "    except:\n",
    "        print(\"❌ Flash Attention WILL NOT WORK on this GPU!\")\n",
    "        print(\"   Consider alternative GPUs or standard attention.\")\n",
    "else:\n",
    "    print(\"❌ Requirements not met. Check PyTorch version and CUDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834651f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
