{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0744f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Force clear everything\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RoPE: Rotary Positional Embedding\n",
    "    Encodes position information via rotation in complex space\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Precompute inverse frequencies for rotation angles\n",
    "        # θᵢ = base^(-2i/dim) for i ∈ [0, dim/2)\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        Apply rotary position embeddings to input tensor\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch, num_heads, seq_len, head_dim]\n",
    "            seq_len: Sequence length\n",
    "\n",
    "        Returns:\n",
    "            Rotated tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Generate position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).float()\n",
    "\n",
    "        # Compute angles: outer product of positions and inverse frequencies\n",
    "        # Shape: [seq_len, head_dim/2]\n",
    "        freqs = torch.outer(positions, self.inv_freq)\n",
    "\n",
    "        # Create complex representation using Euler's formula: e^(iθ) = cos(θ) + i*sin(θ)\n",
    "        # Shape: [seq_len, head_dim/2]\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "        # Reshape input to complex: pair adjacent dimensions as real/imaginary parts\n",
    "        # [batch, heads, seq, head_dim] -> [batch, heads, seq, head_dim/2, 2] -> [batch, heads, seq, head_dim/2]\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        # Apply rotation via complex multiplication\n",
    "        # Broadcast freqs_complex to match batch and head dimensions\n",
    "        freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, head_dim/2]\n",
    "        x_rotated = x_complex * freqs_complex\n",
    "\n",
    "        # Convert back to real representation\n",
    "        # [batch, heads, seq, head_dim/2] -> [batch, heads, seq, head_dim/2, 2] -> [batch, heads, seq, head_dim]\n",
    "        x_out = torch.view_as_real(x_rotated).flatten(-2)\n",
    "\n",
    "        return x_out.type_as(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Initialize RoPE for positional encoding\n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # ============================================\n",
    "        # APPLY ROPE (Rotary Positional Embedding)\n",
    "        # Apply rotation to Q and K for position encoding\n",
    "        # V is NOT rotated - only queries and keys need position info\n",
    "        # ============================================\n",
    "        Q = self.rope(Q, seq_len)\n",
    "        K = self.rope(K, seq_len)\n",
    "        # ============================================\n",
    "\n",
    "        # ============================================\n",
    "        # FLASH ATTENTION (Memory-efficient implementation)\n",
    "        # Replaces manual attention computation with PyTorch's optimized version\n",
    "        # ============================================\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            dropout_p=self.dropout.p if self.training else 0.0,\n",
    "            is_causal=True  # Automatic causal masking - no need for manual mask!\n",
    "        )\n",
    "        # ============================================\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Only token embeddings - no learned positional embeddings\n",
    "        # Position information is now handled by RoPE in the attention layer\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return only token embeddings\n",
    "        # RoPE will add positional information during attention computation\n",
    "        return self.token_embed(token_ids)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # No max_seq_len needed - RoPE handles arbitrary sequence lengths\n",
    "        self.embeddings = Embeddings(vocab_size, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ============================================\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30, patience=20):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Track metrics history for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    # Early stopping tracking\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "        # Save best model and track early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "\n",
    "            print(f\"  Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "            print(f\"  Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! No improvement for {patience} consecutive epochs.\")\n",
    "            print(f\"   Best model was at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def plot_metrics(metrics_history, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot all training metrics including train loss, val loss, and val perplexity.\n",
    "\n",
    "    Args:\n",
    "        metrics_history: Dictionary containing training metrics\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    train_losses = metrics_history['train_losses']\n",
    "    val_losses = metrics_history['val_losses']\n",
    "    val_perplexities = metrics_history['val_perplexities']\n",
    "    best_epoch = metrics_history['best_epoch']\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    axes[0, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 1].scatter([best_epoch], [metrics_history['best_val_loss']],\n",
    "                       color='r', s=100, zorder=5, label='Best Val Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Validation Loss over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Validation Perplexity\n",
    "    axes[1, 0].plot(epochs, val_perplexities, 'purple', linewidth=2, label='Val Perplexity')\n",
    "    axes[1, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Validation Perplexity over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Train vs Validation Loss Comparison\n",
    "    axes[1, 1].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[1, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[1, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Train vs Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nTraining metrics plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "    val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        # max_seq_len removed - RoPE handles arbitrary sequence lengths!\n",
    "        d_model=512,      # ← Change from 320\n",
    "        num_heads=8,      # ← Change from 10\n",
    "        d_ff=2048,        # ← Change from 1280\n",
    "        num_layers=12,    # ← Change from 15\n",
    "        dropout=0.1\n",
    "    ).to('cuda')\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    metrics_history = train_model_properly(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=5\n",
    "    )\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PLOTTING TRAINING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    plot_metrics(metrics_history, save_path='training_metrics.png')\n",
    "\n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompts = [\n",
    "        \"The history of India is \",\n",
    "        \"In mathematics,\",\n",
    "        \"The cat sat on the\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text_proper(model, tokenizer, prompt, max_length=40, device=device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generated}\")\n",
    "        print(\"-\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
