{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0744f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Memory: 0.00 GB\n",
      "\n",
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4e28c2f31048fc9f791a81e46bae33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2aafa19fcb4609aa2d32a5a34c0c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee282a6d9f048829329e78eee381d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3483bf67794293a1607cdedd409818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a5fa10bdea4b799527bec25e46646a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7154e432e17f4253af963560e62dc0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c7b2727c0b46a48c5ff9a54c676e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16369af7199340628a0970dbd04c071a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6157d74a4f4b9ca0b3370192e5c032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f3095132f4c93bb75e6b939f22892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59c6bfae17f4aa5a02e0f7f2d8b9f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0998716ce44d7a8060ed137b5b63a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d953825236748c997d8b7d3f4342261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 115,716,078\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 14126\n",
      "Val batches: 30\n",
      "\n",
      "Creating model...\n",
      "Parameters: 63,611,473\n",
      "GPU Memory: 0.25 GB\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   5%|▍         | 652/14126 [04:00<1:23:40,  2.68it/s, loss=15.9962]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Force clear everything\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RoPE: Rotary Positional Embedding\n",
    "    Encodes position information via rotation in complex space\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Precompute inverse frequencies for rotation angles\n",
    "        # θᵢ = base^(-2i/dim) for i ∈ [0, dim/2)\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        \"\"\"\n",
    "        Apply rotary position embeddings to input tensor\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch, num_heads, seq_len, head_dim]\n",
    "            seq_len: Sequence length\n",
    "\n",
    "        Returns:\n",
    "            Rotated tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Generate position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).float()\n",
    "\n",
    "        # Compute angles: outer product of positions and inverse frequencies\n",
    "        # Shape: [seq_len, head_dim/2]\n",
    "        freqs = torch.outer(positions, self.inv_freq)\n",
    "\n",
    "        # Create complex representation using Euler's formula: e^(iθ) = cos(θ) + i*sin(θ)\n",
    "        # Shape: [seq_len, head_dim/2]\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "        # Reshape input to complex: pair adjacent dimensions as real/imaginary parts\n",
    "        # [batch, heads, seq, head_dim] -> [batch, heads, seq, head_dim/2, 2] -> [batch, heads, seq, head_dim/2]\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        # Apply rotation via complex multiplication\n",
    "        # Broadcast freqs_complex to match batch and head dimensions\n",
    "        freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, head_dim/2]\n",
    "        x_rotated = x_complex * freqs_complex\n",
    "\n",
    "        # Convert back to real representation\n",
    "        # [batch, heads, seq, head_dim/2] -> [batch, heads, seq, head_dim/2, 2] -> [batch, heads, seq, head_dim]\n",
    "        x_out = torch.view_as_real(x_rotated).flatten(-2)\n",
    "\n",
    "        return x_out.type_as(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Initialize RoPE for positional encoding\n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # ============================================\n",
    "        # APPLY ROPE (Rotary Positional Embedding)\n",
    "        # Apply rotation to Q and K for position encoding\n",
    "        # V is NOT rotated - only queries and keys need position info\n",
    "        # ============================================\n",
    "        Q = self.rope(Q, seq_len)\n",
    "        K = self.rope(K, seq_len)\n",
    "        # ============================================\n",
    "\n",
    "        # ============================================\n",
    "        # FLASH ATTENTION (Memory-efficient implementation)\n",
    "        # Replaces manual attention computation with PyTorch's optimized version\n",
    "        # ============================================\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            dropout_p=self.dropout.p if self.training else 0.0,\n",
    "            is_causal=True  # Automatic causal masking - no need for manual mask!\n",
    "        )\n",
    "        # ============================================\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Only token embeddings - no learned positional embeddings\n",
    "        # Position information is now handled by RoPE in the attention layer\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return only token embeddings\n",
    "        # RoPE will add positional information during attention computation\n",
    "        return self.token_embed(token_ids)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # No max_seq_len needed - RoPE handles arbitrary sequence lengths\n",
    "        self.embeddings = Embeddings(vocab_size, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ============================================\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30, patience=20):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Track metrics history for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    # Early stopping tracking\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "        # Save best model and track early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "\n",
    "            print(f\"  Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "            print(f\"  Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! No improvement for {patience} consecutive epochs.\")\n",
    "            print(f\"   Best model was at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def plot_metrics(metrics_history, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot all training metrics including train loss, val loss, and val perplexity.\n",
    "\n",
    "    Args:\n",
    "        metrics_history: Dictionary containing training metrics\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    train_losses = metrics_history['train_losses']\n",
    "    val_losses = metrics_history['val_losses']\n",
    "    val_perplexities = metrics_history['val_perplexities']\n",
    "    best_epoch = metrics_history['best_epoch']\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    axes[0, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 1].scatter([best_epoch], [metrics_history['best_val_loss']],\n",
    "                       color='r', s=100, zorder=5, label='Best Val Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Validation Loss over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Validation Perplexity\n",
    "    axes[1, 0].plot(epochs, val_perplexities, 'purple', linewidth=2, label='Val Perplexity')\n",
    "    axes[1, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Validation Perplexity over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Train vs Validation Loss Comparison\n",
    "    axes[1, 1].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[1, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[1, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Train vs Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nTraining metrics plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "    val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        # max_seq_len removed - RoPE handles arbitrary sequence lengths!\n",
    "        d_model=512,      # ← Change from 320\n",
    "        num_heads=8,      # ← Change from 10\n",
    "        d_ff=2048,        # ← Change from 1280\n",
    "        num_layers=12,    # ← Change from 15\n",
    "        dropout=0.1\n",
    "    ).to('cuda')\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    metrics_history = train_model_properly(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=5\n",
    "    )\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PLOTTING TRAINING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    plot_metrics(metrics_history, save_path='training_metrics.png')\n",
    "\n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompts = [\n",
    "        \"The history of India is \",\n",
    "        \"In mathematics,\",\n",
    "        \"The cat sat on the\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text_proper(model, tokenizer, prompt, max_length=40, device=device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generated}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543d7f3",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefec362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUICK SANITY CHECK - RoPE + Flash Attention\n",
      "======================================================================\n",
      "\n",
      "Device: cuda\n",
      "\n",
      "Loading TINY dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d48da9a9dfa4b27907ee1b667386bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeba920760f454eaccb5cd8129f321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a3f174f6314a8996f8cc8a9ea07e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a7f5547ad43c6bab590eebbfe249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c4e58f04741ab82ac998affdb6a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775cacbd64fe4208adf8f6d492612299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0febddfa02474ca572fb734db7ec8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360eb5a68eec4972b8ec84024724559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee59c972964072961a88bed79a176a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c2f342fa9a4eb0867d84e1fc84f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating tiny datasets...\n",
      "Tokenizing TINY dataset (1000 examples)...\n",
      "Total tokens: 60,621\n",
      "Tokenizing TINY dataset (100 examples)...\n",
      "Total tokens: 6,165\n",
      "Train batches: 60\n",
      "Val batches: 6\n",
      "\n",
      "Creating SMALL model...\n",
      "Parameters: 6,879,953\n",
      "\n",
      "======================================================================\n",
      "TESTING 2 EPOCHS (Should take ~1-2 minutes)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 60/60 [00:01<00:00, 47.99it/s, loss=34.8228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 34.8228\n",
      "  Val Loss: 20.6155\n",
      "  Val Perplexity: 897849344.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 60/60 [00:00<00:00, 67.71it/s, loss=17.4635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 17.4635\n",
      "  Val Loss: 13.3198\n",
      "  Val Perplexity: 609132.94\n",
      "\n",
      "======================================================================\n",
      "TESTING GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The cat\n",
      "Generated: The cat in theod for in in were were were been Ry in . Santos could could to to in in\n",
      "\n",
      "======================================================================\n",
      "SUCCESS! All checks passed. RoPE + Flash Attention working correctly!\n",
      "======================================================================\n",
      "\n",
      "You can now run the full training with confidence.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS (Same as main script)\n",
    "# ============================================\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        positions = torch.arange(seq_len, device=x.device).float()\n",
    "        freqs = torch.outer(positions, self.inv_freq)\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "        freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(0)\n",
    "        x_rotated = x_complex * freqs_complex\n",
    "        x_out = torch.view_as_real(x_rotated).flatten(-2)\n",
    "        return x_out.type_as(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        Q = self.rope(Q, seq_len)\n",
    "        K = self.rope(K, seq_len)\n",
    "        \n",
    "        # Flash Attention\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            dropout_p=self.dropout.p if self.training else 0.0,\n",
    "            is_causal=True\n",
    "        )\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        return self.token_embed(token_ids)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MINIMAL DATASET (Just first 1000 examples)\n",
    "# ============================================\n",
    "\n",
    "class TinyWikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128, num_examples=1000):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Tokenizing TINY dataset ({num_examples} examples)...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for i, example in enumerate(data):\n",
    "            if i >= num_examples:  # Only use first N examples\n",
    "                break\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# QUICK TEST\n",
    "# ============================================\n",
    "\n",
    "def quick_test():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QUICK SANITY CHECK - RoPE + Flash Attention\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "    # Load tiny dataset\n",
    "    print(\"\\nLoading TINY dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create minimal datasets\n",
    "    print(\"\\nCreating tiny datasets...\")\n",
    "    train_dataset = TinyWikiTextDataset(dataset['train'], tokenizer, max_length=128, num_examples=1000)\n",
    "    val_dataset = TinyWikiTextDataset(dataset['validation'], tokenizer, max_length=128, num_examples=100)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create SMALL model\n",
    "    print(\"\\nCreating SMALL model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        d_model=128,      # Small for quick test\n",
    "        num_heads=4,      # Small for quick test\n",
    "        d_ff=512,         # Small for quick test\n",
    "        num_layers=2,     # Small for quick test\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    \n",
    "    # Quick training test\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING 2 EPOCHS (Should take ~1-2 minutes)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/2\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Quick validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, target_ids in val_loader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                target_ids = target_ids.to(device)\n",
    "                logits = model(input_ids)\n",
    "                \n",
    "                batch_size, seq_len, vocab_size = logits.shape\n",
    "                logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "                targets_flat = target_ids.view(batch_size * seq_len)\n",
    "                loss = criterion(logits_flat, targets_flat)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_perplexity = torch.exp(torch.tensor(avg_val_loss))\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "    \n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model.eval()\n",
    "    prompt = \"The cat\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(20):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, 50)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    generated = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUCCESS! All checks passed. RoPE + Flash Attention working correctly!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nYou can now run the full training with confidence.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8dc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
