{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.4903,  1.9947,  0.6297], requires_grad=True)\n",
      "Output: tensor([-0.4903,  3.9895,  1.8890], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([2, 10])\n",
      "Output embeddings shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "#Create\n",
    "vocab_size = 1000  # Size of the vocabulary\n",
    "d_model = 512    # Dimension of the embeddings\n",
    "token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "#Test\n",
    "token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "output= token_embed (token_ids)\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output position embeddings shape: torch.Size([10, 512])\n",
      "Positions Used: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "#Create\n",
    "pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "output = pos_embed(10)  # Example input sequence length\n",
    "print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98afba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# Test\n",
    "embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "output = embeddings(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: -0.0006\n",
      "  Std: 0.9969\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: 0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create FFN\n",
    "ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "Multi-head attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "✅ Transformer Block works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# Forward pass\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Number of blocks: 6\n"
     ]
    }
   ],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create model with 6 stacked blocks\n",
    "model = Transformer(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,  # 6 transformer blocks!\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "output = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# Create complete language model\n",
    "model = LanguageModel(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))\n",
    "logits = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa64319cafa84fb99bbecf3bf767ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff8acc2c0eb4c32992cb280d8487b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52366507ce7a4cf699a2b48d83c79fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ba141559ad45309bdd992fca9d798f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05274b10823b4c1eaf150752aba86b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467c2c0e9574478da5e51b632d03e643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412eceee6a7b468db30ef82990be4ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n",
      "\n",
      "First training example:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677d46739fee46d398d4440834982449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa524c7ff4e414cbd2483ed0a5a57af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349a3061e18e401eaca61b478ee41efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf80c56c7754a6e91db6b4364fd74d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067cfc176e74d16a36c68b607d9fca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "\n",
      "Total sequences: 4584\n",
      "\n",
      "Sample 0:\n",
      "Input shape: torch.Size([512])\n",
      "Target shape: torch.Size([512])\n",
      "First 10 input tokens: tensor([   28,   569, 18354,  7496, 17740,  6711,   796, 10445,    73, 13090])\n",
      "First 10 target tokens: tensor([  569, 18354,  7496, 17740,  6711,   796, 10445,    73, 13090,   645])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8c895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
