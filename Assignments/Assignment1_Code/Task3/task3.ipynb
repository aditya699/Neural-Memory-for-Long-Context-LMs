{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1 (Do not run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.8741,  1.4167,  1.5167], requires_grad=True)\n",
      "Output: tensor([-0.8741,  2.8334,  4.5501], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "# #Create\n",
    "# vocab_size = 1000  # Size of the vocabulary\n",
    "# d_model = 512    # Dimension of the embeddings\n",
    "# token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# #Test\n",
    "# token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "# output= token_embed (token_ids)\n",
    "# print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "# print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "# #Create\n",
    "# pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "# output = pos_embed(10)  # Example input sequence length\n",
    "# print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "# print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98afba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# # Test\n",
    "# embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "# output = embeddings(token_ids)\n",
    "\n",
    "# print(f\"Input shape: {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: 0.0131\n",
      "  Std: 0.9961\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: -0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create FFN\n",
    "# ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# # Test\n",
    "# x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "# output = ffn(x)\n",
    "\n",
    "# print(f\"Input shape: {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# # Test the implementation\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create model\n",
    "#     model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "#     # Create input\n",
    "#     batch_size = 32\n",
    "#     seq_len = 10\n",
    "#     x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(x)\n",
    "    \n",
    "#     print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "#     print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "#     print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# # Input\n",
    "# x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# # Forward pass\n",
    "# output = block(x)\n",
    "\n",
    "# print(f\"Input shape:  {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create model with 6 stacked blocks\n",
    "# model = Transformer(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,  # 6 transformer blocks!\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "# output = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Tie weights between token embeddings and lm_head (Weight Sharing) this is important for reducing the number of parameters and improving generalization. Morevover see token embeddings and lm_head are performing inverse operations. example token embeddings map token IDs to vectors, while lm_head maps vectors back to token logits.\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# # Create complete language model\n",
    "# model = LanguageModel(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))\n",
    "# logits = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "# print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63110246f574d3b9619f91106e284e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb9d20f814d4e2babd2e6f3d54289ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d914ea77d1245efba483e244e0e13f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2205567f7a6e4569a98bc6a72a3d1ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725d7499e2d49a688659c3fb1831a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045fdf868dc14f05982f875a6b09e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ee7af13c564dfa90f7fc6410cff48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75181c8aab614c9abad954eca1c2317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406fef5760b41559009fc5c12ab09c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082a49dfc5a44d5389cf08218021987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ddf0c3c500446b82278509620e8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee748b27664c308f84070ffd8eb5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create our dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_dataset = \u001b[43mWikiTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get first sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mWikiTextDataset.__init__\u001b[39m\u001b[34m(self, data, tokenizer, max_length)\u001b[39m\n\u001b[32m     19\u001b[39m     text = example[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Skip empty lines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         all_tokens.extend(tokens)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Step 2: Convert to PyTorch tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2732\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2694\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2695\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2715\u001b[39m     **kwargs,\n\u001b[32m   2716\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2717\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2718\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2719\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2730\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     encoded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3123\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3096\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3111\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3114\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3115\u001b[39m     padding=padding,\n\u001b[32m   3116\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m     **kwargs,\n\u001b[32m   3121\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3142\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2.py:281\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8c895",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "```\n",
    "Loss(So loss is computed for each token)(Define the Loss)\n",
    "↓\n",
    "Optimizer(Optimizer updates the model's weights to minimize loss.)\n",
    "↓\n",
    "Training Loop(Forward-Loss-clear Gradients-Backward-Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 5])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "\n",
      "After reshaping:\n",
      "Logits flat: torch.Size([6, 5])\n",
      "Targets flat: torch.Size([6])\n",
      "\n",
      "Loss: 2.0549\n"
     ]
    }
   ],
   "source": [
    "#Define the Loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake model output (logits)\n",
    "# [batch=2, seq=3, vocab_size=5]\n",
    "logits = torch.randn(2, 3, 5)\n",
    "\n",
    "# Target token IDs\n",
    "# [batch=2, seq=3]\n",
    "targets = torch.tensor([\n",
    "    [2, 4, 1],  # Sequence 1: correct tokens are 2, 4, 1\n",
    "    [0, 3, 2]   # Sequence 2: correct tokens are 0, 3, 2\n",
    "])\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)    # [2, 3, 5]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# BUT! CrossEntropyLoss expects:\n",
    "# logits: [N, vocab_size] where N = batch * seq\n",
    "# targets: [N]\n",
    "\n",
    "# So we need to reshape!\n",
    "logits_flat = logits.view(-1, 5)      # [6, 5]  (2*3=6 positions)\n",
    "targets_flat = targets.view(-1)        # [6]\n",
    "\n",
    "print(\"\\nAfter reshaping:\")\n",
    "print(\"Logits flat:\", logits_flat.shape)   # [6, 5]\n",
    "print(\"Targets flat:\", targets_flat.shape) # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Logits shape: torch.Size([32, 512, 50257])\n",
      "\n",
      "Flattened logits: torch.Size([16384, 50257])\n",
      "Flattened targets: torch.Size([16384])\n",
      "\n",
      "Loss: 11.0243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate model training\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Get a batch from dataloader\n",
    "input_ids, target_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")    # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Logits shape: {logits.shape}\")      # [32, 512, 50257]\n",
    "\n",
    "# Reshape for loss calculation\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "logits_flat = logits.view(batch_size * seq_len, vocab_size)  # [16384, 50257]\n",
    "targets_flat = target_ids.view(batch_size * seq_len)         # [16384]\n",
    "\n",
    "print(f\"\\nFlattened logits: {logits_flat.shape}\")\n",
    "print(f\"Flattened targets: {targets_flat.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created!\n",
      "Learning rate: 3e-4 = 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),  # All model weights to optimize\n",
    "    lr=3e-4,            # Learning rate (how big each update is)\n",
    "    weight_decay=0.01   # Regularization (prevents overfitting)\n",
    ")\n",
    "\n",
    "print(\"Optimizer created!\")\n",
    "print(f\"Learning rate: 3e-4 = {3e-4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fca023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Training ===\n",
      "Weight sample: -0.2838\n",
      "Loss: 1.8065\n",
      "Gradient sample: -0.151754\n",
      "=== After Training ===\n",
      "Weight sample: -0.2828\n",
      "Weight changed! ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(2, 10)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()           # Clear old gradients\n",
    "output = model(x)               # Forward\n",
    "loss = criterion(output, targets)  # Loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()                 # Calculate gradients\n",
    "print(f\"Gradient sample: {model.weight.grad[0, 0].item():.6f}\")\n",
    "\n",
    "optimizer.step()                # Update weights\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "print(\"Weight changed! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2794887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the language model\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            # Move data to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)  # [batch, seq, vocab]\n",
    "            \n",
    "            # Reshape for loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bf95b",
   "metadata": {},
   "source": [
    "# Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a23b0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after clearing: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after cleanup: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete ALL variables\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "del train_loader\n",
    "del train_dataset\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22332564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c80b82048e470b9012b2fe197a985d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057796956ac43cfa9c6440b034b37e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42992a4da2054fd4926e7eebd10a7f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc021f20604e3ebe39ca40180275e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d53f74a0349d7bbc222f81c40d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6d4ba893344c00a4d8dd3db6c882e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31973e826bd49358453ab05aa904754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5640029c2af348c2a3f9742ead92beec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5440a1482c14aa0b745ae720856390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc681d0ee0834c28bb41000258030d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beb87617ddb4006aa7c1afcf89bed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d975c24fa52a41c5baf31f003f6a5d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Check GPU is clean\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  # Should be 0!\n",
    "\n",
    "# 3. Load data (only what we need)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde39b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial GPU Memory: 0.00 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Dataset ready. GPU Memory: 0.00 GB\n",
      "Model created. GPU Memory: 0.00 GB\n",
      "Parameters: 3,399,569\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# ============================================\n",
    "# 1. COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "# MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        #This initializes the linear layers for query, key, value and output projections\n",
    "        #Linear layers are just weight matrices that will be learned during training\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Dropout layer for regularization this is used to prevent overfitting by randomly setting some of the attention weights to zero during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Here we r unpack the input tensor x to get batch size and sequence length\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        #Here we r passing the input x through the linear layers to get the query, key, and value matrices\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        # Now we reshape Q, K, V to separate the heads for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        #NOTE: Causal masking is essential in autoregressive models to ensure that each token can only attend to previous tokens and itself, preventing any \"future\" information leakage.\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# FeedForward class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "# LanguageModel class\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# WikiTextDataset class\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SETUP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE MODEL (TINY!)\n",
    "# ============================================\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=16,        # SMALLER!\n",
    "    num_heads=2,\n",
    "    d_ff=64,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"Model created. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. OPTIMIZER & LOSS\n",
    "# ============================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================\n",
    "# 5. TRAIN!\n",
    "# ============================================\n",
    "\n",
    "# print(\"\\nStarting training...\\n\")\n",
    "# train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
    "\n",
    "# print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f50a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.23 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "MHA Model parameters: 70,559,825\n",
      "GPU Memory after model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear GPU first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=256)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Create PROPER MHA model (bigger than test)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=256,\n",
    "    d_model=512,     # Assignment size\n",
    "    num_heads=8,     # Assignment size  \n",
    "    d_ff=2048,       # Assignment size\n",
    "    num_layers=6     # Assignment size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"MHA Model parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory after model: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afed9b",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and calculate perplexity\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape and calculate loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()  # Back to training mode\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aa804",
   "metadata": {},
   "source": [
    "# Training MHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING MHA MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1146/1146 [01:36<00:00, 11.82it/s, loss=6.6002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 6.6002\n",
      "  Val Loss: 6.2297\n",
      "  Val Perplexity: 507.59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.76it/s, loss=5.6089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 5.6089\n",
      "  Val Loss: 5.8502\n",
      "  Val Perplexity: 347.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.74it/s, loss=5.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 5.0045\n",
      "  Val Loss: 5.6814\n",
      "  Val Perplexity: 293.35\n",
      "\n",
      "============================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MHA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model = mha_model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "        targets_flat = target_ids.view(batch_size * seq_len)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_perplexity = validate_model(mha_model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d841",
   "metadata": {},
   "source": [
    "# Save the MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e78c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA model saved as 'mha_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': 0.0419,\n",
    "    'val_loss': 0.0996,\n",
    "    'val_perplexity': 1.10,\n",
    "}, 'mha_model.pt')\n",
    "\n",
    "print(\"✅ MHA model saved as 'mha_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e73e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The history of\n",
      "Generating...\n",
      "\n",
      "Output: The history of New Hampshire County , it provides a tropical wave that turned into the southeast of June and its southern Africa . The depression was upgraded through two days before the\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: In mathematics,\n",
      "Generating...\n",
      "\n",
      "Output: In mathematics,@ 000 men in the race , allowing several occasions , and a third half of five of 100 miles ( 36 km / h ) . After five hours\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: The cat sat on the\n",
      "Generating...\n",
      "\n",
      "Output: The cat sat on the west of the tower and west bank before entering the intersection with the north that attracts their north is about 12 @.@ 7 miles ( 0 @.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_fixed(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with top-k sampling (more stable)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generating...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # TOP-K SAMPLING (key difference!)\n",
    "            # Only consider top 50 most likely tokens\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test with better sampling\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_fixed(mha_model, tokenizer, prompt, max_length=30, top_k=50, device=device)\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0217fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.11 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "======================================================================\n",
      "TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\n",
      "======================================================================\n",
      "Parameters: 3,399,569\n",
      "Model Size: 3.40M (vs 70.7M before)\n",
      "Size Reduction: 20.8x smaller\n",
      "GPU Memory: 0.10 GB\n",
      "\n",
      "Dataset tokens: 2,347,038\n",
      "Chinchilla optimal params: 117,352\n",
      "Our model params: 3,399,569\n",
      "Ratio (ours/optimal): 29.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=17.2818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 17.2818\n",
      "  Val Loss: 9.4304\n",
      "  Val Perplexity: 12461.92\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 573/573 [00:21<00:00, 26.35it/s, loss=9.0392] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 9.0392\n",
      "  Val Loss: 7.7933\n",
      "  Val Perplexity: 2424.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=7.9895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 7.9895\n",
      "  Val Loss: 7.4918\n",
      "  Val Perplexity: 1793.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=7.6620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 7.6620\n",
      "  Val Loss: 7.3710\n",
      "  Val Perplexity: 1589.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=7.4919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 7.4919\n",
      "  Val Loss: 7.3031\n",
      "  Val Perplexity: 1484.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=7.3856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 7.3856\n",
      "  Val Loss: 7.2611\n",
      "  Val Perplexity: 1423.77\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=7.3077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 7.3077\n",
      "  Val Loss: 7.2225\n",
      "  Val Perplexity: 1369.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=7.2472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 7.2472\n",
      "  Val Loss: 7.1941\n",
      "  Val Perplexity: 1331.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.1967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 7.1967\n",
      "  Val Loss: 7.1640\n",
      "  Val Perplexity: 1292.12\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=7.1518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 7.1518\n",
      "  Val Loss: 7.1366\n",
      "  Val Perplexity: 1257.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=7.1103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11:\n",
      "  Train Loss: 7.1103\n",
      "  Val Loss: 7.1141\n",
      "  Val Perplexity: 1229.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:\n",
      "  Train Loss: 7.0726\n",
      "  Val Loss: 7.0844\n",
      "  Val Perplexity: 1193.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:\n",
      "  Train Loss: 7.0371\n",
      "  Val Loss: 7.0547\n",
      "  Val Perplexity: 1158.31\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:\n",
      "  Train Loss: 7.0039\n",
      "  Val Loss: 7.0269\n",
      "  Val Perplexity: 1126.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.9719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "  Train Loss: 6.9719\n",
      "  Val Loss: 6.9973\n",
      "  Val Perplexity: 1093.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=6.9402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:\n",
      "  Train Loss: 6.9402\n",
      "  Val Loss: 6.9682\n",
      "  Val Perplexity: 1062.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.9086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:\n",
      "  Train Loss: 6.9086\n",
      "  Val Loss: 6.9382\n",
      "  Val Perplexity: 1030.94\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.8775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:\n",
      "  Train Loss: 6.8775\n",
      "  Val Loss: 6.9086\n",
      "  Val Perplexity: 1000.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.8468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:\n",
      "  Train Loss: 6.8468\n",
      "  Val Loss: 6.8807\n",
      "  Val Perplexity: 973.33\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/300: 100%|██████████| 573/573 [00:22<00:00, 25.78it/s, loss=6.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Train Loss: 6.8150\n",
      "  Val Loss: 6.8526\n",
      "  Val Perplexity: 946.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=6.7826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:\n",
      "  Train Loss: 6.7826\n",
      "  Val Loss: 6.8194\n",
      "  Val Perplexity: 915.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=6.7494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:\n",
      "  Train Loss: 6.7494\n",
      "  Val Loss: 6.7898\n",
      "  Val Perplexity: 888.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.7157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23:\n",
      "  Train Loss: 6.7157\n",
      "  Val Loss: 6.7597\n",
      "  Val Perplexity: 862.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/300: 100%|██████████| 573/573 [00:22<00:00, 25.97it/s, loss=6.6823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:\n",
      "  Train Loss: 6.6823\n",
      "  Val Loss: 6.7281\n",
      "  Val Perplexity: 835.59\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=6.6478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:\n",
      "  Train Loss: 6.6478\n",
      "  Val Loss: 6.6979\n",
      "  Val Perplexity: 810.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=6.6135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:\n",
      "  Train Loss: 6.6135\n",
      "  Val Loss: 6.6672\n",
      "  Val Perplexity: 786.16\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=6.5799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:\n",
      "  Train Loss: 6.5799\n",
      "  Val Loss: 6.6371\n",
      "  Val Perplexity: 762.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.5470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:\n",
      "  Train Loss: 6.5470\n",
      "  Val Loss: 6.6070\n",
      "  Val Perplexity: 740.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.5139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:\n",
      "  Train Loss: 6.5139\n",
      "  Val Loss: 6.5786\n",
      "  Val Perplexity: 719.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.4835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:\n",
      "  Train Loss: 6.4835\n",
      "  Val Loss: 6.5555\n",
      "  Val Perplexity: 703.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=6.4533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:\n",
      "  Train Loss: 6.4533\n",
      "  Val Loss: 6.5314\n",
      "  Val Perplexity: 686.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=6.4249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:\n",
      "  Train Loss: 6.4249\n",
      "  Val Loss: 6.5110\n",
      "  Val Perplexity: 672.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=6.3976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:\n",
      "  Train Loss: 6.3976\n",
      "  Val Loss: 6.4873\n",
      "  Val Perplexity: 656.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=6.3709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:\n",
      "  Train Loss: 6.3709\n",
      "  Val Loss: 6.4699\n",
      "  Val Perplexity: 645.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:\n",
      "  Train Loss: 6.3447\n",
      "  Val Loss: 6.4494\n",
      "  Val Perplexity: 632.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.3202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:\n",
      "  Train Loss: 6.3202\n",
      "  Val Loss: 6.4301\n",
      "  Val Perplexity: 620.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.2953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:\n",
      "  Train Loss: 6.2953\n",
      "  Val Loss: 6.4136\n",
      "  Val Perplexity: 610.11\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=6.2716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:\n",
      "  Train Loss: 6.2716\n",
      "  Val Loss: 6.3966\n",
      "  Val Perplexity: 599.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.2481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:\n",
      "  Train Loss: 6.2481\n",
      "  Val Loss: 6.3787\n",
      "  Val Perplexity: 589.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.2248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:\n",
      "  Train Loss: 6.2248\n",
      "  Val Loss: 6.3651\n",
      "  Val Perplexity: 581.18\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=6.2026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:\n",
      "  Train Loss: 6.2026\n",
      "  Val Loss: 6.3507\n",
      "  Val Perplexity: 572.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=6.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:\n",
      "  Train Loss: 6.1795\n",
      "  Val Loss: 6.3363\n",
      "  Val Perplexity: 564.69\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=6.1574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:\n",
      "  Train Loss: 6.1574\n",
      "  Val Loss: 6.3197\n",
      "  Val Perplexity: 555.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.1353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:\n",
      "  Train Loss: 6.1353\n",
      "  Val Loss: 6.3071\n",
      "  Val Perplexity: 548.47\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.1128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:\n",
      "  Train Loss: 6.1128\n",
      "  Val Loss: 6.2887\n",
      "  Val Perplexity: 538.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/300: 100%|██████████| 573/573 [00:22<00:00, 25.87it/s, loss=6.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:\n",
      "  Train Loss: 6.0909\n",
      "  Val Loss: 6.2757\n",
      "  Val Perplexity: 531.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:\n",
      "  Train Loss: 6.0695\n",
      "  Val Loss: 6.2647\n",
      "  Val Perplexity: 525.70\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=6.0470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:\n",
      "  Train Loss: 6.0470\n",
      "  Val Loss: 6.2507\n",
      "  Val Perplexity: 518.40\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49:\n",
      "  Train Loss: 6.0255\n",
      "  Val Loss: 6.2357\n",
      "  Val Perplexity: 510.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50:\n",
      "  Train Loss: 6.0039\n",
      "  Val Loss: 6.2240\n",
      "  Val Perplexity: 504.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.9823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51:\n",
      "  Train Loss: 5.9823\n",
      "  Val Loss: 6.2094\n",
      "  Val Perplexity: 497.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.9610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52:\n",
      "  Train Loss: 5.9610\n",
      "  Val Loss: 6.1995\n",
      "  Val Perplexity: 492.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=5.9388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53:\n",
      "  Train Loss: 5.9388\n",
      "  Val Loss: 6.1850\n",
      "  Val Perplexity: 485.41\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.9185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54:\n",
      "  Train Loss: 5.9185\n",
      "  Val Loss: 6.1737\n",
      "  Val Perplexity: 479.98\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.8972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55:\n",
      "  Train Loss: 5.8972\n",
      "  Val Loss: 6.1587\n",
      "  Val Perplexity: 472.83\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.8764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56:\n",
      "  Train Loss: 5.8764\n",
      "  Val Loss: 6.1517\n",
      "  Val Perplexity: 469.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.8560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57:\n",
      "  Train Loss: 5.8560\n",
      "  Val Loss: 6.1389\n",
      "  Val Perplexity: 463.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.8352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58:\n",
      "  Train Loss: 5.8352\n",
      "  Val Loss: 6.1257\n",
      "  Val Perplexity: 457.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59:\n",
      "  Train Loss: 5.8150\n",
      "  Val Loss: 6.1153\n",
      "  Val Perplexity: 452.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.7939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60:\n",
      "  Train Loss: 5.7939\n",
      "  Val Loss: 6.1075\n",
      "  Val Perplexity: 449.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=5.7743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61:\n",
      "  Train Loss: 5.7743\n",
      "  Val Loss: 6.0926\n",
      "  Val Perplexity: 442.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=5.7544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62:\n",
      "  Train Loss: 5.7544\n",
      "  Val Loss: 6.0851\n",
      "  Val Perplexity: 439.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.7345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63:\n",
      "  Train Loss: 5.7345\n",
      "  Val Loss: 6.0757\n",
      "  Val Perplexity: 435.15\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.7148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64:\n",
      "  Train Loss: 5.7148\n",
      "  Val Loss: 6.0636\n",
      "  Val Perplexity: 429.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65:\n",
      "  Train Loss: 5.6955\n",
      "  Val Loss: 6.0552\n",
      "  Val Perplexity: 426.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.6763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66:\n",
      "  Train Loss: 5.6763\n",
      "  Val Loss: 6.0447\n",
      "  Val Perplexity: 421.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.6568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67:\n",
      "  Train Loss: 5.6568\n",
      "  Val Loss: 6.0344\n",
      "  Val Perplexity: 417.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.6381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68:\n",
      "  Train Loss: 5.6381\n",
      "  Val Loss: 6.0232\n",
      "  Val Perplexity: 412.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69:\n",
      "  Train Loss: 5.6185\n",
      "  Val Loss: 6.0126\n",
      "  Val Perplexity: 408.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=5.5998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70:\n",
      "  Train Loss: 5.5998\n",
      "  Val Loss: 6.0051\n",
      "  Val Perplexity: 405.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.5808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71:\n",
      "  Train Loss: 5.5808\n",
      "  Val Loss: 5.9956\n",
      "  Val Perplexity: 401.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72:\n",
      "  Train Loss: 5.5614\n",
      "  Val Loss: 5.9816\n",
      "  Val Perplexity: 396.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.5438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73:\n",
      "  Train Loss: 5.5438\n",
      "  Val Loss: 5.9745\n",
      "  Val Perplexity: 393.26\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.5256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74:\n",
      "  Train Loss: 5.5256\n",
      "  Val Loss: 5.9710\n",
      "  Val Perplexity: 391.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.5069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75:\n",
      "  Train Loss: 5.5069\n",
      "  Val Loss: 5.9571\n",
      "  Val Perplexity: 386.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=5.4891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76:\n",
      "  Train Loss: 5.4891\n",
      "  Val Loss: 5.9491\n",
      "  Val Perplexity: 383.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/300: 100%|██████████| 573/573 [00:22<00:00, 25.88it/s, loss=5.4718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77:\n",
      "  Train Loss: 5.4718\n",
      "  Val Loss: 5.9448\n",
      "  Val Perplexity: 381.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.4534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78:\n",
      "  Train Loss: 5.4534\n",
      "  Val Loss: 5.9369\n",
      "  Val Perplexity: 378.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.4358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79:\n",
      "  Train Loss: 5.4358\n",
      "  Val Loss: 5.9293\n",
      "  Val Perplexity: 375.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=5.4189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80:\n",
      "  Train Loss: 5.4189\n",
      "  Val Loss: 5.9201\n",
      "  Val Perplexity: 372.46\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.4020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81:\n",
      "  Train Loss: 5.4020\n",
      "  Val Loss: 5.9130\n",
      "  Val Perplexity: 369.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/300: 100%|██████████| 573/573 [00:21<00:00, 26.29it/s, loss=5.3851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82:\n",
      "  Train Loss: 5.3851\n",
      "  Val Loss: 5.9069\n",
      "  Val Perplexity: 367.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=5.3686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83:\n",
      "  Train Loss: 5.3686\n",
      "  Val Loss: 5.8972\n",
      "  Val Perplexity: 364.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.3516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84:\n",
      "  Train Loss: 5.3516\n",
      "  Val Loss: 5.8922\n",
      "  Val Perplexity: 362.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=5.3350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85:\n",
      "  Train Loss: 5.3350\n",
      "  Val Loss: 5.8852\n",
      "  Val Perplexity: 359.67\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.3195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86:\n",
      "  Train Loss: 5.3195\n",
      "  Val Loss: 5.8777\n",
      "  Val Perplexity: 357.00\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.3030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87:\n",
      "  Train Loss: 5.3030\n",
      "  Val Loss: 5.8724\n",
      "  Val Perplexity: 355.09\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.2877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88:\n",
      "  Train Loss: 5.2877\n",
      "  Val Loss: 5.8661\n",
      "  Val Perplexity: 352.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.2721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89:\n",
      "  Train Loss: 5.2721\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 351.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90:\n",
      "  Train Loss: 5.2560\n",
      "  Val Loss: 5.8551\n",
      "  Val Perplexity: 349.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.2417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91:\n",
      "  Train Loss: 5.2417\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.27\n",
      "  📊 Best model still at Epoch 90 (Val Loss: 5.8551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.2266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92:\n",
      "  Train Loss: 5.2266\n",
      "  Val Loss: 5.8484\n",
      "  Val Perplexity: 346.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.2121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93:\n",
      "  Train Loss: 5.2121\n",
      "  Val Loss: 5.8430\n",
      "  Val Perplexity: 344.82\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.1973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94:\n",
      "  Train Loss: 5.1973\n",
      "  Val Loss: 5.8386\n",
      "  Val Perplexity: 343.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/300: 100%|██████████| 573/573 [00:21<00:00, 26.26it/s, loss=5.1825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95:\n",
      "  Train Loss: 5.1825\n",
      "  Val Loss: 5.8325\n",
      "  Val Perplexity: 341.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96:\n",
      "  Train Loss: 5.1687\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.74\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97:\n",
      "  Train Loss: 5.1546\n",
      "  Val Loss: 5.8275\n",
      "  Val Perplexity: 339.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=5.1403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98:\n",
      "  Train Loss: 5.1403\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.76\n",
      "  📊 Best model still at Epoch 97 (Val Loss: 5.8275)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.1272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99:\n",
      "  Train Loss: 5.1272\n",
      "  Val Loss: 5.8204\n",
      "  Val Perplexity: 337.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100:\n",
      "  Train Loss: 5.1135\n",
      "  Val Loss: 5.8191\n",
      "  Val Perplexity: 336.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.1004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101:\n",
      "  Train Loss: 5.1004\n",
      "  Val Loss: 5.8183\n",
      "  Val Perplexity: 336.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102:\n",
      "  Train Loss: 5.0872\n",
      "  Val Loss: 5.8113\n",
      "  Val Perplexity: 334.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.0753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103:\n",
      "  Train Loss: 5.0753\n",
      "  Val Loss: 5.8030\n",
      "  Val Perplexity: 331.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104:\n",
      "  Train Loss: 5.0623\n",
      "  Val Loss: 5.8014\n",
      "  Val Perplexity: 330.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.0495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105:\n",
      "  Train Loss: 5.0495\n",
      "  Val Loss: 5.8080\n",
      "  Val Perplexity: 332.96\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=5.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106:\n",
      "  Train Loss: 5.0374\n",
      "  Val Loss: 5.8018\n",
      "  Val Perplexity: 330.91\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107:\n",
      "  Train Loss: 5.0251\n",
      "  Val Loss: 5.7969\n",
      "  Val Perplexity: 329.27\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108:\n",
      "  Train Loss: 5.0142\n",
      "  Val Loss: 5.7983\n",
      "  Val Perplexity: 329.75\n",
      "  📊 Best model still at Epoch 107 (Val Loss: 5.7969)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109:\n",
      "  Train Loss: 5.0020\n",
      "  Val Loss: 5.7950\n",
      "  Val Perplexity: 328.65\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110:\n",
      "  Train Loss: 4.9903\n",
      "  Val Loss: 5.7906\n",
      "  Val Perplexity: 327.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.9797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111:\n",
      "  Train Loss: 4.9797\n",
      "  Val Loss: 5.7909\n",
      "  Val Perplexity: 327.29\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112:\n",
      "  Train Loss: 4.9691\n",
      "  Val Loss: 5.7951\n",
      "  Val Perplexity: 328.68\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.9580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113:\n",
      "  Train Loss: 4.9580\n",
      "  Val Loss: 5.7962\n",
      "  Val Perplexity: 329.06\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.9469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114:\n",
      "  Train Loss: 4.9469\n",
      "  Val Loss: 5.7853\n",
      "  Val Perplexity: 325.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=4.9363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115:\n",
      "  Train Loss: 4.9363\n",
      "  Val Loss: 5.7926\n",
      "  Val Perplexity: 327.85\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116:\n",
      "  Train Loss: 4.9268\n",
      "  Val Loss: 5.7856\n",
      "  Val Perplexity: 325.58\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117:\n",
      "  Train Loss: 4.9171\n",
      "  Val Loss: 5.7922\n",
      "  Val Perplexity: 327.74\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.9066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118:\n",
      "  Train Loss: 4.9066\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.8967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119:\n",
      "  Train Loss: 4.8967\n",
      "  Val Loss: 5.7872\n",
      "  Val Perplexity: 326.09\n",
      "  📊 Best model still at Epoch 118 (Val Loss: 5.7850)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120:\n",
      "  Train Loss: 4.8883\n",
      "  Val Loss: 5.7828\n",
      "  Val Perplexity: 324.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=4.8788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121:\n",
      "  Train Loss: 4.8788\n",
      "  Val Loss: 5.7842\n",
      "  Val Perplexity: 325.11\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.8692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122:\n",
      "  Train Loss: 4.8692\n",
      "  Val Loss: 5.7858\n",
      "  Val Perplexity: 325.65\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.8605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123:\n",
      "  Train Loss: 4.8605\n",
      "  Val Loss: 5.7808\n",
      "  Val Perplexity: 324.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124:\n",
      "  Train Loss: 4.8523\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.27\n",
      "  📊 Best model still at Epoch 123 (Val Loss: 5.7808)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=4.8435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125:\n",
      "  Train Loss: 4.8435\n",
      "  Val Loss: 5.7782\n",
      "  Val Perplexity: 323.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.8349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126:\n",
      "  Train Loss: 4.8349\n",
      "  Val Loss: 5.7851\n",
      "  Val Perplexity: 325.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.8264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127:\n",
      "  Train Loss: 4.8264\n",
      "  Val Loss: 5.7815\n",
      "  Val Perplexity: 324.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.8189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128:\n",
      "  Train Loss: 4.8189\n",
      "  Val Loss: 5.7861\n",
      "  Val Perplexity: 325.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.8105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129:\n",
      "  Train Loss: 4.8105\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=4.8032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130:\n",
      "  Train Loss: 4.8032\n",
      "  Val Loss: 5.7830\n",
      "  Val Perplexity: 324.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.7954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131:\n",
      "  Train Loss: 4.7954\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.29\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=4.7874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132:\n",
      "  Train Loss: 4.7874\n",
      "  Val Loss: 5.7833\n",
      "  Val Perplexity: 324.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.7810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133:\n",
      "  Train Loss: 4.7810\n",
      "  Val Loss: 5.7840\n",
      "  Val Perplexity: 325.07\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134:\n",
      "  Train Loss: 4.7739\n",
      "  Val Loss: 5.7885\n",
      "  Val Perplexity: 326.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.7667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135:\n",
      "  Train Loss: 4.7667\n",
      "  Val Loss: 5.7910\n",
      "  Val Perplexity: 327.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136:\n",
      "  Train Loss: 4.7597\n",
      "  Val Loss: 5.7893\n",
      "  Val Perplexity: 326.79\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.7533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137:\n",
      "  Train Loss: 4.7533\n",
      "  Val Loss: 5.7864\n",
      "  Val Perplexity: 325.85\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138:\n",
      "  Train Loss: 4.7473\n",
      "  Val Loss: 5.7855\n",
      "  Val Perplexity: 325.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139:\n",
      "  Train Loss: 4.7406\n",
      "  Val Loss: 5.7886\n",
      "  Val Perplexity: 326.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/300: 100%|██████████| 573/573 [00:22<00:00, 25.83it/s, loss=4.7337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140:\n",
      "  Train Loss: 4.7337\n",
      "  Val Loss: 5.7882\n",
      "  Val Perplexity: 326.43\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/300: 100%|██████████| 573/573 [00:22<00:00, 25.94it/s, loss=4.7287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141:\n",
      "  Train Loss: 4.7287\n",
      "  Val Loss: 5.7875\n",
      "  Val Perplexity: 326.19\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.7231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142:\n",
      "  Train Loss: 4.7231\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143:\n",
      "  Train Loss: 4.7168\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144:\n",
      "  Train Loss: 4.7111\n",
      "  Val Loss: 5.7941\n",
      "  Val Perplexity: 328.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.7053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145:\n",
      "  Train Loss: 4.7053\n",
      "  Val Loss: 5.7961\n",
      "  Val Perplexity: 329.02\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=4.7015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146:\n",
      "  Train Loss: 4.7015\n",
      "  Val Loss: 5.7959\n",
      "  Val Perplexity: 328.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=4.6960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147:\n",
      "  Train Loss: 4.6960\n",
      "  Val Loss: 5.7968\n",
      "  Val Perplexity: 329.23\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=4.6905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148:\n",
      "  Train Loss: 4.6905\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.84\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.6853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149:\n",
      "  Train Loss: 4.6853\n",
      "  Val Loss: 5.8067\n",
      "  Val Perplexity: 332.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150:\n",
      "  Train Loss: 4.6807\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.6765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151:\n",
      "  Train Loss: 4.6765\n",
      "  Val Loss: 5.8083\n",
      "  Val Perplexity: 333.06\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152:\n",
      "  Train Loss: 4.6715\n",
      "  Val Loss: 5.8106\n",
      "  Val Perplexity: 333.81\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.6671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153:\n",
      "  Train Loss: 4.6671\n",
      "  Val Loss: 5.8089\n",
      "  Val Perplexity: 333.27\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154:\n",
      "  Train Loss: 4.6625\n",
      "  Val Loss: 5.8132\n",
      "  Val Perplexity: 334.67\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.6580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155:\n",
      "  Train Loss: 4.6580\n",
      "  Val Loss: 5.8153\n",
      "  Val Perplexity: 335.40\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156:\n",
      "  Train Loss: 4.6547\n",
      "  Val Loss: 5.8108\n",
      "  Val Perplexity: 333.88\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/300: 100%|██████████| 573/573 [00:22<00:00, 25.81it/s, loss=4.6494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157:\n",
      "  Train Loss: 4.6494\n",
      "  Val Loss: 5.8130\n",
      "  Val Perplexity: 334.63\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158:\n",
      "  Train Loss: 4.6463\n",
      "  Val Loss: 5.8192\n",
      "  Val Perplexity: 336.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/300: 100%|██████████| 573/573 [00:22<00:00, 25.86it/s, loss=4.6418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159:\n",
      "  Train Loss: 4.6418\n",
      "  Val Loss: 5.8141\n",
      "  Val Perplexity: 335.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160:\n",
      "  Train Loss: 4.6382\n",
      "  Val Loss: 5.8186\n",
      "  Val Perplexity: 336.52\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161:\n",
      "  Train Loss: 4.6342\n",
      "  Val Loss: 5.8263\n",
      "  Val Perplexity: 339.10\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.6309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162:\n",
      "  Train Loss: 4.6309\n",
      "  Val Loss: 5.8299\n",
      "  Val Perplexity: 340.32\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.6272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163:\n",
      "  Train Loss: 4.6272\n",
      "  Val Loss: 5.8290\n",
      "  Val Perplexity: 340.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164:\n",
      "  Train Loss: 4.6243\n",
      "  Val Loss: 5.8307\n",
      "  Val Perplexity: 340.58\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.6207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165:\n",
      "  Train Loss: 4.6207\n",
      "  Val Loss: 5.8317\n",
      "  Val Perplexity: 340.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166:\n",
      "  Train Loss: 4.6168\n",
      "  Val Loss: 5.8319\n",
      "  Val Perplexity: 341.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/300: 100%|██████████| 573/573 [00:22<00:00, 25.92it/s, loss=4.6140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167:\n",
      "  Train Loss: 4.6140\n",
      "  Val Loss: 5.8300\n",
      "  Val Perplexity: 340.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/300: 100%|██████████| 573/573 [00:22<00:00, 25.61it/s, loss=4.6106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168:\n",
      "  Train Loss: 4.6106\n",
      "  Val Loss: 5.8423\n",
      "  Val Perplexity: 344.56\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=4.6070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169:\n",
      "  Train Loss: 4.6070\n",
      "  Val Loss: 5.8412\n",
      "  Val Perplexity: 344.20\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/300: 100%|██████████| 573/573 [00:22<00:00, 25.74it/s, loss=4.6043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170:\n",
      "  Train Loss: 4.6043\n",
      "  Val Loss: 5.8384\n",
      "  Val Perplexity: 343.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171:\n",
      "  Train Loss: 4.6021\n",
      "  Val Loss: 5.8431\n",
      "  Val Perplexity: 344.83\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.5984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172:\n",
      "  Train Loss: 4.5984\n",
      "  Val Loss: 5.8439\n",
      "  Val Perplexity: 345.13\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/300: 100%|██████████| 573/573 [00:22<00:00, 25.91it/s, loss=4.5964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173:\n",
      "  Train Loss: 4.5964\n",
      "  Val Loss: 5.8477\n",
      "  Val Perplexity: 346.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.5928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174:\n",
      "  Train Loss: 4.5928\n",
      "  Val Loss: 5.8537\n",
      "  Val Perplexity: 348.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175:\n",
      "  Train Loss: 4.5904\n",
      "  Val Loss: 5.8522\n",
      "  Val Perplexity: 348.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/300: 100%|██████████| 573/573 [00:22<00:00, 25.85it/s, loss=4.5878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176:\n",
      "  Train Loss: 4.5878\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 350.99\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.5843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177:\n",
      "  Train Loss: 4.5843\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.57\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.5822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178:\n",
      "  Train Loss: 4.5822\n",
      "  Val Loss: 5.8606\n",
      "  Val Perplexity: 350.92\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.5791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179:\n",
      "  Train Loss: 4.5791\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.59\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.5771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180:\n",
      "  Train Loss: 4.5771\n",
      "  Val Loss: 5.8684\n",
      "  Val Perplexity: 353.69\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.5752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181:\n",
      "  Train Loss: 4.5752\n",
      "  Val Loss: 5.8683\n",
      "  Val Perplexity: 353.64\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182:\n",
      "  Train Loss: 4.5727\n",
      "  Val Loss: 5.8679\n",
      "  Val Perplexity: 353.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=4.5705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183:\n",
      "  Train Loss: 4.5705\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.25\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.5688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184:\n",
      "  Train Loss: 4.5688\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185:\n",
      "  Train Loss: 4.5664\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.5638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186:\n",
      "  Train Loss: 4.5638\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187:\n",
      "  Train Loss: 4.5614\n",
      "  Val Loss: 5.8798\n",
      "  Val Perplexity: 357.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/300:  27%|██▋       | 156/573 [00:05<00:15, 26.18it/s, loss=4.5186]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRatio (ours/optimal): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_params/optimal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Train for 300 epochs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m best_val_loss, best_epoch = \u001b[43mtrain_model_properly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmha_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMHA TRAINING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain_model_properly\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Backward with gradient clipping\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\u001b[39;00m\n\u001b[32m     93\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# Prevents exploding gradients\u001b[39;00m\n\u001b[32m     96\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Better Training with RIGHT MODEL SIZE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CLEAR GPU AND START FRESH\n",
    "# ============================================\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# BETTER HYPERPARAMETERS (Less Overfitting)\n",
    "# ============================================\n",
    "\n",
    "# Create datasets with LONGER sequences\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "# ============================================\n",
    "# BETTER TRAINING FUNCTION (WITH BEST MODEL SAVING!)\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ============================================\n",
    "        # TRAINING PHASE\n",
    "        # ============================================\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            # NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # ============================================\n",
    "        # VALIDATION PHASE\n",
    "        # ============================================\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # SAVE BEST MODEL (THE KEY FIX!)\n",
    "        # ============================================\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "            \n",
    "            print(f\"  ✅ Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            print(f\"  📊 Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return best_val_loss, best_epoch\n",
    "\n",
    "# ============================================\n",
    "# TRAIN MHA MODEL WITH OPTIMAL SIZE (1.8M params)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# mha_model = LanguageModel(\n",
    "#     vocab_size=50257,\n",
    "#     max_seq_len=512,\n",
    "#     d_model=192,        # Reduced from 512 → 192\n",
    "#     num_heads=4,        # Reduced from 8 → 4\n",
    "#     d_ff=768,           # Reduced from 2048 → 768\n",
    "#     num_layers=4,       # Reduced from 6 → 4\n",
    "#     dropout=0.2         # Kept at 0.2\n",
    "# ).to(device)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=64,        # SMALLER!\n",
    "    num_heads=4,\n",
    "    d_ff=256,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_params = sum(p.numel() for p in mha_model.parameters())\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Model Size: {num_params/1e6:.2f}M (vs 70.7M before)\")\n",
    "print(f\"Size Reduction: {70.7/(num_params/1e6):.1f}x smaller\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\\n\")\n",
    "\n",
    "# Chinchilla check\n",
    "tokens = 2_347_038\n",
    "optimal_params = tokens / 20\n",
    "print(f\"Dataset tokens: {tokens:,}\")\n",
    "print(f\"Chinchilla optimal params: {optimal_params:,.0f}\")\n",
    "print(f\"Our model params: {num_params:,}\")\n",
    "print(f\"Ratio (ours/optimal): {num_params/optimal_params:.1f}x\")\n",
    "\n",
    "\n",
    "# Train for 300 epochs\n",
    "best_val_loss, best_epoch = train_model_properly(\n",
    "    mha_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=300\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA TRAINING COMPLETE!\")\n",
    "print(f\"Best model saved at Epoch {best_epoch}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04cfc5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING MHA MODEL GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of\n",
      "Output: The history of The New Age , the New York and W.com ranked in Ireland , with the 2012 . The United Nations and one @-@ year , in Canada ranked Billboard Hot 100 in the third consecutive weeks\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 000 and federal federal federal federal federal government , the Government departments , and the government .= = State = Post = = = = =Although the government Government Act of of the executive Department is the\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the most significant in the same time of the name of a small species , including the name , from the age of 1611th century . Another new genera was the Middle Middle Trinitys in the English\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING MHA MODEL GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_proper(mha_model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5bd7",
   "metadata": {},
   "source": [
    "# Full Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2301e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Memory: 0.03 GB\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Creating datasets...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 115,716,078\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 7063\n",
      "Val batches: 15\n",
      "\n",
      "Creating model...\n",
      "Parameters: 869,153\n",
      "GPU Memory: 0.03 GB\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=8.2508] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 8.2508\n",
      "  Val Loss: 7.2842\n",
      "  Val Perplexity: 1457.12\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=7.1734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 7.1734\n",
      "  Val Loss: 6.9867\n",
      "  Val Perplexity: 1082.15\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  15%|█▍        | 1046/7063 [02:15<13:01,  7.70it/s, loss=7.0350]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Causal masking\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ============================================\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30, patience=20):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Track metrics history for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    # Early stopping tracking\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "        # Save best model and track early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "\n",
    "            print(f\"  Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "            print(f\"  Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! No improvement for {patience} consecutive epochs.\")\n",
    "            print(f\"   Best model was at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def plot_metrics(metrics_history, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot all training metrics including train loss, val loss, and val perplexity.\n",
    "\n",
    "    Args:\n",
    "        metrics_history: Dictionary containing training metrics\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    train_losses = metrics_history['train_losses']\n",
    "    val_losses = metrics_history['val_losses']\n",
    "    val_perplexities = metrics_history['val_perplexities']\n",
    "    best_epoch = metrics_history['best_epoch']\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    axes[0, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 1].scatter([best_epoch], [metrics_history['best_val_loss']],\n",
    "                       color='r', s=100, zorder=5, label='Best Val Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Validation Loss over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Validation Perplexity\n",
    "    axes[1, 0].plot(epochs, val_perplexities, 'purple', linewidth=2, label='Val Perplexity')\n",
    "    axes[1, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Validation Perplexity over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Train vs Validation Loss Comparison\n",
    "    axes[1, 1].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[1, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[1, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Train vs Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nTraining metrics plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "    val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        max_seq_len=512,\n",
    "        d_model=16,\n",
    "        num_heads=2,\n",
    "        d_ff=64,\n",
    "        num_layers=2,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    metrics_history = train_model_properly(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10\n",
    "    )\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PLOTTING TRAINING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    plot_metrics(metrics_history, save_path='training_metrics.png')\n",
    "\n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompts = [\n",
    "        \"The history of India is \",\n",
    "        \"In mathematics,\",\n",
    "        \"The cat sat on the\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text_proper(model, tokenizer, prompt, max_length=40, device=device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generated}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ec5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
