{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.8741,  1.4167,  1.5167], requires_grad=True)\n",
      "Output: tensor([-0.8741,  2.8334,  4.5501], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([2, 10])\n",
      "Output embeddings shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "#Create\n",
    "vocab_size = 1000  # Size of the vocabulary\n",
    "d_model = 512    # Dimension of the embeddings\n",
    "token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "#Test\n",
    "token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "output= token_embed (token_ids)\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output position embeddings shape: torch.Size([10, 512])\n",
      "Positions Used: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "#Create\n",
    "pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "output = pos_embed(10)  # Example input sequence length\n",
    "print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98afba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# Test\n",
    "embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "output = embeddings(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: 0.0131\n",
      "  Std: 0.9961\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: -0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create FFN\n",
    "ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "Multi-head attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "✅ Transformer Block works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# Forward pass\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Number of blocks: 6\n"
     ]
    }
   ],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create model with 6 stacked blocks\n",
    "model = Transformer(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,  # 6 transformer blocks!\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "output = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# Create complete language model\n",
    "model = LanguageModel(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))\n",
    "logits = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, hf_transfer, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [datasets]/23\u001b[0m [datasets]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 hf_transfer-0.1.9 huggingface-hub-0.36.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 regex-2025.11.3 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63110246f574d3b9619f91106e284e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb9d20f814d4e2babd2e6f3d54289ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d914ea77d1245efba483e244e0e13f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2205567f7a6e4569a98bc6a72a3d1ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725d7499e2d49a688659c3fb1831a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045fdf868dc14f05982f875a6b09e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ee7af13c564dfa90f7fc6410cff48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75181c8aab614c9abad954eca1c2317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406fef5760b41559009fc5c12ab09c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082a49dfc5a44d5389cf08218021987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ddf0c3c500446b82278509620e8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee748b27664c308f84070ffd8eb5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create our dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_dataset = \u001b[43mWikiTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get first sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mWikiTextDataset.__init__\u001b[39m\u001b[34m(self, data, tokenizer, max_length)\u001b[39m\n\u001b[32m     19\u001b[39m     text = example[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Skip empty lines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         all_tokens.extend(tokens)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Step 2: Convert to PyTorch tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2732\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2694\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2695\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2715\u001b[39m     **kwargs,\n\u001b[32m   2716\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2717\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2718\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2719\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2730\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     encoded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3123\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3096\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3111\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3114\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3115\u001b[39m     padding=padding,\n\u001b[32m   3116\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m     **kwargs,\n\u001b[32m   3121\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3142\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2.py:281\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8c895",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "```\n",
    "Loss(So loss is computed for each token)(Define the Loss)\n",
    "↓\n",
    "Optimizer(Optimizer updates the model's weights to minimize loss.)\n",
    "↓\n",
    "Training Loop(Forward-Loss-clear Gradients-Backward-Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 5])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "\n",
      "After reshaping:\n",
      "Logits flat: torch.Size([6, 5])\n",
      "Targets flat: torch.Size([6])\n",
      "\n",
      "Loss: 2.0549\n"
     ]
    }
   ],
   "source": [
    "#Define the Loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake model output (logits)\n",
    "# [batch=2, seq=3, vocab_size=5]\n",
    "logits = torch.randn(2, 3, 5)\n",
    "\n",
    "# Target token IDs\n",
    "# [batch=2, seq=3]\n",
    "targets = torch.tensor([\n",
    "    [2, 4, 1],  # Sequence 1: correct tokens are 2, 4, 1\n",
    "    [0, 3, 2]   # Sequence 2: correct tokens are 0, 3, 2\n",
    "])\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)    # [2, 3, 5]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# BUT! CrossEntropyLoss expects:\n",
    "# logits: [N, vocab_size] where N = batch * seq\n",
    "# targets: [N]\n",
    "\n",
    "# So we need to reshape!\n",
    "logits_flat = logits.view(-1, 5)      # [6, 5]  (2*3=6 positions)\n",
    "targets_flat = targets.view(-1)        # [6]\n",
    "\n",
    "print(\"\\nAfter reshaping:\")\n",
    "print(\"Logits flat:\", logits_flat.shape)   # [6, 5]\n",
    "print(\"Targets flat:\", targets_flat.shape) # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Logits shape: torch.Size([32, 512, 50257])\n",
      "\n",
      "Flattened logits: torch.Size([16384, 50257])\n",
      "Flattened targets: torch.Size([16384])\n",
      "\n",
      "Loss: 11.0243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate model training\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Get a batch from dataloader\n",
    "input_ids, target_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")    # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Logits shape: {logits.shape}\")      # [32, 512, 50257]\n",
    "\n",
    "# Reshape for loss calculation\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "logits_flat = logits.view(batch_size * seq_len, vocab_size)  # [16384, 50257]\n",
    "targets_flat = target_ids.view(batch_size * seq_len)         # [16384]\n",
    "\n",
    "print(f\"\\nFlattened logits: {logits_flat.shape}\")\n",
    "print(f\"Flattened targets: {targets_flat.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created!\n",
      "Learning rate: 3e-4 = 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),  # All model weights to optimize\n",
    "    lr=3e-4,            # Learning rate (how big each update is)\n",
    "    weight_decay=0.01   # Regularization (prevents overfitting)\n",
    ")\n",
    "\n",
    "print(\"Optimizer created!\")\n",
    "print(f\"Learning rate: 3e-4 = {3e-4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fca023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Training ===\n",
      "Weight sample: -0.2838\n",
      "Loss: 1.8065\n",
      "Gradient sample: -0.151754\n",
      "=== After Training ===\n",
      "Weight sample: -0.2828\n",
      "Weight changed! ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(2, 10)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()           # Clear old gradients\n",
    "output = model(x)               # Forward\n",
    "loss = criterion(output, targets)  # Loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()                 # Calculate gradients\n",
    "print(f\"Gradient sample: {model.weight.grad[0, 0].item():.6f}\")\n",
    "\n",
    "optimizer.step()                # Update weights\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "print(\"Weight changed! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2794887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the language model\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            # Move data to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)  # [batch, seq, vocab]\n",
    "            \n",
    "            # Reshape for loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bf95b",
   "metadata": {},
   "source": [
    "# Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a23b0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after clearing: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after cleanup: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete ALL variables\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "del train_loader\n",
    "del train_dataset\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22332564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Check GPU is clean\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  # Should be 0!\n",
    "\n",
    "# 3. Load data (only what we need)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Continue to next step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde39b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial GPU Memory: 0.00 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Dataset ready. GPU Memory: 0.00 GB\n",
      "Model created. GPU Memory: 0.05 GB\n",
      "Parameters: 13,329,233\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 9168/9168 [02:25<00:00, 62.92it/s, loss=6.7767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 6.7767\n",
      "TRAINING COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# ============================================\n",
    "# 1. COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "# MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        #This initializes the linear layers for query, key, value and output projections\n",
    "        #Linear layers are just weight matrices that will be learned during training\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Dropout layer for regularization this is used to prevent overfitting by randomly setting some of the attention weights to zero during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Here we r unpack the input tensor x to get batch size and sequence length\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        #Here we r passing the input x through the linear layers to get the query, key, and value matrices\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        # Now we reshape Q, K, V to separate the heads for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        #NOTE: Causal masking is essential in autoregressive models to ensure that each token can only attend to previous tokens and itself, preventing any \"future\" information leakage.\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# FeedForward class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "# LanguageModel class\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# WikiTextDataset class\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SETUP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE MODEL (TINY!)\n",
    "# ============================================\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=128,\n",
    "    d_model=128,\n",
    "    num_heads=2,\n",
    "    d_ff=512,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. OPTIMIZER & LOSS\n",
    "# ============================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================\n",
    "# 5. TRAIN!\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
    "\n",
    "print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f50a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.23 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "MHA Model parameters: 70,559,825\n",
      "GPU Memory after model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear GPU first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=256)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Create PROPER MHA model (bigger than test)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=256,\n",
    "    d_model=512,     # Assignment size\n",
    "    num_heads=8,     # Assignment size  \n",
    "    d_ff=2048,       # Assignment size\n",
    "    num_layers=6     # Assignment size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"MHA Model parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory after model: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afed9b",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and calculate perplexity\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape and calculate loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()  # Back to training mode\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aa804",
   "metadata": {},
   "source": [
    "# Training MHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING MHA MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1146/1146 [01:36<00:00, 11.82it/s, loss=6.6002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 6.6002\n",
      "  Val Loss: 6.2297\n",
      "  Val Perplexity: 507.59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.76it/s, loss=5.6089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 5.6089\n",
      "  Val Loss: 5.8502\n",
      "  Val Perplexity: 347.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.74it/s, loss=5.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 5.0045\n",
      "  Val Loss: 5.6814\n",
      "  Val Perplexity: 293.35\n",
      "\n",
      "============================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MHA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model = mha_model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "        targets_flat = target_ids.view(batch_size * seq_len)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_perplexity = validate_model(mha_model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d841",
   "metadata": {},
   "source": [
    "# Save the MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e78c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA model saved as 'mha_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': 0.0419,\n",
    "    'val_loss': 0.0996,\n",
    "    'val_perplexity': 1.10,\n",
    "}, 'mha_model.pt')\n",
    "\n",
    "print(\"✅ MHA model saved as 'mha_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e73e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The history of\n",
      "Generating...\n",
      "\n",
      "Output: The history of New Hampshire County , it provides a tropical wave that turned into the southeast of June and its southern Africa . The depression was upgraded through two days before the\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: In mathematics,\n",
      "Generating...\n",
      "\n",
      "Output: In mathematics,@ 000 men in the race , allowing several occasions , and a third half of five of 100 miles ( 36 km / h ) . After five hours\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: The cat sat on the\n",
      "Generating...\n",
      "\n",
      "Output: The cat sat on the west of the tower and west bank before entering the intersection with the north that attracts their north is about 12 @.@ 7 miles ( 0 @.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_fixed(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with top-k sampling (more stable)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generating...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # TOP-K SAMPLING (key difference!)\n",
    "            # Only consider top 50 most likely tokens\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test with better sampling\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_fixed(mha_model, tokenizer, prompt, max_length=30, top_k=50, device=device)\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0217fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 2.95 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "======================================================================\n",
      "TRAINING MHA MODEL (PROPERLY)\n",
      "======================================================================\n",
      "Parameters: 70,690,897\n",
      "GPU Memory: 3.23 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 573/573 [01:46<00:00,  5.37it/s, loss=6.8676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 6.8676\n",
      "  Val Loss: 6.4539\n",
      "  Val Perplexity: 635.16\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=6.0458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 6.0458\n",
      "  Val Loss: 6.1286\n",
      "  Val Perplexity: 458.80\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=5.5513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 5.5513\n",
      "  Val Loss: 5.9496\n",
      "  Val Perplexity: 383.62\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=5.1569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 5.1569\n",
      "  Val Loss: 5.8209\n",
      "  Val Perplexity: 337.27\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=4.8107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 4.8107\n",
      "  Val Loss: 5.7808\n",
      "  Val Perplexity: 324.03\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=4.4971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 4.4971\n",
      "  Val Loss: 5.7587\n",
      "  Val Perplexity: 316.95\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=4.2113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 4.2113\n",
      "  Val Loss: 5.7916\n",
      "  Val Perplexity: 327.54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=3.9562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 3.9562\n",
      "  Val Loss: 5.8210\n",
      "  Val Perplexity: 337.29\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 573/573 [01:47<00:00,  5.35it/s, loss=3.7274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 3.7274\n",
      "  Val Loss: 5.8835\n",
      "  Val Perplexity: 359.08\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 573/573 [01:47<00:00,  5.34it/s, loss=3.5286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 3.5286\n",
      "  Val Loss: 5.9524\n",
      "  Val Perplexity: 384.68\n",
      "\n",
      "======================================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "Final Val Perplexity: 384.68\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Better Trainning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CLEAR GPU AND START FRESH\n",
    "# ============================================\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# BETTER HYPERPARAMETERS (Less Overfitting)\n",
    "# ============================================\n",
    "\n",
    "# Create datasets with LONGER sequences\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "# ============================================\n",
    "# BETTER TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"  ✅ Best model so far!\")\n",
    "        print()\n",
    "    \n",
    "    return val_loss, val_perplexity\n",
    "\n",
    "# ============================================\n",
    "# TRAIN MHA MODEL PROPERLY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MHA MODEL (PROPERLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    dropout=0.2  # Higher dropout to reduce overfitting\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.1)  # Higher weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\\n\")\n",
    "\n",
    "# Train for 10 epochs\n",
    "mha_val_loss, mha_perplexity = train_model_properly(\n",
    "    mha_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'val_loss': mha_val_loss,\n",
    "    'val_perplexity': mha_perplexity,\n",
    "}, 'mha_model_proper.pt')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA TRAINING COMPLETE!\")\n",
    "print(f\"Final Val Perplexity: {mha_perplexity:.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04cfc5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING MHA MODEL GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of\n",
      "Output: The history of the country , abolishing slavery .= = Early history = =The Scots were first in the 1930s , the Hudson 's history . The town was defended by about the present . There were possibly\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 000 , which became known as a pest for over the United States Congress . As a result of media attention states as the availability of the European colonization of the new interpretations , it was established in the\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the podium is known south @-@ eastward towards the southeast , with its length of disturbed weather patterns , which must be made it impossible with high tides ( 240 km ) , with over the south side\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING MHA MODEL GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_proper(mha_model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301e23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
