{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1 (Do not run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.8741,  1.4167,  1.5167], requires_grad=True)\n",
      "Output: tensor([-0.8741,  2.8334,  4.5501], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "# #Create\n",
    "# vocab_size = 1000  # Size of the vocabulary\n",
    "# d_model = 512    # Dimension of the embeddings\n",
    "# token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# #Test\n",
    "# token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "# output= token_embed (token_ids)\n",
    "# print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "# print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "# #Create\n",
    "# pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "# output = pos_embed(10)  # Example input sequence length\n",
    "# print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "# print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98afba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# # Test\n",
    "# embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "# output = embeddings(token_ids)\n",
    "\n",
    "# print(f\"Input shape: {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: 0.0131\n",
      "  Std: 0.9961\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: -0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create FFN\n",
    "# ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# # Test\n",
    "# x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "# output = ffn(x)\n",
    "\n",
    "# print(f\"Input shape: {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# # Test the implementation\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create model\n",
    "#     model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "#     # Create input\n",
    "#     batch_size = 32\n",
    "#     seq_len = 10\n",
    "#     x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(x)\n",
    "    \n",
    "#     print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "#     print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "#     print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# # Input\n",
    "# x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# # Forward pass\n",
    "# output = block(x)\n",
    "\n",
    "# print(f\"Input shape:  {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create model with 6 stacked blocks\n",
    "# model = Transformer(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,  # 6 transformer blocks!\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "# output = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Tie weights between token embeddings and lm_head (Weight Sharing) this is important for reducing the number of parameters and improving generalization. Morevover see token embeddings and lm_head are performing inverse operations. example token embeddings map token IDs to vectors, while lm_head maps vectors back to token logits.\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# # Create complete language model\n",
    "# model = LanguageModel(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))\n",
    "# logits = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "# print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63110246f574d3b9619f91106e284e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb9d20f814d4e2babd2e6f3d54289ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d914ea77d1245efba483e244e0e13f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2205567f7a6e4569a98bc6a72a3d1ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725d7499e2d49a688659c3fb1831a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045fdf868dc14f05982f875a6b09e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ee7af13c564dfa90f7fc6410cff48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75181c8aab614c9abad954eca1c2317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406fef5760b41559009fc5c12ab09c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082a49dfc5a44d5389cf08218021987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ddf0c3c500446b82278509620e8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee748b27664c308f84070ffd8eb5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create our dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_dataset = \u001b[43mWikiTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get first sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mWikiTextDataset.__init__\u001b[39m\u001b[34m(self, data, tokenizer, max_length)\u001b[39m\n\u001b[32m     19\u001b[39m     text = example[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Skip empty lines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         all_tokens.extend(tokens)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Step 2: Convert to PyTorch tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2732\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2694\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2695\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2715\u001b[39m     **kwargs,\n\u001b[32m   2716\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2717\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2718\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2719\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2730\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     encoded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3123\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3096\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3111\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3114\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3115\u001b[39m     padding=padding,\n\u001b[32m   3116\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m     **kwargs,\n\u001b[32m   3121\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3142\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2.py:281\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8c895",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "```\n",
    "Loss(So loss is computed for each token)(Define the Loss)\n",
    "↓\n",
    "Optimizer(Optimizer updates the model's weights to minimize loss.)\n",
    "↓\n",
    "Training Loop(Forward-Loss-clear Gradients-Backward-Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 5])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "\n",
      "After reshaping:\n",
      "Logits flat: torch.Size([6, 5])\n",
      "Targets flat: torch.Size([6])\n",
      "\n",
      "Loss: 2.0549\n"
     ]
    }
   ],
   "source": [
    "#Define the Loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake model output (logits)\n",
    "# [batch=2, seq=3, vocab_size=5]\n",
    "logits = torch.randn(2, 3, 5)\n",
    "\n",
    "# Target token IDs\n",
    "# [batch=2, seq=3]\n",
    "targets = torch.tensor([\n",
    "    [2, 4, 1],  # Sequence 1: correct tokens are 2, 4, 1\n",
    "    [0, 3, 2]   # Sequence 2: correct tokens are 0, 3, 2\n",
    "])\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)    # [2, 3, 5]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# BUT! CrossEntropyLoss expects:\n",
    "# logits: [N, vocab_size] where N = batch * seq\n",
    "# targets: [N]\n",
    "\n",
    "# So we need to reshape!\n",
    "logits_flat = logits.view(-1, 5)      # [6, 5]  (2*3=6 positions)\n",
    "targets_flat = targets.view(-1)        # [6]\n",
    "\n",
    "print(\"\\nAfter reshaping:\")\n",
    "print(\"Logits flat:\", logits_flat.shape)   # [6, 5]\n",
    "print(\"Targets flat:\", targets_flat.shape) # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Logits shape: torch.Size([32, 512, 50257])\n",
      "\n",
      "Flattened logits: torch.Size([16384, 50257])\n",
      "Flattened targets: torch.Size([16384])\n",
      "\n",
      "Loss: 11.0243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate model training\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Get a batch from dataloader\n",
    "input_ids, target_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")    # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Logits shape: {logits.shape}\")      # [32, 512, 50257]\n",
    "\n",
    "# Reshape for loss calculation\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "logits_flat = logits.view(batch_size * seq_len, vocab_size)  # [16384, 50257]\n",
    "targets_flat = target_ids.view(batch_size * seq_len)         # [16384]\n",
    "\n",
    "print(f\"\\nFlattened logits: {logits_flat.shape}\")\n",
    "print(f\"Flattened targets: {targets_flat.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created!\n",
      "Learning rate: 3e-4 = 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),  # All model weights to optimize\n",
    "    lr=3e-4,            # Learning rate (how big each update is)\n",
    "    weight_decay=0.01   # Regularization (prevents overfitting)\n",
    ")\n",
    "\n",
    "print(\"Optimizer created!\")\n",
    "print(f\"Learning rate: 3e-4 = {3e-4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fca023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Training ===\n",
      "Weight sample: -0.2838\n",
      "Loss: 1.8065\n",
      "Gradient sample: -0.151754\n",
      "=== After Training ===\n",
      "Weight sample: -0.2828\n",
      "Weight changed! ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(2, 10)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()           # Clear old gradients\n",
    "output = model(x)               # Forward\n",
    "loss = criterion(output, targets)  # Loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()                 # Calculate gradients\n",
    "print(f\"Gradient sample: {model.weight.grad[0, 0].item():.6f}\")\n",
    "\n",
    "optimizer.step()                # Update weights\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "print(\"Weight changed! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2794887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the language model\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            # Move data to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)  # [batch, seq, vocab]\n",
    "            \n",
    "            # Reshape for loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bf95b",
   "metadata": {},
   "source": [
    "# Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a23b0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after clearing: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after cleanup: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete ALL variables\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "del train_loader\n",
    "del train_dataset\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22332564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c80b82048e470b9012b2fe197a985d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057796956ac43cfa9c6440b034b37e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42992a4da2054fd4926e7eebd10a7f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc021f20604e3ebe39ca40180275e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d53f74a0349d7bbc222f81c40d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6d4ba893344c00a4d8dd3db6c882e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31973e826bd49358453ab05aa904754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5640029c2af348c2a3f9742ead92beec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5440a1482c14aa0b745ae720856390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc681d0ee0834c28bb41000258030d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beb87617ddb4006aa7c1afcf89bed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d975c24fa52a41c5baf31f003f6a5d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Check GPU is clean\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  # Should be 0!\n",
    "\n",
    "# 3. Load data (only what we need)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde39b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial GPU Memory: 0.00 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Dataset ready. GPU Memory: 0.00 GB\n",
      "Model created. GPU Memory: 0.00 GB\n",
      "Parameters: 3,399,569\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# ============================================\n",
    "# 1. COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "# MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        #This initializes the linear layers for query, key, value and output projections\n",
    "        #Linear layers are just weight matrices that will be learned during training\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Dropout layer for regularization this is used to prevent overfitting by randomly setting some of the attention weights to zero during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Here we r unpack the input tensor x to get batch size and sequence length\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        #Here we r passing the input x through the linear layers to get the query, key, and value matrices\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        # Now we reshape Q, K, V to separate the heads for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        #NOTE: Causal masking is essential in autoregressive models to ensure that each token can only attend to previous tokens and itself, preventing any \"future\" information leakage.\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# FeedForward class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "# LanguageModel class\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# WikiTextDataset class\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SETUP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE MODEL (TINY!)\n",
    "# ============================================\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=16,        # SMALLER!\n",
    "    num_heads=2,\n",
    "    d_ff=64,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"Model created. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. OPTIMIZER & LOSS\n",
    "# ============================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================\n",
    "# 5. TRAIN!\n",
    "# ============================================\n",
    "\n",
    "# print(\"\\nStarting training...\\n\")\n",
    "# train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
    "\n",
    "# print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f50a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.23 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "MHA Model parameters: 70,559,825\n",
      "GPU Memory after model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear GPU first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=256)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Create PROPER MHA model (bigger than test)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=256,\n",
    "    d_model=512,     # Assignment size\n",
    "    num_heads=8,     # Assignment size  \n",
    "    d_ff=2048,       # Assignment size\n",
    "    num_layers=6     # Assignment size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"MHA Model parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory after model: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afed9b",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and calculate perplexity\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape and calculate loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()  # Back to training mode\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aa804",
   "metadata": {},
   "source": [
    "# Training MHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING MHA MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1146/1146 [01:36<00:00, 11.82it/s, loss=6.6002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 6.6002\n",
      "  Val Loss: 6.2297\n",
      "  Val Perplexity: 507.59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.76it/s, loss=5.6089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 5.6089\n",
      "  Val Loss: 5.8502\n",
      "  Val Perplexity: 347.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.74it/s, loss=5.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 5.0045\n",
      "  Val Loss: 5.6814\n",
      "  Val Perplexity: 293.35\n",
      "\n",
      "============================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MHA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model = mha_model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "        targets_flat = target_ids.view(batch_size * seq_len)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_perplexity = validate_model(mha_model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d841",
   "metadata": {},
   "source": [
    "# Save the MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e78c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA model saved as 'mha_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': 0.0419,\n",
    "    'val_loss': 0.0996,\n",
    "    'val_perplexity': 1.10,\n",
    "}, 'mha_model.pt')\n",
    "\n",
    "print(\"✅ MHA model saved as 'mha_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e73e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The history of\n",
      "Generating...\n",
      "\n",
      "Output: The history of New Hampshire County , it provides a tropical wave that turned into the southeast of June and its southern Africa . The depression was upgraded through two days before the\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: In mathematics,\n",
      "Generating...\n",
      "\n",
      "Output: In mathematics,@ 000 men in the race , allowing several occasions , and a third half of five of 100 miles ( 36 km / h ) . After five hours\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: The cat sat on the\n",
      "Generating...\n",
      "\n",
      "Output: The cat sat on the west of the tower and west bank before entering the intersection with the north that attracts their north is about 12 @.@ 7 miles ( 0 @.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_fixed(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with top-k sampling (more stable)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generating...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # TOP-K SAMPLING (key difference!)\n",
    "            # Only consider top 50 most likely tokens\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test with better sampling\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_fixed(mha_model, tokenizer, prompt, max_length=30, top_k=50, device=device)\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0217fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.11 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "======================================================================\n",
      "TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\n",
      "======================================================================\n",
      "Parameters: 3,399,569\n",
      "Model Size: 3.40M (vs 70.7M before)\n",
      "Size Reduction: 20.8x smaller\n",
      "GPU Memory: 0.10 GB\n",
      "\n",
      "Dataset tokens: 2,347,038\n",
      "Chinchilla optimal params: 117,352\n",
      "Our model params: 3,399,569\n",
      "Ratio (ours/optimal): 29.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=17.2818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 17.2818\n",
      "  Val Loss: 9.4304\n",
      "  Val Perplexity: 12461.92\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 573/573 [00:21<00:00, 26.35it/s, loss=9.0392] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 9.0392\n",
      "  Val Loss: 7.7933\n",
      "  Val Perplexity: 2424.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=7.9895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 7.9895\n",
      "  Val Loss: 7.4918\n",
      "  Val Perplexity: 1793.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=7.6620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 7.6620\n",
      "  Val Loss: 7.3710\n",
      "  Val Perplexity: 1589.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=7.4919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 7.4919\n",
      "  Val Loss: 7.3031\n",
      "  Val Perplexity: 1484.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=7.3856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 7.3856\n",
      "  Val Loss: 7.2611\n",
      "  Val Perplexity: 1423.77\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=7.3077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 7.3077\n",
      "  Val Loss: 7.2225\n",
      "  Val Perplexity: 1369.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=7.2472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 7.2472\n",
      "  Val Loss: 7.1941\n",
      "  Val Perplexity: 1331.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.1967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 7.1967\n",
      "  Val Loss: 7.1640\n",
      "  Val Perplexity: 1292.12\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=7.1518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 7.1518\n",
      "  Val Loss: 7.1366\n",
      "  Val Perplexity: 1257.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=7.1103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11:\n",
      "  Train Loss: 7.1103\n",
      "  Val Loss: 7.1141\n",
      "  Val Perplexity: 1229.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:\n",
      "  Train Loss: 7.0726\n",
      "  Val Loss: 7.0844\n",
      "  Val Perplexity: 1193.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:\n",
      "  Train Loss: 7.0371\n",
      "  Val Loss: 7.0547\n",
      "  Val Perplexity: 1158.31\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:\n",
      "  Train Loss: 7.0039\n",
      "  Val Loss: 7.0269\n",
      "  Val Perplexity: 1126.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.9719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "  Train Loss: 6.9719\n",
      "  Val Loss: 6.9973\n",
      "  Val Perplexity: 1093.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=6.9402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:\n",
      "  Train Loss: 6.9402\n",
      "  Val Loss: 6.9682\n",
      "  Val Perplexity: 1062.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.9086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:\n",
      "  Train Loss: 6.9086\n",
      "  Val Loss: 6.9382\n",
      "  Val Perplexity: 1030.94\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.8775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:\n",
      "  Train Loss: 6.8775\n",
      "  Val Loss: 6.9086\n",
      "  Val Perplexity: 1000.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.8468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:\n",
      "  Train Loss: 6.8468\n",
      "  Val Loss: 6.8807\n",
      "  Val Perplexity: 973.33\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/300: 100%|██████████| 573/573 [00:22<00:00, 25.78it/s, loss=6.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Train Loss: 6.8150\n",
      "  Val Loss: 6.8526\n",
      "  Val Perplexity: 946.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=6.7826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:\n",
      "  Train Loss: 6.7826\n",
      "  Val Loss: 6.8194\n",
      "  Val Perplexity: 915.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=6.7494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:\n",
      "  Train Loss: 6.7494\n",
      "  Val Loss: 6.7898\n",
      "  Val Perplexity: 888.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.7157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23:\n",
      "  Train Loss: 6.7157\n",
      "  Val Loss: 6.7597\n",
      "  Val Perplexity: 862.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/300: 100%|██████████| 573/573 [00:22<00:00, 25.97it/s, loss=6.6823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:\n",
      "  Train Loss: 6.6823\n",
      "  Val Loss: 6.7281\n",
      "  Val Perplexity: 835.59\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=6.6478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:\n",
      "  Train Loss: 6.6478\n",
      "  Val Loss: 6.6979\n",
      "  Val Perplexity: 810.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=6.6135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:\n",
      "  Train Loss: 6.6135\n",
      "  Val Loss: 6.6672\n",
      "  Val Perplexity: 786.16\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=6.5799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:\n",
      "  Train Loss: 6.5799\n",
      "  Val Loss: 6.6371\n",
      "  Val Perplexity: 762.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.5470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:\n",
      "  Train Loss: 6.5470\n",
      "  Val Loss: 6.6070\n",
      "  Val Perplexity: 740.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.5139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:\n",
      "  Train Loss: 6.5139\n",
      "  Val Loss: 6.5786\n",
      "  Val Perplexity: 719.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.4835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:\n",
      "  Train Loss: 6.4835\n",
      "  Val Loss: 6.5555\n",
      "  Val Perplexity: 703.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=6.4533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:\n",
      "  Train Loss: 6.4533\n",
      "  Val Loss: 6.5314\n",
      "  Val Perplexity: 686.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=6.4249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:\n",
      "  Train Loss: 6.4249\n",
      "  Val Loss: 6.5110\n",
      "  Val Perplexity: 672.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=6.3976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:\n",
      "  Train Loss: 6.3976\n",
      "  Val Loss: 6.4873\n",
      "  Val Perplexity: 656.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=6.3709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:\n",
      "  Train Loss: 6.3709\n",
      "  Val Loss: 6.4699\n",
      "  Val Perplexity: 645.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:\n",
      "  Train Loss: 6.3447\n",
      "  Val Loss: 6.4494\n",
      "  Val Perplexity: 632.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.3202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:\n",
      "  Train Loss: 6.3202\n",
      "  Val Loss: 6.4301\n",
      "  Val Perplexity: 620.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.2953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:\n",
      "  Train Loss: 6.2953\n",
      "  Val Loss: 6.4136\n",
      "  Val Perplexity: 610.11\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=6.2716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:\n",
      "  Train Loss: 6.2716\n",
      "  Val Loss: 6.3966\n",
      "  Val Perplexity: 599.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.2481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:\n",
      "  Train Loss: 6.2481\n",
      "  Val Loss: 6.3787\n",
      "  Val Perplexity: 589.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.2248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:\n",
      "  Train Loss: 6.2248\n",
      "  Val Loss: 6.3651\n",
      "  Val Perplexity: 581.18\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=6.2026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:\n",
      "  Train Loss: 6.2026\n",
      "  Val Loss: 6.3507\n",
      "  Val Perplexity: 572.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=6.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:\n",
      "  Train Loss: 6.1795\n",
      "  Val Loss: 6.3363\n",
      "  Val Perplexity: 564.69\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=6.1574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:\n",
      "  Train Loss: 6.1574\n",
      "  Val Loss: 6.3197\n",
      "  Val Perplexity: 555.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.1353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:\n",
      "  Train Loss: 6.1353\n",
      "  Val Loss: 6.3071\n",
      "  Val Perplexity: 548.47\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.1128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:\n",
      "  Train Loss: 6.1128\n",
      "  Val Loss: 6.2887\n",
      "  Val Perplexity: 538.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/300: 100%|██████████| 573/573 [00:22<00:00, 25.87it/s, loss=6.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:\n",
      "  Train Loss: 6.0909\n",
      "  Val Loss: 6.2757\n",
      "  Val Perplexity: 531.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:\n",
      "  Train Loss: 6.0695\n",
      "  Val Loss: 6.2647\n",
      "  Val Perplexity: 525.70\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=6.0470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:\n",
      "  Train Loss: 6.0470\n",
      "  Val Loss: 6.2507\n",
      "  Val Perplexity: 518.40\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49:\n",
      "  Train Loss: 6.0255\n",
      "  Val Loss: 6.2357\n",
      "  Val Perplexity: 510.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50:\n",
      "  Train Loss: 6.0039\n",
      "  Val Loss: 6.2240\n",
      "  Val Perplexity: 504.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.9823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51:\n",
      "  Train Loss: 5.9823\n",
      "  Val Loss: 6.2094\n",
      "  Val Perplexity: 497.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.9610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52:\n",
      "  Train Loss: 5.9610\n",
      "  Val Loss: 6.1995\n",
      "  Val Perplexity: 492.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=5.9388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53:\n",
      "  Train Loss: 5.9388\n",
      "  Val Loss: 6.1850\n",
      "  Val Perplexity: 485.41\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.9185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54:\n",
      "  Train Loss: 5.9185\n",
      "  Val Loss: 6.1737\n",
      "  Val Perplexity: 479.98\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.8972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55:\n",
      "  Train Loss: 5.8972\n",
      "  Val Loss: 6.1587\n",
      "  Val Perplexity: 472.83\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.8764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56:\n",
      "  Train Loss: 5.8764\n",
      "  Val Loss: 6.1517\n",
      "  Val Perplexity: 469.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.8560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57:\n",
      "  Train Loss: 5.8560\n",
      "  Val Loss: 6.1389\n",
      "  Val Perplexity: 463.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.8352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58:\n",
      "  Train Loss: 5.8352\n",
      "  Val Loss: 6.1257\n",
      "  Val Perplexity: 457.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59:\n",
      "  Train Loss: 5.8150\n",
      "  Val Loss: 6.1153\n",
      "  Val Perplexity: 452.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.7939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60:\n",
      "  Train Loss: 5.7939\n",
      "  Val Loss: 6.1075\n",
      "  Val Perplexity: 449.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=5.7743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61:\n",
      "  Train Loss: 5.7743\n",
      "  Val Loss: 6.0926\n",
      "  Val Perplexity: 442.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=5.7544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62:\n",
      "  Train Loss: 5.7544\n",
      "  Val Loss: 6.0851\n",
      "  Val Perplexity: 439.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.7345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63:\n",
      "  Train Loss: 5.7345\n",
      "  Val Loss: 6.0757\n",
      "  Val Perplexity: 435.15\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.7148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64:\n",
      "  Train Loss: 5.7148\n",
      "  Val Loss: 6.0636\n",
      "  Val Perplexity: 429.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65:\n",
      "  Train Loss: 5.6955\n",
      "  Val Loss: 6.0552\n",
      "  Val Perplexity: 426.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.6763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66:\n",
      "  Train Loss: 5.6763\n",
      "  Val Loss: 6.0447\n",
      "  Val Perplexity: 421.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.6568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67:\n",
      "  Train Loss: 5.6568\n",
      "  Val Loss: 6.0344\n",
      "  Val Perplexity: 417.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.6381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68:\n",
      "  Train Loss: 5.6381\n",
      "  Val Loss: 6.0232\n",
      "  Val Perplexity: 412.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69:\n",
      "  Train Loss: 5.6185\n",
      "  Val Loss: 6.0126\n",
      "  Val Perplexity: 408.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=5.5998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70:\n",
      "  Train Loss: 5.5998\n",
      "  Val Loss: 6.0051\n",
      "  Val Perplexity: 405.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.5808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71:\n",
      "  Train Loss: 5.5808\n",
      "  Val Loss: 5.9956\n",
      "  Val Perplexity: 401.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72:\n",
      "  Train Loss: 5.5614\n",
      "  Val Loss: 5.9816\n",
      "  Val Perplexity: 396.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.5438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73:\n",
      "  Train Loss: 5.5438\n",
      "  Val Loss: 5.9745\n",
      "  Val Perplexity: 393.26\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.5256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74:\n",
      "  Train Loss: 5.5256\n",
      "  Val Loss: 5.9710\n",
      "  Val Perplexity: 391.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.5069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75:\n",
      "  Train Loss: 5.5069\n",
      "  Val Loss: 5.9571\n",
      "  Val Perplexity: 386.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=5.4891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76:\n",
      "  Train Loss: 5.4891\n",
      "  Val Loss: 5.9491\n",
      "  Val Perplexity: 383.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/300: 100%|██████████| 573/573 [00:22<00:00, 25.88it/s, loss=5.4718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77:\n",
      "  Train Loss: 5.4718\n",
      "  Val Loss: 5.9448\n",
      "  Val Perplexity: 381.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.4534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78:\n",
      "  Train Loss: 5.4534\n",
      "  Val Loss: 5.9369\n",
      "  Val Perplexity: 378.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.4358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79:\n",
      "  Train Loss: 5.4358\n",
      "  Val Loss: 5.9293\n",
      "  Val Perplexity: 375.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=5.4189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80:\n",
      "  Train Loss: 5.4189\n",
      "  Val Loss: 5.9201\n",
      "  Val Perplexity: 372.46\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.4020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81:\n",
      "  Train Loss: 5.4020\n",
      "  Val Loss: 5.9130\n",
      "  Val Perplexity: 369.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/300: 100%|██████████| 573/573 [00:21<00:00, 26.29it/s, loss=5.3851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82:\n",
      "  Train Loss: 5.3851\n",
      "  Val Loss: 5.9069\n",
      "  Val Perplexity: 367.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=5.3686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83:\n",
      "  Train Loss: 5.3686\n",
      "  Val Loss: 5.8972\n",
      "  Val Perplexity: 364.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.3516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84:\n",
      "  Train Loss: 5.3516\n",
      "  Val Loss: 5.8922\n",
      "  Val Perplexity: 362.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=5.3350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85:\n",
      "  Train Loss: 5.3350\n",
      "  Val Loss: 5.8852\n",
      "  Val Perplexity: 359.67\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.3195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86:\n",
      "  Train Loss: 5.3195\n",
      "  Val Loss: 5.8777\n",
      "  Val Perplexity: 357.00\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.3030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87:\n",
      "  Train Loss: 5.3030\n",
      "  Val Loss: 5.8724\n",
      "  Val Perplexity: 355.09\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.2877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88:\n",
      "  Train Loss: 5.2877\n",
      "  Val Loss: 5.8661\n",
      "  Val Perplexity: 352.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.2721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89:\n",
      "  Train Loss: 5.2721\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 351.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90:\n",
      "  Train Loss: 5.2560\n",
      "  Val Loss: 5.8551\n",
      "  Val Perplexity: 349.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.2417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91:\n",
      "  Train Loss: 5.2417\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.27\n",
      "  📊 Best model still at Epoch 90 (Val Loss: 5.8551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.2266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92:\n",
      "  Train Loss: 5.2266\n",
      "  Val Loss: 5.8484\n",
      "  Val Perplexity: 346.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.2121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93:\n",
      "  Train Loss: 5.2121\n",
      "  Val Loss: 5.8430\n",
      "  Val Perplexity: 344.82\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.1973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94:\n",
      "  Train Loss: 5.1973\n",
      "  Val Loss: 5.8386\n",
      "  Val Perplexity: 343.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/300: 100%|██████████| 573/573 [00:21<00:00, 26.26it/s, loss=5.1825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95:\n",
      "  Train Loss: 5.1825\n",
      "  Val Loss: 5.8325\n",
      "  Val Perplexity: 341.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96:\n",
      "  Train Loss: 5.1687\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.74\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97:\n",
      "  Train Loss: 5.1546\n",
      "  Val Loss: 5.8275\n",
      "  Val Perplexity: 339.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=5.1403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98:\n",
      "  Train Loss: 5.1403\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.76\n",
      "  📊 Best model still at Epoch 97 (Val Loss: 5.8275)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.1272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99:\n",
      "  Train Loss: 5.1272\n",
      "  Val Loss: 5.8204\n",
      "  Val Perplexity: 337.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100:\n",
      "  Train Loss: 5.1135\n",
      "  Val Loss: 5.8191\n",
      "  Val Perplexity: 336.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.1004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101:\n",
      "  Train Loss: 5.1004\n",
      "  Val Loss: 5.8183\n",
      "  Val Perplexity: 336.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102:\n",
      "  Train Loss: 5.0872\n",
      "  Val Loss: 5.8113\n",
      "  Val Perplexity: 334.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.0753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103:\n",
      "  Train Loss: 5.0753\n",
      "  Val Loss: 5.8030\n",
      "  Val Perplexity: 331.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104:\n",
      "  Train Loss: 5.0623\n",
      "  Val Loss: 5.8014\n",
      "  Val Perplexity: 330.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.0495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105:\n",
      "  Train Loss: 5.0495\n",
      "  Val Loss: 5.8080\n",
      "  Val Perplexity: 332.96\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=5.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106:\n",
      "  Train Loss: 5.0374\n",
      "  Val Loss: 5.8018\n",
      "  Val Perplexity: 330.91\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107:\n",
      "  Train Loss: 5.0251\n",
      "  Val Loss: 5.7969\n",
      "  Val Perplexity: 329.27\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108:\n",
      "  Train Loss: 5.0142\n",
      "  Val Loss: 5.7983\n",
      "  Val Perplexity: 329.75\n",
      "  📊 Best model still at Epoch 107 (Val Loss: 5.7969)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109:\n",
      "  Train Loss: 5.0020\n",
      "  Val Loss: 5.7950\n",
      "  Val Perplexity: 328.65\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110:\n",
      "  Train Loss: 4.9903\n",
      "  Val Loss: 5.7906\n",
      "  Val Perplexity: 327.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.9797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111:\n",
      "  Train Loss: 4.9797\n",
      "  Val Loss: 5.7909\n",
      "  Val Perplexity: 327.29\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112:\n",
      "  Train Loss: 4.9691\n",
      "  Val Loss: 5.7951\n",
      "  Val Perplexity: 328.68\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.9580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113:\n",
      "  Train Loss: 4.9580\n",
      "  Val Loss: 5.7962\n",
      "  Val Perplexity: 329.06\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.9469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114:\n",
      "  Train Loss: 4.9469\n",
      "  Val Loss: 5.7853\n",
      "  Val Perplexity: 325.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=4.9363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115:\n",
      "  Train Loss: 4.9363\n",
      "  Val Loss: 5.7926\n",
      "  Val Perplexity: 327.85\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116:\n",
      "  Train Loss: 4.9268\n",
      "  Val Loss: 5.7856\n",
      "  Val Perplexity: 325.58\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117:\n",
      "  Train Loss: 4.9171\n",
      "  Val Loss: 5.7922\n",
      "  Val Perplexity: 327.74\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.9066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118:\n",
      "  Train Loss: 4.9066\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.8967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119:\n",
      "  Train Loss: 4.8967\n",
      "  Val Loss: 5.7872\n",
      "  Val Perplexity: 326.09\n",
      "  📊 Best model still at Epoch 118 (Val Loss: 5.7850)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120:\n",
      "  Train Loss: 4.8883\n",
      "  Val Loss: 5.7828\n",
      "  Val Perplexity: 324.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=4.8788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121:\n",
      "  Train Loss: 4.8788\n",
      "  Val Loss: 5.7842\n",
      "  Val Perplexity: 325.11\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.8692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122:\n",
      "  Train Loss: 4.8692\n",
      "  Val Loss: 5.7858\n",
      "  Val Perplexity: 325.65\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.8605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123:\n",
      "  Train Loss: 4.8605\n",
      "  Val Loss: 5.7808\n",
      "  Val Perplexity: 324.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124:\n",
      "  Train Loss: 4.8523\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.27\n",
      "  📊 Best model still at Epoch 123 (Val Loss: 5.7808)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=4.8435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125:\n",
      "  Train Loss: 4.8435\n",
      "  Val Loss: 5.7782\n",
      "  Val Perplexity: 323.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.8349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126:\n",
      "  Train Loss: 4.8349\n",
      "  Val Loss: 5.7851\n",
      "  Val Perplexity: 325.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.8264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127:\n",
      "  Train Loss: 4.8264\n",
      "  Val Loss: 5.7815\n",
      "  Val Perplexity: 324.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.8189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128:\n",
      "  Train Loss: 4.8189\n",
      "  Val Loss: 5.7861\n",
      "  Val Perplexity: 325.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.8105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129:\n",
      "  Train Loss: 4.8105\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=4.8032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130:\n",
      "  Train Loss: 4.8032\n",
      "  Val Loss: 5.7830\n",
      "  Val Perplexity: 324.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.7954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131:\n",
      "  Train Loss: 4.7954\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.29\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=4.7874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132:\n",
      "  Train Loss: 4.7874\n",
      "  Val Loss: 5.7833\n",
      "  Val Perplexity: 324.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.7810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133:\n",
      "  Train Loss: 4.7810\n",
      "  Val Loss: 5.7840\n",
      "  Val Perplexity: 325.07\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134:\n",
      "  Train Loss: 4.7739\n",
      "  Val Loss: 5.7885\n",
      "  Val Perplexity: 326.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.7667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135:\n",
      "  Train Loss: 4.7667\n",
      "  Val Loss: 5.7910\n",
      "  Val Perplexity: 327.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136:\n",
      "  Train Loss: 4.7597\n",
      "  Val Loss: 5.7893\n",
      "  Val Perplexity: 326.79\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.7533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137:\n",
      "  Train Loss: 4.7533\n",
      "  Val Loss: 5.7864\n",
      "  Val Perplexity: 325.85\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138:\n",
      "  Train Loss: 4.7473\n",
      "  Val Loss: 5.7855\n",
      "  Val Perplexity: 325.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139:\n",
      "  Train Loss: 4.7406\n",
      "  Val Loss: 5.7886\n",
      "  Val Perplexity: 326.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/300: 100%|██████████| 573/573 [00:22<00:00, 25.83it/s, loss=4.7337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140:\n",
      "  Train Loss: 4.7337\n",
      "  Val Loss: 5.7882\n",
      "  Val Perplexity: 326.43\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/300: 100%|██████████| 573/573 [00:22<00:00, 25.94it/s, loss=4.7287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141:\n",
      "  Train Loss: 4.7287\n",
      "  Val Loss: 5.7875\n",
      "  Val Perplexity: 326.19\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.7231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142:\n",
      "  Train Loss: 4.7231\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143:\n",
      "  Train Loss: 4.7168\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144:\n",
      "  Train Loss: 4.7111\n",
      "  Val Loss: 5.7941\n",
      "  Val Perplexity: 328.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.7053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145:\n",
      "  Train Loss: 4.7053\n",
      "  Val Loss: 5.7961\n",
      "  Val Perplexity: 329.02\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=4.7015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146:\n",
      "  Train Loss: 4.7015\n",
      "  Val Loss: 5.7959\n",
      "  Val Perplexity: 328.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=4.6960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147:\n",
      "  Train Loss: 4.6960\n",
      "  Val Loss: 5.7968\n",
      "  Val Perplexity: 329.23\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=4.6905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148:\n",
      "  Train Loss: 4.6905\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.84\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.6853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149:\n",
      "  Train Loss: 4.6853\n",
      "  Val Loss: 5.8067\n",
      "  Val Perplexity: 332.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150:\n",
      "  Train Loss: 4.6807\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.6765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151:\n",
      "  Train Loss: 4.6765\n",
      "  Val Loss: 5.8083\n",
      "  Val Perplexity: 333.06\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152:\n",
      "  Train Loss: 4.6715\n",
      "  Val Loss: 5.8106\n",
      "  Val Perplexity: 333.81\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.6671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153:\n",
      "  Train Loss: 4.6671\n",
      "  Val Loss: 5.8089\n",
      "  Val Perplexity: 333.27\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154:\n",
      "  Train Loss: 4.6625\n",
      "  Val Loss: 5.8132\n",
      "  Val Perplexity: 334.67\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.6580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155:\n",
      "  Train Loss: 4.6580\n",
      "  Val Loss: 5.8153\n",
      "  Val Perplexity: 335.40\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156:\n",
      "  Train Loss: 4.6547\n",
      "  Val Loss: 5.8108\n",
      "  Val Perplexity: 333.88\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/300: 100%|██████████| 573/573 [00:22<00:00, 25.81it/s, loss=4.6494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157:\n",
      "  Train Loss: 4.6494\n",
      "  Val Loss: 5.8130\n",
      "  Val Perplexity: 334.63\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158:\n",
      "  Train Loss: 4.6463\n",
      "  Val Loss: 5.8192\n",
      "  Val Perplexity: 336.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/300: 100%|██████████| 573/573 [00:22<00:00, 25.86it/s, loss=4.6418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159:\n",
      "  Train Loss: 4.6418\n",
      "  Val Loss: 5.8141\n",
      "  Val Perplexity: 335.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160:\n",
      "  Train Loss: 4.6382\n",
      "  Val Loss: 5.8186\n",
      "  Val Perplexity: 336.52\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161:\n",
      "  Train Loss: 4.6342\n",
      "  Val Loss: 5.8263\n",
      "  Val Perplexity: 339.10\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.6309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162:\n",
      "  Train Loss: 4.6309\n",
      "  Val Loss: 5.8299\n",
      "  Val Perplexity: 340.32\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.6272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163:\n",
      "  Train Loss: 4.6272\n",
      "  Val Loss: 5.8290\n",
      "  Val Perplexity: 340.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164:\n",
      "  Train Loss: 4.6243\n",
      "  Val Loss: 5.8307\n",
      "  Val Perplexity: 340.58\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.6207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165:\n",
      "  Train Loss: 4.6207\n",
      "  Val Loss: 5.8317\n",
      "  Val Perplexity: 340.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166:\n",
      "  Train Loss: 4.6168\n",
      "  Val Loss: 5.8319\n",
      "  Val Perplexity: 341.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/300: 100%|██████████| 573/573 [00:22<00:00, 25.92it/s, loss=4.6140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167:\n",
      "  Train Loss: 4.6140\n",
      "  Val Loss: 5.8300\n",
      "  Val Perplexity: 340.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/300: 100%|██████████| 573/573 [00:22<00:00, 25.61it/s, loss=4.6106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168:\n",
      "  Train Loss: 4.6106\n",
      "  Val Loss: 5.8423\n",
      "  Val Perplexity: 344.56\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=4.6070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169:\n",
      "  Train Loss: 4.6070\n",
      "  Val Loss: 5.8412\n",
      "  Val Perplexity: 344.20\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/300: 100%|██████████| 573/573 [00:22<00:00, 25.74it/s, loss=4.6043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170:\n",
      "  Train Loss: 4.6043\n",
      "  Val Loss: 5.8384\n",
      "  Val Perplexity: 343.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171:\n",
      "  Train Loss: 4.6021\n",
      "  Val Loss: 5.8431\n",
      "  Val Perplexity: 344.83\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.5984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172:\n",
      "  Train Loss: 4.5984\n",
      "  Val Loss: 5.8439\n",
      "  Val Perplexity: 345.13\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/300: 100%|██████████| 573/573 [00:22<00:00, 25.91it/s, loss=4.5964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173:\n",
      "  Train Loss: 4.5964\n",
      "  Val Loss: 5.8477\n",
      "  Val Perplexity: 346.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.5928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174:\n",
      "  Train Loss: 4.5928\n",
      "  Val Loss: 5.8537\n",
      "  Val Perplexity: 348.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175:\n",
      "  Train Loss: 4.5904\n",
      "  Val Loss: 5.8522\n",
      "  Val Perplexity: 348.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/300: 100%|██████████| 573/573 [00:22<00:00, 25.85it/s, loss=4.5878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176:\n",
      "  Train Loss: 4.5878\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 350.99\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.5843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177:\n",
      "  Train Loss: 4.5843\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.57\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.5822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178:\n",
      "  Train Loss: 4.5822\n",
      "  Val Loss: 5.8606\n",
      "  Val Perplexity: 350.92\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.5791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179:\n",
      "  Train Loss: 4.5791\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.59\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.5771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180:\n",
      "  Train Loss: 4.5771\n",
      "  Val Loss: 5.8684\n",
      "  Val Perplexity: 353.69\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.5752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181:\n",
      "  Train Loss: 4.5752\n",
      "  Val Loss: 5.8683\n",
      "  Val Perplexity: 353.64\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182:\n",
      "  Train Loss: 4.5727\n",
      "  Val Loss: 5.8679\n",
      "  Val Perplexity: 353.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=4.5705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183:\n",
      "  Train Loss: 4.5705\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.25\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.5688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184:\n",
      "  Train Loss: 4.5688\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185:\n",
      "  Train Loss: 4.5664\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.5638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186:\n",
      "  Train Loss: 4.5638\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187:\n",
      "  Train Loss: 4.5614\n",
      "  Val Loss: 5.8798\n",
      "  Val Perplexity: 357.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/300:  27%|██▋       | 156/573 [00:05<00:15, 26.18it/s, loss=4.5186]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRatio (ours/optimal): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_params/optimal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Train for 300 epochs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m best_val_loss, best_epoch = \u001b[43mtrain_model_properly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmha_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMHA TRAINING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain_model_properly\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Backward with gradient clipping\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\u001b[39;00m\n\u001b[32m     93\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# Prevents exploding gradients\u001b[39;00m\n\u001b[32m     96\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Better Training with RIGHT MODEL SIZE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CLEAR GPU AND START FRESH\n",
    "# ============================================\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# BETTER HYPERPARAMETERS (Less Overfitting)\n",
    "# ============================================\n",
    "\n",
    "# Create datasets with LONGER sequences\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "# ============================================\n",
    "# BETTER TRAINING FUNCTION (WITH BEST MODEL SAVING!)\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ============================================\n",
    "        # TRAINING PHASE\n",
    "        # ============================================\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            # NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # ============================================\n",
    "        # VALIDATION PHASE\n",
    "        # ============================================\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # SAVE BEST MODEL (THE KEY FIX!)\n",
    "        # ============================================\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "            \n",
    "            print(f\"  ✅ Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            print(f\"  📊 Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return best_val_loss, best_epoch\n",
    "\n",
    "# ============================================\n",
    "# TRAIN MHA MODEL WITH OPTIMAL SIZE (1.8M params)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# mha_model = LanguageModel(\n",
    "#     vocab_size=50257,\n",
    "#     max_seq_len=512,\n",
    "#     d_model=192,        # Reduced from 512 → 192\n",
    "#     num_heads=4,        # Reduced from 8 → 4\n",
    "#     d_ff=768,           # Reduced from 2048 → 768\n",
    "#     num_layers=4,       # Reduced from 6 → 4\n",
    "#     dropout=0.2         # Kept at 0.2\n",
    "# ).to(device)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=64,        # SMALLER!\n",
    "    num_heads=4,\n",
    "    d_ff=256,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_params = sum(p.numel() for p in mha_model.parameters())\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Model Size: {num_params/1e6:.2f}M (vs 70.7M before)\")\n",
    "print(f\"Size Reduction: {70.7/(num_params/1e6):.1f}x smaller\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\\n\")\n",
    "\n",
    "# Chinchilla check\n",
    "tokens = 2_347_038\n",
    "optimal_params = tokens / 20\n",
    "print(f\"Dataset tokens: {tokens:,}\")\n",
    "print(f\"Chinchilla optimal params: {optimal_params:,.0f}\")\n",
    "print(f\"Our model params: {num_params:,}\")\n",
    "print(f\"Ratio (ours/optimal): {num_params/optimal_params:.1f}x\")\n",
    "\n",
    "\n",
    "# Train for 300 epochs\n",
    "best_val_loss, best_epoch = train_model_properly(\n",
    "    mha_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=300\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA TRAINING COMPLETE!\")\n",
    "print(f\"Best model saved at Epoch {best_epoch}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04cfc5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING MHA MODEL GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of\n",
      "Output: The history of The New Age , the New York and W.com ranked in Ireland , with the 2012 . The United Nations and one @-@ year , in Canada ranked Billboard Hot 100 in the third consecutive weeks\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 000 and federal federal federal federal federal government , the Government departments , and the government .= = State = Post = = = = =Although the government Government Act of of the executive Department is the\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the most significant in the same time of the name of a small species , including the name , from the age of 1611th century . Another new genera was the Middle Middle Trinitys in the English\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING MHA MODEL GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_proper(mha_model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5bd7",
   "metadata": {},
   "source": [
    "# Full Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2301e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Memory: 0.03 GB\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Creating datasets...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 115,716,078\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 7063\n",
      "Val batches: 15\n",
      "\n",
      "Creating model...\n",
      "Parameters: 869,153\n",
      "GPU Memory: 0.03 GB\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=8.2508] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 8.2508\n",
      "  Val Loss: 7.2842\n",
      "  Val Perplexity: 1457.12\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=7.1734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 7.1734\n",
      "  Val Loss: 6.9867\n",
      "  Val Perplexity: 1082.15\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=6.9394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 6.9394\n",
      "  Val Loss: 6.7338\n",
      "  Val Perplexity: 840.37\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 7063/7063 [15:18<00:00,  7.69it/s, loss=6.6895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 6.6895\n",
      "  Val Loss: 6.4804\n",
      "  Val Perplexity: 652.23\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.4793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 6.4793\n",
      "  Val Loss: 6.3027\n",
      "  Val Perplexity: 546.04\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.3284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 6.3284\n",
      "  Val Loss: 6.1547\n",
      "  Val Perplexity: 470.93\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.2119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 6.2119\n",
      "  Val Loss: 6.0556\n",
      "  Val Perplexity: 426.51\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.1308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 6.1308\n",
      "  Val Loss: 5.9870\n",
      "  Val Perplexity: 398.24\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.0689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 6.0689\n",
      "  Val Loss: 5.9348\n",
      "  Val Perplexity: 377.98\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 7063/7063 [15:19<00:00,  7.68it/s, loss=6.0220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 6.0220\n",
      "  Val Loss: 5.8991\n",
      "  Val Perplexity: 364.71\n",
      "  Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "Best Epoch: 10\n",
      "Best Val Loss: 5.8991\n",
      "Best Val Perplexity: 364.71\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PLOTTING TRAINING METRICS\n",
      "======================================================================\n",
      "\n",
      "Training metrics plot saved to: training_metrics.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAPZCAYAAAD+1mNdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4FFXfxvF70xNCqAnd0KUjiCA1IAiEJkivoVkARUV9FHlUkAcQERsoIiX03juEjogCiihVQJq00EJoqTvvH/uysiQLoWWyyfdzXbmcOTuze2/mgGd/nD1jMQzDEAAAAAAAAAAASMLN7AAAAAAAAAAAAKRVFNEBAAAAAAAAAHCCIjoAAAAAAAAAAE5QRAcAAAAAAAAAwAmK6AAAAAAAAAAAOEERHQAAAAAAAAAAJyiiAwAAAAAAAADgBEV0AAAAAAAAAACcoIgOAAAAAAAAAIATFNEBAAAkFSxYUBaL5YF+jh079tjzTZo0yeE1Bw4c+Eif/87370q6du2a5JoEBQUpNjY22ePPnDkjLy+vJOc86t/p4+CK12nv3r165513VLlyZQUFBcnLy0vZsmVTqVKl1KNHD61atcrsiKZxxesJAACQEVFEBwAAQLpz/vx5zZgxI9nHvvvuO8XHxz/W1z927JhDcbR27dqP9fXSohs3bqhbt24qW7asRo4cqR07duj8+fOKj49XVFSU9u/fr4kTJyo0NFTPPvusjh49anZkAAAAIFkeZgcAAABICxo1aqTIyEiHtn379mn//v32/eDgYFWqVCnJuZkyZXrs+QoWLKiWLVva90uVKvVInz+59+/qRo0apW7dujm0xcbGauzYsSYleniucp1iYmJUt25d/fzzzw7tTz75pIoXL65z585p586dslqtkqRffvlFlStX1s8//6wiRYqYEdkUrnI9AQAAMjqK6AAAALLNTr7TwIEDNWjQIPt+7dq1NWnSpFRM9a/atWs/1tnMyb1/V7dr1y5t2bJFNWvWtLfNmDFD58+fNzHVw3GV6/TOO+84FNB9fX01Y8YMNW/e3N62b98+NW3aVH///bck6cKFC3rxxRe1a9cuublljC/Musr1BAAAyOgyxugUAADgMUlurfKjR4+qa9euypcvnzw8PNS1a1dJ0sWLFzV48GC1bNlSpUuXVu7cueXt7S0/Pz898cQTatasmaZPn26fnXuv17ld7dq1k6zTvn79ejVu3FjZs2eXj4+PSpcurS+//FKGYSR5/rutzbxx40aHx7p27aro6Gh9+OGHKlGihHx8fJQzZ061atVKBw4ccPq7mjZtmqpUqaJMmTIpa9aseu6557R8+fJHvvRJvnz57NvffPONw2Nff/11ssfdze7du9WrVy+VLl1aAQEB8vb2Vv78+dW6dWtFREQ4HHvrvRQqVMihfdOmTU7f452/e8MwNG7cOFWpUkUBAQEO6+6nZA3t69eva8yYMQoNDVXevHnl7e2tgIAAFS1aVB06dNCaNWscjj958qTeeecdVahQQVmzZpWHh4eyZcumokWLKjQ0VB999JF27dqVot/Vref74YcfHNpGjBjhUECXbN+mWLBggUPB/I8//tDcuXMlSXPnznV4r//5z3+Sfb2qVavaj/Hw8NA///zj8PiRI0cc3p+Xl5dy586tJk2aaN68ecn+eUjpn+vGjRs7HLdv374kz3Xw4EGHYxo0aGB/LCXXMyoqSiNGjFBISIhy5swpT09PZc+eXTVq1NCXX36p69evOxx/7tw5ubm52Z+zRYsWDo9/+umn9sc8PT117do1+2N3/lns2LFjspkAAAAyHAMAAADJ+vjjjw1J9p+wsLAkx4SHhzsc06xZMyMgICDZ83bs2OHQ7uynQYMGRlxc3F1f5+OPP3Z4PCQkxOHxLl26OH3+N954I8n7CA4Odjjmdhs2bHB4rGbNmkahQoWSfe6sWbMaR48eTfL8vXv3dprn5ZdfdtgPCQm5j6tkGGFhYQ7nDxw40HB3dzckGe7u7saJEyeSvI+sWbMa77zzzl1/p4ZhGAMGDDAsFstdr1e3bt2MhIQEwzAM4+jRoym6xre/xzt/9507d05y/K3f6d2uk2EYxvbt25Mcc+fP7f344MGDRvbs2e+Z9+23307x9fj2228dzs2cObNx8+ZNp8fXq1fP4fg2bdoYhmEYcXFxRlBQkL09b968RmJiosO5hw4dcji3SZMmSbJ4eXnd9b2FhoYa169fdzgvpX+uFy1a5ND23nvvJXl/AwYMcDhm3rx59sfudT23bNli5M6d+675ixUrZhw8eNDhvLJly9ofz5Ejh2G1Wu2PNWzY0OH8lStXOn3fEydOdHrdAAAAMhJmogMAADxCS5YsUXR0tPLnz6/Q0FBVrlxZ7u7uDsfkzp1bVapUUcOGDdWsWTNVq1ZNvr6+9sdXr16tb7/99qFyTJkyRf7+/nruuedUtGhRh8dGjRqlkydPPvBzb9myRUePHlWJEiX03HPPycfHx/5YVFSUhg4d6nD8zJkzkyxbUbRoUT3//PPKli1bklnLDys4ONg+6zkxMdH+u7x9FnrPnj3vuZb9iBEjNGTIEPtMZR8fH9WuXVsNGzZUjhw57MeFh4drwIABkmzr47ds2VKhoaEOz5UzZ061bNnS/hMSEuL0dadOnSpvb29VrlxZDRs2VK5cuVL0vo8dO6YGDRro+PHj9jYPDw9VqFBBTZs2VcWKFZMskzJy5EhdunTJvl+iRAk1bdpUzz33nJ588kl5eXml6LVv98svvzjsV6pUyaGP3Kl69eoO+9u3b5ckeXp62r/FIUmnT5/WunXrHI6dOnWqw/7LL79s3547d6769OmjuLg4SZK7u7uqVaumxo0bO3wLYeXKlerevftd35OzP9dNmjRR3rx57cfd+U0SwzA0bdo0+36uXLnUrFmzu77WLUeOHFHjxo119uxZe1uZMmXUpEkTlS5d2t526NAhhYaG6saNG/a2559/3r598eJF/fnnn5KkhIQEbd261eF1Nm7cmOy2JNWrVy9FWQEAANI9s6v4AAAAadWDzETX/89GvX3GbExMjGEYhhEVFWX89ddfyb7W2bNnjUyZMtmfo0qVKnd9nXvNRA8ODjaOHTtmGIZhxMfHG3Xr1nV4fPLkyQ7n389M9Dtf/87HCxUq5HD+7bNiJRmvvPKKfWbsuXPnjBIlSjidpZ0Sd85EDw8PNzZv3uwwE3fv3r2Gm5ubIdlmpx87dizJ9b39PUVFRRn+/v72xwoXLmycOnXK/vi1a9eMihUr2h/38vIyTp8+bX/8zhnpd3tPd/7ug4ODjX379tkfT0hIsM90v9t1uvPbB08++aSxd+9eh2NOnjxpLFmyxL7//PPP24+vW7dukmzXrl0zli1bZqxevfruF+E2oaGhDjnat29/1+O///57h+P9/Pzsjx06dMjhmwCdOnVyOLdw4cL2x/Lnz2//PSUmJhpPPPGE/bFs2bI5/E7j4+ONxo0bO7zuzp077Y/fz5/r//73vw7HrV271n7Mpk2bHB57//33HfLf7Xp26tTJ4bGZM2c6PD506FCHxz///HP7YytWrHB47JtvvjEMwzB++eUXe9utb2vc/nfN7XmefPLJu102AACADIWZ6AAAAI9Q8eLFNWTIEIcZv97e3pKkLFmyKC4uTn379lWFChWULVs2eXp6ymKxKHfu3A5rG99tbfGUeP/99xUcHCzJNhu5UaNGDo+fOnXqgZ87X758+u9//2vfr127tjJnzpzsc589e9Y+C1aSvLy8NGzYMPv6z0FBQerfv/8DZ3GmZs2aqlChgiTbTNxmzZrZZwi/8MIL9t+NMxEREQ5rRbu7u6tv375q1aqVWrVqpbCwMIfH4+LitHr16keS/X//+59Klizp8Np3fpvhTlarVYsXL3ZoGzt2rEqVKuXQlj9/fjVt2tS+f/vvYceOHfrkk0+0cOFC/fnnn7p586YyZcqkxo0bq379+g/zlu7KSGZN8luKFi2qOnXq2PcXLlxo/3OydetW+01JJal79+7239Nvv/2mEydO2B/z8/PThx9+aL9+7dq10+nTpx1ea+nSpU5z3O3Pdc+ePR3ab58df/u2xWLRSy+95PQ1bme1WrVkyRL7vpeXl+bNm2fP36pVqySzxm/PX6tWLYdvEWzatMnhv5Ls653/+uuvunbtmo4dO+bwLQZmoQMAAPzLw+wAAAAA6UnNmjWdFjznzJmjjh07KiEh4Z7Pc+XKlYfK8cwzzzjsZ8mSxWE/Njb2gZ+7QoUK8vBwHEZmyZJFV69elST78hmSHIpykvTEE08oW7ZsDm3lypV74Cx388Ybb9iXAzly5IhD+70cPXrUYf/QoUM6dOjQfZ3zoB7kxqoXL1506DMeHh6qVq3aPc97++23NW/ePEVFRSk6Oloff/yx/TF3d3eVK1dOrVq1Ut++feXv75+iLIGBgQ77Z86cuevxty9XItn+YeV2L7/8stavXy/JdtPU+fPnq0uXLg4Fajc3N/Xs2dO+f+e1OHXqlObPn3/XHHe7fnf7cx0cHKz69etr1apVkqT58+fru+++k5ubm/0mqZJUt25dFS5c+K4Zbrl48aKio6Pt+3FxcfeVP1OmTKpataq9aL5582YZhmEvvD/55JNq166dpkyZooSEBP34449JrgNFdAAAgH8xEx0AAOARun195NvFxcWpV69eDgX0wMBANWjQwL5Otp+f3yPLcfua3ZLuOZP5YZ77fp7/zjW5JdlnpT9q7dq1S1KQfeqpp1SrVq3H8nq3f5PgYTjrQ49DiRIltGfPHn3wwQd6+umnHdYuT0xM1K5duzRgwAA999xzSkxMTNFzVq5c2WH/119/VUxMjNPj71yj+85/AGrRooVDYX7q1KmKi4vTnDlz7G0NGzZUgQIFUpTPmbtdv3tdk9vXYr927ZoWLlyopUuXOvzDxu3HPA535r+9CH7+/Hn9+eef+vHHHyVJISEhqlGjhv3P7caNGx1mtru7uzt8AwAAACCjo4gOAADwCCVXJJakvXv3OtzA8amnntLJkye1atUqzZs3T7NmzUqtiKnqzmVTTpw44bAMiiTt3r37sby2t7e3XnnlFYe2vn37pujcQoUKOey/+uqrMgzjrj+ff/65/fiH+YcBZ33obnLkyKGAgAD7fkJCgn766acUnZsvXz4NGTJEO3fu1PXr13Xq1ClFRESoZs2a9mN27NihLVu2pOj5mjZt6vBNhatXr2rixInJHvvnn3/aZ5nf8uKLLzrse3l5KSwszL6/fv16ff/997p8+bK97c4C9Z3Xr2HDhve8fvPmzXP6nu51TZo2bao8efLY96dMmeIwUz4oKMh+s9uUyJEjh8MSSQEBAYqNjb1r/gsXLjg8x50zyb/88kv77PZbSzBVrFhRUtIi+jPPPJPk2ysAAAAZGUV0AACAVBAfH++w7+XlJU9PT0m29Y/79++vGzdumBHtscqdO7fKli1r34+JidHAgQPt+5GRkRo2bNhje/1evXopV65cypEjh4oWLar27dun6Ly6des6fDNg8uTJWrNmTZLjrl69qrlz5yo0NNSh3dfX12H/zvW3HzU3Nzc1a9bMoe2VV17R/v37HdrOnj3rsHb2woULNX/+fPs/bLi5uSlv3ryqV6+eQxH91rkp8cQTTzgsrSJJ77zzjsMa35K0f/9+vfjii/a16iWpTJkyatOmTZLnvL1IbrVa9d5779n38+bNqyZNmjgcX7FiReXLl8++v2bNGk2ZMiXJ88bExGjFihVq06aN/vnnnxS9v+R4eHioW7du9v1169bZl3eRpK5du9r/vKeEm5ubw3uKjo5Wv379kizDZBiGfvnlF7355ptauHChw2N3FsKnTZtm3w4JCXH4744dO1gPHQAA4C5YEx0AACAVlClTRv7+/vZi5fbt21W8eHGVKFFC+/bt09GjR2WxWO56k0VX1b9/f3Xo0MG+P3LkSC1btkzBwcHasWOHw4ziRy1PnjwpLv7eLlu2bBowYIAGDBggSbp586YaNGigEiVKqHDhwrJarTp58qQOHjyY7Br3QUFByp49u/3bB4cOHdJTTz2lIkWKyGKxqGfPnmrYsOHDvbk7DBo0yGEJkYMHD6pcuXIqV66c8uXLp7Nnz2rXrl3q2LGj/eaimzZt0tdffy0vLy+VKFFC+fLlk5eXl06ePKnffvvN4flvv9npvXzxxRf69ddftWPHDkm2398LL7ygEiVKqFixYoqMjNSOHTscCujZs2fXggULkp31XaxYMdWuXds+W/r25WFuv6HoLW5ubvrss8/sN8+0Wq0KCwvTxx9/rBIlSsjNzU2nT5/W/v377YXpzz77LMXvLzkvvfSShg0bJsMwlJiYaF/+5n5uKHq7gQMHaunSpfa/M7799lvNnDlT5cuXV+bMmXXhwgXt3bvXfr2feuoph/NvLcmyaNEiSbL302LFitmXp6ldu7Y+//xzh+sgSc8///x95wUAAEjPmIkOAACQCvz8/DR06FCHtiNHjmj58uU6evSoXnvtNT3xxBMmpXu82rdvr969ezu0HTx4UGvWrNHly5eTLLHi5eWVmvGc+uCDD/Tee+85FHUPHDigFStWaNWqVdq7d6+9MJncmvA9evRw2N+9e7cWLFig+fPn6/Dhw488b+HChbVq1SqHtcETEhL022+/aenSpdqxY4fTm9rGxcXpjz/+0MqVK7V48eIkBfRXXnlF5cuXT3EWX19frV+/Xp06dXJoP3DggJYuXapffvnFoXD7zDPPaPv27SpWrJjT50xuTfE7byh6uw4dOuibb75x6E/Hjh3TqlWrtGLFCv3+++8OM7sf9r4BBQsWTLb4XKdOHRUtWvS+n6948eJatmyZcufObW+7dOmSNmzYoCVLluinn35KcjPZOyU3o/zW7HNJDuui33LrpqQAAAD4F0V0AACAVPL6669r3rx5evbZZ+Xr6yt/f39VrlxZ4eHhGjVqlNnxHqtvv/1WU6ZMUeXKleXr66ssWbKobt26WrNmTZJlSFLzxpr38umnn2rXrl167bXXVL58eQUEBMjd3V3+/v4qUaKEWrdurW+//TbZpUCGDBmi//3vfypVqpTDDTsfp2effVb79u3T6NGjVb9+feXOnVteXl7y9/dXkSJF1K5dO4dvBbz66qv67LPP1KJFC5UoUUI5c+aUh4eHfH19VahQIbVs2VKLFi3S999/f99Z/P39NXXqVP3xxx9666239PTTTytHjhzy8PBQlixZVKJECXXt2lXLly/X9u3bVaRIkbs+34svvpjkprb169dPsu7+7V5//XXt379f7733np555hlly5ZN7u7u8vPzU5EiRdSsWTN9/vnn+vvvvx/6xqRS8oX+h7mhaEhIiA4cOKAvv/xSdevWVVBQkDw9PeXt7a18+fKpTp06GjBggH7++eck/2AhJT+jvHbt2vbtLFmyJJnBXqtWrftaegYAACAjsBjp8TvDAAAASFOOHz+ebLEzNjZWoaGh2rBhg71t2rRp9mU4AAAAAMBsFNEBAADw2NWuXVuHDx9WrVq1lDdvXvn4+Oj06dNavny5IiMj7ceVK1dOv/76a7JLUwAAAACAGfh0AgAAgFRx6tQpzZw50+njlStX1qJFiyigAwAAAEhT+IQCAACAx+7tt99W4cKFtWPHDp09e1ZRUVHy8fFRnjx59PTTT6t169Zq3ry5w008AQAAACAtYDkXAAAAAAAAAACcYKoPAAAAAAAAAABOUEQHAAAAAAAAAMAJiugAAAAAAAAAADhBER0AAAAAAAAAACcoogMAAAAAAAAA4ARFdAAAAAAAAAAAnKCIDgAAAAAAAACAExTRAQAAAAAAAABwgiI6AAAAAAAAAABOUEQHACe6du2qggULPtC5AwcOlMViebSBgGRMmjRJFotFO3fuNDsKAACA3bFjx2SxWDRp0iR72/2MkS0WiwYOHPhIM9WuXVu1a9d+pM8JJMdisei1114zOwaAR4giOgCXY7FYUvSzceNGs6OaomvXrvL39zc7Rrpxq0jt7Ofnn382OyIAAMBDadasmfz8/HT16lWnx3Ts2FFeXl66ePFiKia7f/v27dPAgQN17Ngxs6PYbdy4URaLRfPmzTM7Srpxt/H5q6++anY8AOmQh9kBAOB+TZ061WF/ypQpioiISNJesmTJh3qdcePGyWq1PtC5//3vf/X+++8/1Osjbfnkk09UqFChJO1FixY1IQ0AAMCj07FjRy1dulQLFy5Uly5dkjx+48YNLV68WA0bNlSOHDke+HVSY4y8b98+DRo0SLVr107yrdI1a9Y81tdG6nr++eeT7a/Fixc3IQ2A9I4iOgCX06lTJ4f9n3/+WREREUna73Tjxg35+fml+HU8PT0fKJ8keXh4yMODv2JdxfXr15UpU6a7HhMaGqpKlSqlUiIAAIDU06xZM2XOnFkzZsxItii5ePFiXb9+XR07dnyo1zF7jOzl5WXaa+P+xMTEyMvLS25uzhdQKF68+D0/AwLAo8JyLgDSpdq1a6tMmTL69ddfVatWLfn5+emDDz6QZPsQ0LhxY+XNm1fe3t4qUqSIBg8erMTERIfnuHNN9FvrOn7++ef64YcfVKRIEXl7e+uZZ57Rjh07HM5Nbr3HW+viLVq0SGXKlJG3t7dKly6tVatWJcm/ceNGVapUST4+PipSpIjGjh37yNdZnzt3rp5++mn5+voqZ86c6tSpk06dOuVwzNmzZ9WtWzflz59f3t7eypMnj1544QWHr8fu3LlTDRo0UM6cOeXr66tChQqpe/fuKcrw3XffqXTp0vL29lbevHnVp08fRUVF2R9/7bXX5O/vrxs3biQ5t3379sqdO7fDdVu5cqVq1qypTJkyKXPmzGrcuLH27t3rcN6t5W6OHDmiRo0aKXPmzA/9gVBy7B9ffvmlgoOD5evrq5CQEO3ZsyfJ8evXr7dnzZo1q1544QXt378/yXGnTp1Sjx497P21UKFC6tWrl+Li4hyOi42NVb9+/RQYGKhMmTKpRYsWOn/+vMMxD3OtAABA+uXr66sXX3xR69atU2RkZJLHZ8yYocyZM6tZs2a6dOmS3nnnHZUtW1b+/v4KCAhQaGiodu/efc/XSW48Gxsbq7feekuBgYH21/jnn3+SnHv8+HH17t1bTz75pHx9fZUjRw61bt3aYVw6adIktW7dWpJUp06dJMs8JrcmemRkpHr06KFcuXLJx8dH5cuX1+TJkx2OuZ/PAQ/j77//VuvWrZU9e3b5+fnp2Wef1fLly5McN2rUKJUuXVp+fn7Kli2bKlWqpBkzZtgfv3r1qt58800VLFhQ3t7eCgoK0vPPP6/ffvvtnhl27dql0NBQBQQEyN/fX3Xr1nVYvnDnzp2yWCxJfkeStHr1alksFi1btszedurUKXXv3l25cuWyf/6ZOHGiw3m3lruZNWuW/vvf/ypfvnzy8/NTdHR0in5vd3P758Jq1arZx8Dff/99kmNT0hckyWq16uuvv1bZsmXl4+OjwMBANWzYMNl7FN3rs9/DXCsAqYtpkgDSrYsXLyo0NFTt2rVTp06dlCtXLkm2wbW/v7/69esnf39/rV+/Xh999JGio6M1YsSIez7vjBkzdPXqVb3yyiuyWCz67LPP9OKLL+rvv/++5+z1H3/8UQsWLFDv3r2VOXNmffPNN2rZsqVOnDhh/2rsrl271LBhQ+XJk0eDBg1SYmKiPvnkEwUGBj78L+X/TZo0Sd26ddMzzzyjYcOG6dy5c/r666+1detW7dq1S1mzZpUktWzZUnv37tXrr7+uggULKjIyUhERETpx4oR9v379+goMDNT777+vrFmz6tixY1qwYME9MwwcOFCDBg1SvXr11KtXLx08eFBjxozRjh07tHXrVnl6eqpt27b69ttvtXz5cvsHIsn2rYKlS5eqa9eucnd3l2Rb5icsLEwNGjTQ8OHDdePGDY0ZM0Y1atTQrl27HP5BJCEhQQ0aNFCNGjX0+eefp+gbCleuXNGFCxcc2iwWS5KvNE+ZMkVXr15Vnz59FBMTo6+//lrPPfec/vzzT3sfXLt2rUJDQ1W4cGENHDhQN2/e1KhRo1S9enX99ttv9qynT59W5cqVFRUVpZdfflklSpTQqVOnNG/ePN24ccNhNtXrr7+ubNmy6eOPP9axY8f01Vdf6bXXXtPs2bMl6aGuFQAASP86duyoyZMna86cOQ43RLx06ZJWr16t9u3by9fXV3v37tWiRYvUunVrFSpUSOfOndPYsWMVEhKiffv2KW/evPf1uj179tS0adPUoUMHVatWTevXr1fjxo2THLdjxw799NNPateunfLnz69jx45pzJgxql27tvbt2yc/Pz/VqlVLffv21TfffKMPPvjAvryjs2Ueb968qdq1a+vw4cN67bXXVKhQIc2dO1ddu3ZVVFSU3njjDYfjH+ZzwL2cO3dO1apV040bN9S3b1/lyJFDkydPVrNmzTRv3jy1aNFCkm3Jyb59+6pVq1Z64403FBMToz/++EO//PKLOnToIEl69dVXNW/ePL322msqVaqULl68qB9//FH79+9XxYoVnWbYu3evatasqYCAAP3nP/+Rp6enxo4dq9q1a2vTpk2qUqWKKlWqpMKFC2vOnDkKCwtzOH/27NnKli2bGjRoYH9Pzz77rH0yUWBgoFauXKkePXooOjpab775psP5gwcPlpeXl9555x3Fxsbe85sDMTExScbnkhQQEOBw7uXLl9WoUSO1adNG7du315w5c9SrVy95eXnZJ5TcT1/o0aOHJk2apNDQUPXs2VMJCQnasmWLfv75Z4dvrqbks9+DXisAJjAAwMX16dPHuPOvs5CQEEOS8f333yc5/saNG0naXnnlFcPPz8+IiYmxt4WFhRnBwcH2/aNHjxqSjBw5chiXLl2yty9evNiQZCxdutTe9vHHHyfJJMnw8vIyDh8+bG/bvXu3IckYNWqUva1p06aGn5+fcerUKXvboUOHDA8PjyTPmZywsDAjU6ZMTh+Pi4szgoKCjDJlyhg3b960ty9btsyQZHz00UeGYRjG5cuXDUnGiBEjnD7XwoULDUnGjh077pnrdpGRkYaXl5dRv359IzEx0d4+evRoQ5IxceJEwzAMw2q1Gvny5TNatmzpcP6cOXMMScbmzZsNwzCMq1evGlmzZjVeeuklh+POnj1rZMmSxaE9LCzMkGS8//77KcoaHh5uSEr2x9vb237crf7h6+tr/PPPP/b2X375xZBkvPXWW/a2p556yggKCjIuXrxob9u9e7fh5uZmdOnSxd7WpUsXw83NLdnfr9VqdchXr149e5thGMZbb71luLu7G1FRUYZhPPi1AgAAGUNCQoKRJ08eo2rVqg7t33//vSHJWL16tWEYhhETE+MwfjMM2zjI29vb+OSTTxzaJBnh4eH2tjvHyL///rshyejdu7fD83Xo0MGQZHz88cf2tuTG8Nu2bTMkGVOmTLG3zZ0715BkbNiwIcnxISEhRkhIiH3/q6++MiQZ06ZNs7fFxcUZVatWNfz9/Y3o6GiH95KSzwHJ2bBhgyHJmDt3rtNj3nzzTUOSsWXLFnvb1atXjUKFChkFCxa0/85feOEFo3Tp0nd9vSxZshh9+vS56zHJad68ueHl5WUcOXLE3nb69Gkjc+bMRq1atext/fv3Nzw9PR1+F7GxsUbWrFmN7t2729t69Ohh5MmTx7hw4YLD67Rr187IkiWL/Zre+v0ULlw42eucHGfjc0nGzJkz7cfd+lw4cuRIh6y3xuNxcXGGYaS8L6xfv96QZPTt2zdJptvH4in97Peg1wpA6mM5FwDplre3t7p165ak3dfX17599epVXbhwQTVr1tSNGzd04MCBez5v27ZtlS1bNvt+zZo1Jdm+fnkv9erVU5EiRez75cqVU0BAgP3cxMRErV27Vs2bN3eYxVO0aFGFhobe8/lTYufOnYqMjFTv3r3l4+Njb2/cuLFKlChh/8qor6+vvLy8tHHjRl2+fDnZ57o1Y33ZsmWKj49PcYa1a9cqLi5Ob775psM6hy+99JICAgLsGSwWi1q3bq0VK1bo2rVr9uNmz56tfPnyqUaNGpKkiIgIRUVFqX379rpw4YL9x93dXVWqVNGGDRuSZOjVq1eK80rSt99+q4iICIeflStXJjmuefPmypcvn32/cuXKqlKlilasWCFJOnPmjH7//Xd17dpV2bNntx9Xrlw5Pf/88/bjrFarFi1apKZNmya7FvudX4V++eWXHdpq1qypxMREHT9+XNKDXysAAJAxuLu7q127dtq2bZvDEikzZsxQrly5VLduXUm2Mfat8VtiYqIuXrwof39/Pfnkk/e9BMWtcU/fvn0d2u+coSw5juHj4+N18eJFFS1aVFmzZn3gpS9WrFih3Llzq3379vY2T09P9e3bV9euXdOmTZscjn+YzwEpyVK5cmX7+FaS/P399fLLL+vYsWPat2+fJNuY7p9//rnrMjJZs2bVL7/8otOnT6f49RMTE7VmzRo1b95chQsXtrfnyZNHHTp00I8//mhfXqVt27aKj493+EbjmjVrFBUVpbZt20qSDMPQ/Pnz1bRpUxmG4TBGb9Cgga5cuZLkuoWFhTlc53t54YUXkozPIyIiVKdOHYfjPDw89Morr9j3vby89MorrygyMlK//vqrpJT3hfnz58tisejjjz9OkufO8fm9PvtJD3atAJiDIjqAdCtfvnzJfgVw7969atGihbJkyaKAgAAFBgbab0hz5cqVez7vE0884bB/ayDtrNB8t3NvnX/r3MjISN28eVNFixZNclxybQ/iVlH1ySefTPJYiRIl7I97e3tr+PDhWrlypXLlyqVatWrps88+09mzZ+3Hh4SEqGXLlho0aJBy5sypF154QeHh4YqNjX2gDF5eXipcuLD9cck2SL9586aWLFkiSbp27ZpWrFih1q1b2weqhw4dkiQ999xzCgwMdPhZs2ZNkrU9PTw8lD9//nv/sm5TuXJl1atXz+HnzgG6JBUrVixJW/Hixe0fRu/2+y9ZsqQuXLig69ev6/z584qOjlaZMmVSlO9e/fJBrxUAAMg4bt0n5tb62v/884+2bNmidu3a2ZfQs1qt+vLLL1WsWDF5e3srZ86cCgwM1B9//JGisfTtjh8/Ljc3N4dCo5T8OOnmzZv66KOPVKBAAYfXjYqKuu/Xvf31ixUrluTmlbeWf7l9TCo93OeAlGRxNj68Pct7770nf39/Va5cWcWKFVOfPn20detWh3M+++wz7dmzRwUKFFDlypU1cODAexb6z58/rxs3bjjNYLVadfLkSUlS+fLlVaJECfuygZJtkkvOnDn13HPP2Z8vKipKP/zwQ5Lx+a2JTneO0QsVKnTXjHfKnz9/kvF5vXr17Eso3pI3b15lypTJoa148eKS5DBGT0lfOHLkiPLmzeswGcaZe332kx7sWgEwB0V0AOlWcrMYoqKiFBISot27d+uTTz7R0qVLFRERoeHDh0uyfSi4l1sfIO5kGMZjPdcMb775pv766y8NGzZMPj4++vDDD1WyZEnt2rVLkm22xbx587Rt2za99tpr9hsHPf300w4zxx/Gs88+q4IFC2rOnDmSpKVLl+rmzZv2WS7Sv9dt6tSpyc5GWbx4scNz3j6DKr24V99KjWsFAABc29NPP60SJUpo5syZkqSZM2fKMAyHm7APHTpU/fr1U61atTRt2jStXr1aERERKl26dIrG0g/q9ddf15AhQ9SmTRvNmTNHa9asUUREhHLkyPFYX/d2aWEsX7JkSR08eFCzZs1SjRo1NH/+fNWoUcNhZnSbNm30999/a9SoUcqbN69GjBih0qVLJ/stygfVtm1bbdiwQRcuXFBsbKyWLFmili1bysPDduu9W9ekU6dOyY7PIyIiVL16dYfnvJ9Z6K4gJf0lNa4VgEeDG4sCyFA2btyoixcvasGCBapVq5a9/ejRoyam+ldQUJB8fHx0+PDhJI8l1/YggoODJUkHDx60zxS55eDBg/bHbylSpIjefvttvf322zp06JCeeuopjRw5UtOmTbMf8+yzz+rZZ5/VkCFDNGPGDHXs2FGzZs1Sz54975nh9q+LxsXF6ejRo6pXr57D8W3atNHXX3+t6OhozZ49WwULFtSzzz7rkFGy/f7uPDe13ZoVf7u//vrLfrPQ29/7nQ4cOKCcOXMqU6ZM8vX1VUBAgPbs2fNI893vtQIAABlLx44d9eGHH+qPP/7QjBkzVKxYMT3zzDP2x+fNm6c6depowoQJDudFRUUpZ86c9/VawcHBslqtOnLkiMMM6OTGSfPmzVNYWJhGjhxpb4uJiVFUVJTDcXcuqXGv1//jjz9ktVodJljcWuLxznHx4xQcHOx0fHhnlkyZMqlt27Zq27at4uLi9OKLL2rIkCHq37+/fbnGPHnyqHfv3urdu7ciIyNVsWJFDRkyxOkSkYGBgfLz83Oawc3NTQUKFLC3tW3bVoMGDdL8+fOVK1cuRUdHq127dg7PlzlzZiUmJpo+Pj99+rSuX7/uMBv9r7/+kiSHMXpK+kKRIkW0evVqXbp0KUWz0VPifq8VAHOkr2l4AHAPt2YD3P6v/3Fxcfruu+/MiuTA3d1d9erV06JFixzWxTt8+PAjm41QqVIlBQUF6fvvv3dYymPlypXav3+/GjduLEm6ceOGYmJiHM4tUqSIMmfObD/v8uXLSWbePPXUU5J012VC6tWrJy8vL33zzTcO50+YMEFXrlyxZ7ilbdu2io2N1eTJk7Vq1Sq1adPG4fEGDRooICBAQ4cOTXa97/PnzzvN8qgtWrRIp06dsu9v375dv/zyi30QnCdPHj311FOaPHmyw4e+PXv2aM2aNWrUqJEkyc3NTc2bN9fSpUu1c+fOJK9zvzOeHvRaAQCAjOXWrPOPPvpIv//+u8MsdMk2Xr1zTDF37lyH8U9K3RofffPNNw7tX331VZJjk3vdUaNGKTEx0aHtVqH0zuJ6cho1aqSzZ886LEuSkJCgUaNGyd/fXyEhISl5G49Eo0aNtH37dm3bts3edv36df3www8qWLCgSpUqJUm6ePGiw3leXl4qVaqUDMNQfHy8EhMTkyxvExQUpLx58951zOfu7q769etr8eLFDmvinzt3TjNmzFCNGjUUEBBgby9ZsqTKli2r2bNna/bs2cqTJ4/DJCV3d3e1bNlS8+fPT3ZSSGqOzxMSEjR27Fj7flxcnMaOHavAwEA9/fTTklLeF1q2bCnDMDRo0KAkr3O/4/MHvVYAzMFMdAAZSrVq1ZQtWzaFhYWpb9++slgsmjp1appaTmXgwIFas2aNqlevrl69eikxMVGjR49WmTJl9Pvvv6foOeLj4/W///0vSXv27NnVu3dvDR8+XN26dVNISIjat2+vc+fO6euvv1bBggX11ltvSbLNzqhbt67atGmjUqVKycPDQwsXLtS5c+fss0wmT56s7777Ti1atFCRIkV09epVjRs3TgEBAfZicHICAwPVv39/DRo0SA0bNlSzZs108OBBfffdd3rmmWfsa9TfUrFiRRUtWlQDBgxQbGysw1IukhQQEKAxY8aoc+fOqlixotq1a6fAwECdOHFCy5cvV/Xq1TV69OgU/e6cWblyZbI3nq1WrZrDbPqiRYuqRo0a6tWrl2JjY/XVV18pR44c+s9//mM/ZsSIEQoNDVXVqlXVo0cP3bx5U6NGjVKWLFk0cOBA+3FDhw7VmjVrFBISopdfflklS5bUmTNnNHfuXP3444/2m4WmxINeKwAAkLEUKlRI1apVsy+Hd2cRvUmTJvrkk0/UrVs3VatWTX/++aemT5/uMB5Kqaeeekrt27fXd999pytXrqhatWpat25dst/AbNKkiaZOnaosWbKoVKlS2rZtm9auXascOXIkeU53d3cNHz5cV65ckbe3t5577jkFBQUlec6XX35ZY8eOVdeuXfXrr7+qYMGCmjdvnrZu3aqvvvpKmTNnvu/3dDfz589PdjwZFham999/XzNnzlRoaKj69u2r7Nmza/LkyTp69Kjmz59vnx1dv3595c6dW9WrV1euXLm0f/9+jR49Wo0bN1bmzJkVFRWl/Pnzq1WrVipfvrz8/f21du1a7dixw2EWf3L+97//KSIiQjVq1FDv3r3l4eGhsWPHKjY2Vp999lmS49u2bauPPvpIPj4+6tGjR5LlEj/99FNt2LBBVapU0UsvvaRSpUrp0qVL+u2337R27VpdunTpIX6bts8rt3879pZcuXLp+eeft+/nzZtXw4cP17Fjx1S8eHHNnj1bv//+u3744Qd5enpKSnlfqFOnjjp37qxvvvlGhw4dUsOGDWW1WrVlyxbVqVNHr732WorzX7169YGvFQATGADg4vr06WPc+ddZSEiIUbp06WSP37p1q/Hss88avr6+Rt68eY3//Oc/xurVqw1JxoYNG+zHhYWFGcHBwfb9o0ePGpKMESNGJHlOScbHH39s3//444+TZJJk9OnTJ8m5wcHBRlhYmEPbunXrjAoVKhheXl5GkSJFjPHjxxtvv/224ePj4+S38K+wsDBDUrI/RYoUsR83e/Zso0KFCoa3t7eRPXt2o2PHjsY///xjf/zChQtGnz59jBIlShiZMmUysmTJYlSpUsWYM2eO/ZjffvvNaN++vfHEE08Y3t7eRlBQkNGkSRNj586d98xpGIYxevRoo0SJEoanp6eRK1cuo1evXsbly5eTPXbAgAGGJKNo0aJOn2/Dhg1GgwYNjCxZshg+Pj5GkSJFjK5duzrkCQsLMzJlypSifIZhGOHh4U5/n5KM8PBwwzAc+8fIkSONAgUKGN7e3kbNmjWN3bt3J3netWvXGtWrVzd8fX2NgIAAo2nTpsa+ffuSHHf8+HGjS5cuRmBgoOHt7W0ULlzY6NOnjxEbG+uQb8eOHUl+F7f36Ye9VgAAIOP49ttvDUlG5cqVkzwWExNjvP3220aePHkMX19fo3r16sa2bduMkJAQIyQkxH7crbHRrbGSYSQ/Rr5586bRt29fI0eOHEamTJmMpk2bGidPnkwyvr58+bLRrVs3I2fOnIa/v7/RoEED48CBA8mOpceNG2cULlzYcHd3dxgP3ZnRMAzj3Llz9uf18vIyypYt65D59veSks8Bybk1LnP2s2XLFsMwDOPIkSNGq1atjKxZsxo+Pj5G5cqVjWXLljk819ixY41atWoZOXLkMLy9vY0iRYoY7777rnHlyhXDMAwjNjbWePfdd43y5csbmTNnNjJlymSUL1/e+O677+6a8ZbffvvNaNCggeHv72/4+fkZderUMX766adkjz106JD9Pfz444/JHnPu3DmjT58+RoECBQxPT08jd+7cRt26dY0ffvghye9n7ty5KcpoGMZdf5+3X+Nbnwt37txpVK1a1fDx8TGCg4ON0aNHJ5v1Xn3BMAwjISHBGDFihFGiRAnDy8vLCAwMNEJDQ41ff/3VId+9Pvs97LUCkLoshpGGpl8CAJxq3ry59u7dm+ya2zDfsWPHVKhQIY0YMULvvPOO2XEAAACADK927dq6cOHCI7/PEICMhzXRASANunnzpsP+oUOHtGLFCtWuXducQAAAAAAAABkUa6IDQBpUuHBhde3aVYULF9bx48c1ZswYeXl5OayrDQAAAAAAgMePIjoApEENGzbUzJkzdfbsWXl7e6tq1aoaOnSoihUrZnY0AAAAAACADIU10QEAAAAAAAAAcII10QEAAAAAAAAAcIIiOgAAAAAAAAAATrAmegpYrVadPn1amTNnlsViMTsOAAAA0inDMHT16lXlzZtXbm4Zd74L428AAACkhpSOvymip8Dp06dVoEABs2MAAAAggzh58qTy589vdgzTMP4GAABAarrX+JsiegpkzpxZku2XGRAQYHKajMtqter8+fMKDAzM0DOzkDz6B5yhb8AZ+gaciomR0bmzYuPj5TVjhtz8/FLtpaOjo1WgQAH7+DOjYvydNvD3JO6G/gFn6Btwhr4Bp1xg/E0RPQVufYU0ICCAQbyJrFarYmJiFBAQwF+2SIL+AWfoG3CGvgGnvLxkeHoq1jDkFRCQqoP4WzL6EiaMv9MG/p7E3dA/4Ax9A87QN+CUC4y/6bEAAAAAAAAAADhBER0AAAAAAAAAACcoogMAAAAAAAAA4ARFdAAAAAAAAAAAnKCIDgAAAAAAAACAEx5mBwAAAADSFA8PGc2bK/bqVXl5MFwGAAAAHisXGH+nzVQAAACAWTw8pO7ddTMyUpnT6CAeAAAASDdcYPzNci4AAAAAAAAAADhBER0AAAC4nWFIkZFyu3DBtg0AAADg8XGB8XfanB8PAAAAmCU2VpaePRUQFyctWiT5+ZmdCAAAAEi/XGD8zUx0AAAAAAAAAACcoIgOAAAAAAAAAIATFNEBAAAAAAAAAHCCInoad/KktGCB2SkAAACAjGHJwSXadWaX2TEAAACQhlBET8M6d5aCg6UOHaTLl81OAwAAAKRv327/Vi1mt1CjGY10POq42XEAAACQRlBET8Ny5JAMQ4qNlaZPNzsNAAAAkH4lWBM0Y88MWQ2rzl47q9Dpobp8k5ksAAAAoIiepvXo8e/2uHG2gjoAAAAeM3d3GaGhiq1bV3J3NzsNUomHm4cWt1usYtmLSZL2X9ivFrNbKDYh1uRkAAAA6ZwLjL8poqdhZctKVarYtv/4Q/r1V3PzAAAAZAienlKvXroZFmbbRoaR0y+nVnZcqUC/QEnSpuOb1G1xN1kNq8nJAAAA0jEXGH9TRE/jevb8d3v8ePNyAAAAABlBkexFtKzDMvl6+EqSZu6ZqQ/WfWByKgAAAJiJInoa17atlCmTbXvGDOn6dXPzAAAApHuGIV25Ikt0NOvpZVCV81XW7Faz5WaxfVwavnW4xuwYY3IqAACAdMoFxt8U0dO4zJmldu1s21evSnPnmpsHAAAg3YuNlaVzZ2V57TXbHd6RITV9sqlGh46277+28jUtObjExEQAAADplAuMvymiuwCWdAEAAABSX69neum96u9JkqyGVe3mtdOOUztMTgUAAIDURhHdBVSpIpUqZdveulXav9/cPAAAAEBGMbTuULUrY/tq6M2Em2oys4n+vvy3yakAAACQmiiiuwCLxXE2+sSJ5mUBAAAAMhI3i5smvTBJIcEhkqTI65EKnR6qizcumpwMAAAAqYUiuovo3Fny9LRtT54sxcWZmwcAAADIKLw9vLWw7UKVCrR9PfSvi3+p2axmuhl/0+RkAAAASA0U0V1EzpxSixa27fPnpaVLzc0DAAAAZCTZfLNpRYcVyu2fW5L008mf1HlhZyVaE01OBgAAgMeNIroL4QajAAAAgHmCswZrRYcV8vfylyTN3z9f76x5x+RUAAAAeNwooruQunWl4GDb9urV0okT5uYBAABIl9zdZdStq7gaNSR3d7PTII2pkKeC5raeK3eLrW989ctX+urnr8wNBQAA4MpcYPxNEd2FuLlJPXrYtg1DCg83Nw8AAEC65OkpvfGGbrz88r83pQFu07BoQ41tMta+3291P83fN9/ERAAAAC7MBcbfFNFdTNeutmK6JE2cKCWyBCMAAACQ6npU7KEPa30oSTJkqNPCTvrp5E8mpwIAAMDjQBHdxRQoIDVsaNs+cUJau9bcPAAAAOmOYUgxMbYfwzA7DdKwQbUHKax8mCQpJiFGzWY2018X/zI5FQAAgItxgfE3RXQXxA1GAQAAHqPYWFnatFHWl1+WYmPNToM0zGKx6IemP6he4XqSpIs3Lyp0eqgir0eanAwAAMCFuMD4myK6C2rSRAoKsm0vXiydP29uHgAAACCj8nL30vw281UuVzlJ0t+X/1aTGU10Pe66yckAAADwqFBEd0GenlKY7Vujio+Xpk41Nw8AAADSr4IFC8pisST56dOnT7LHjxs3TjVr1lS2bNmULVs21atXT9u3b0/l1KkrwDtAKzqsUP6A/JKkHad3qP389kq0cgMjAACA9IAiuovq0ePf7fHj0+xyQQAAAHBxO3bs0JkzZ+w/ERERkqTWrVsne/zGjRvVvn17bdiwQdu2bVOBAgVUv359nTp1KjVjp7p8Afm0osMKBXgHSJKW/rVUfVf2lcFAHQAAwOVRRHdRTz4p1axp296/X9q2zdw8AAAASJ8CAwOVO3du+8+yZctUpEgRhYSEJHv89OnT1bt3bz311FMqUaKExo8fL6vVqnXr1qVy8tRXNldZLWizQJ5unpKk73Z+pxE/jTA5FQAAAB6Wh9kB8OB69pS2bLFtjx8vVatmbh4AAACkb3FxcZo2bZr69esni8WSonNu3Lih+Ph4Zc+e3ekxsbGxir3tJlLR0dGSJKvVKqvV+nChU1mdgnU0ruk4dV3cVZL03tr3lC9zPrUv097cYA/AarXKMAyXuwZIHfQPOEPfgDP0DTh1q0/c6h+p2EdS2h8poruwVq2k11+XoqOl2bOlr76SAgLMTgUAAID0atGiRYqKilLXrl1TfM57772nvHnzql69ek6PGTZsmAYNGpSk/fz584qJiXmQqKZqkLuB3nvmPQ3fMVyS1H1xd/km+KpaXtea9WK1WnXlyhUZhiE3N77EDEf0DzhD34Az9A04FROjrLGxik9I0KXISLn5+aXaS1+9ejVFx1FEd2F+flLHjtKYMdKNG9KsWdLLL5udCgAAwMW5ucmoXl3x167Jiw94DiZMmKDQ0FDlzZs3Rcd/+umnmjVrljZu3CgfHx+nx/Xv31/9+vWz70dHR6tAgQIKDAxUgIvOEhnSYIguJlzU+F3jFWeNU4+IHtrSdYtKBZYyO1qKWa1WWSwWBQYGUuxAEvQPOEPfgDP0DTgVFyejTh3FXbumoNy55XaXceOjdrcx6u0ooru4nj1tRXTJtqQLRXQAAICH5OUlvfeerkdGKpOXl9lp0ozjx49r7dq1WrBgQYqO//zzz/Xpp59q7dq1Kleu3F2P9fb2lre3d5J2Nzc3l/6QPabJGJ2+dlorDq1QVEyUGs9srJ97/Kw8mfOYHS3FLBaLy18HPD70DzhD34Az9A0ky8dH1vff143ISPn7+KRq/0jpa9FjXVzFilKFCrbtHTuk3bvNzQMAAID0KTw8XEFBQWrcuPE9j/3ss880ePBgrVq1SpUqVUqFdGmTh5uHZrearYp5KkqSTlw5ocYzGutqbMq+NgwAAIC0gSJ6OtCz57/bEyaYlwMAAADpk9VqVXh4uMLCwuTh4fhl1i5duqh///72/eHDh+vDDz/UxIkTVbBgQZ09e1Znz57VtWvXUjt2muDv5a/lHZYrOEuwJGnX2V1qM6+N4hPjTU4GAACAlKKIng506CDdWr5n2jTJBe+9BAAAkHbExMjSrJmydunCwOr/rV27VidOnFD37t2TPHbixAmdOXPGvj9mzBjFxcWpVatWypMnj/3n888/T83IaUpu/9xa2XGlsvlkkyStOrxKvZb3kmEYJicDAABIA1xg/M2a6OlA1qxSq1a2Avrly9LChVL79manAgAAQHpRv359pwXfjRs3OuwfO3bs8QdyQSUDS2pxu8WqN7We4hLjNGHXBAVnCdaHIR+aHQ0AAAD3wEz0dOL2JV3GjzcvBwAAAIDk1QyuqSnNp9j3P9r4kSb/PtnERAAAAEgJiujpRK1aUtGitu3166UjR8zNAwAAACCptmXaasTzI+z7PZf2VMSRCBMTAQAA4F4ooqcTFovjbPSJE83LAgAAAMC5t6u+rT7P9JEkJVgT1HJOS/1x7g+TUwEAAMAZiujpSFiY5O5u2w4PlxISzM0DAAAAICmLxaKvG36tF558QZJ0Ne6qGk1vpH+i/zE5GQAAAJJDET0dyZ1batrUtn3mjLRypbl5AAAAACTP3c1dM1rOUJV8VSRJp66eUuj0UF2JuWJyMgAAANyJIno6ww1GAQAAHpKbm4xKlRRfvrzkxnAZj4+fp5+Wtl+qItmKSJL2RO5RyzktFZcYZ3IyAACAVOQC4++0mQoPrEEDKV8+2/by5dLp0+bmAQAAcDleXtJHH+n622/btoHHKDBToFZ2XKmcfjklSeuOrlPPJT1lGIbJyQAAAFKJC4y/Xa6InpiYqA8//FCFChWSr6+vihQposGDB99zkLlx40ZVrFhR3t7eKlq0qCZNmpQ6gVOZh4fUrZttOzFRmjzZ3DwAAAAA7q5YjmJa0m6JfDx8JElT/5iqDzd8aHIqAAAA3OJyRfThw4drzJgxGj16tPbv36/hw4frs88+06hRo5yec/ToUTVu3Fh16tTR77//rjfffFM9e/bU6tWrUzF56une/d/tCRMkq9W8LAAAAADurWqBqprx4gxZZJEkDdkyRON+HWdyKgAAAEguWET/6aef9MILL6hx48YqWLCgWrVqpfr162v79u1Oz/n+++9VqFAhjRw5UiVLltRrr72mVq1a6csvv0zF5KmnUCGpbl3b9pEj0ubN5uYBAABwKTExsrRuraw9e0oxMWanQQbSomQLfdXwK/t+r+W9tPLQSvMCAQAApAYXGH+7XBG9WrVqWrdunf766y9J0u7du/Xjjz8qNDTU6Tnbtm1TvXr1HNoaNGigbdu2PdasZuIGowAAAA8hNlaK4+aOSH19q/RVv2f7SZISjUS1nttav57+1eRUAAAAj1kaH397mB3gfr3//vuKjo5WiRIl5O7ursTERA0ZMkQdO3Z0es7Zs2eVK1cuh7ZcuXIpOjpaN2/elK+vr8NjsbGxio2Nte9HR0dLkqxWq6wusjZKs2ZS9uwWXbpk0bx5hr7+2lC2bGanejhWq1WGYbjMNUDqon/AGfoGnKFvwKlbfeJW/0jFPkJ/hCSNqD9CJ6NPau6+uboef12NZzTWzz1/VsGsBc2OBgAAkCG5XBF9zpw5mj59umbMmKHSpUvb1zjPmzevwsLCHslrDBs2TIMGDUrSfv78ecWk0a8UJOfFFzNr/PhMio21aOzYq+re/YbZkR6K1WrVlStXZBiG3Nxc7ksUeMzoH3CGvgFn6BtwKiZGWWNjFZ+QoEuRkXLz80u1l7569WqqvRbSLjeLm6a0mKIz187oxxM/6tz1cwqdHqqt3bcqu292s+MBAABkOC5XRH/33Xf1/vvvq127dpKksmXL6vjx4xo2bJjTInru3Ll17tw5h7Zz584pICAgySx0Serfv7/69etn34+OjlaBAgUUGBiogICAR/huHq/XXvt3KZfZszPrvff8ZbGYm+lhWK1WWSwWBQYGUuxAEvQPOEPfgDP0DTgVEyN5e0uSgoKCUrWI7uPjk2qvhbTNx8NHi9stVrUJ1XTw4kEduHBAzWc115rOa+TjQT8BAABITS5XRL9x40aSD7ru7u53/epr1apVtWLFCoe2iIgIVa1aNdnjvb295f3/H5xu5+bm5lIfssuXl6pUkX75RfrjD4t27bKoUiWzUz0ci8XictcBqYf+AWfoG3CGvoFkubnJkCQT+gd9EbfL7ptdKzuu1LMTnlXk9UhtObFFXRd11YyWM+Rmoa8AAACkFpcbeTVt2lRDhgzR8uXLdezYMS1cuFBffPGFWrRoYT+mf//+6tKli33/1Vdf1d9//63//Oc/OnDggL777jvNmTNHb731lhlvIVVxg1EAAADAdRXKVkjLOyyXn6ftGxGz987W+2vfNzkVAABAxuJyRfRRo0apVatW6t27t0qWLKl33nlHr7zyigYPHmw/5syZMzpx4oR9v1ChQlq+fLkiIiJUvnx5jRw5UuPHj1eDBg3MeAupqm1bKVMm2/aMGdL16+bmAQAASPPc3KQyZZTw5JO2bcBklfJW0uxWs+2zz0f8NELfbv/W5FQAAACPiAuMvy2GYRhmh0jroqOjlSVLFl25csWl1kS/pWdPacIE23Z4uNS1q6lxHpjValVkZKRtbdI0+gcK5qF/wBn6Bpyhb+BuzOofrj7ufFT4PSRv7M6xenX5q5JsNx9d0GaBXijxwmN7Pf6exN3QP+AMfQPO0DdwN2l9/E2PzQBY0gUAAABwfa9UekX9a/SXJFkNq9rPb69f/vnF5FQAAADpH0X0DKBKFal0adv21q3SgQPm5gEAAADwYIY8N0QdynaQJN1MuKkmM5vo8KXDJqcCAABI3yiiZwAWi9Sjx7/7t5Z2AQAAQDJiYmTp1ElZeveWYmLMTgM4sFgsmthsomoXrC1JunDjgkKnh+rCjQvmBgMAAHhQLjD+poieQXTuLHl62rYnT5bi4szNAwAAkKZFR8ty7ZrZKYBkeXt4a2HbhSodaPu66eFLh9VsZjPdjL9pcjIAAIAHlMbH3xTRM4icOaUWLWzb589LS5eamwcAAADAg8vqk1UrOq5Q3sx5JUnb/tmmjgs6KtGaaHIyAACA9IciegbCDUYBAACA9OOJLE9oeYfl8vfylyQtPLBQ/Vb3k2EYJicDAABIXyiiZyB160rBwbbt1aulEyfMzQMAAADg4TyV+ynNbzNfHm4ekqRvtn+jL3/+0uRUAAAA6QtF9AzEze3fG4wahhQebm4eAAAAAA+vfpH6Gtd0nH3/7TVva+7euSYmAgAASF8oomcwXbvaiumSNHGilMiSiQAAAIDL6/pUVw0MGWjf77yws3488aN5gQAAANIRiugZTIECUsOGtu0TJ6S1a83NAwAAkOa4uUnFiimxUKF/Zx8ALuCjkI/U7alukqTYxFi9MOsFHbxw0ORUAAAA9+AC4++0mQqPFTcYBQAAuAsvLxkjR+rqoEGSl5fZaYAUs1gsGttkrOoXqS9JunTzkkKnh+rctXMmJwMAALgLFxh/U0TPgJo0kYKCbNuLF0vnz5ubBwAAAMCj4enuqbmt56p8rvKSpKNRR9VkZhNdj7tucjIAAADXRRE9A/L0lMLCbNvx8dLUqebmAQAAAPDoBHgHaEXHFSoQUECStPP0TrWd11YJ1gSTkwEAALgmiugZVI8e/26PHy8ZhnlZAAAA0pTYWFl69lRAv35SbKzZaYAHkjdzXq3suFJZvLNIkpYfWq7XVrwmg4E/AABIa1xg/E0RPYN68kmpZk3b9v790rZt5uYBAABIMwxDioyU24ULzDSASysdVFoL2y6Up5unJGnsr2P16Y+fmpwKAADgDi4w/qaInoFxg1EAAAAgfatTqI7CXwi373+w/gNN/2O6iYkAAABcD0X0DKxVKykgwLY9e7YUHW1uHgAAAACPXsdyHTX0uaH2/W6Lu2nD0Q0mJgIAAHAtFNEzMD8/qWNH2/aNG9KsWebmAQAAAPB4vF/jfb3y9CuSpHhrvFrMbqE9kXtMTgUAAOAaKKJncCzpAgAAAKR/FotFoxuNVpPiTSRJV2KvqNH0Rjp99bTJyQAAANI+iugZXMWKUoUKtu0dO6Tdu83NAwAAAODx8HDz0KyWs1QpbyVJ0snok2o0vZGiY1nXEQAA4G4oosNhNvqECeblAAAASBMsFqlAAVnz5rVtA+lIJq9MWtZ+mQpmLShJ2n1ut1rNaaX4xHhzgwEAgIzLBcbfFNGhDh0kHx/b9rRpUkyMuXkAAABM5e0t49tvFf3pp5K3t9lpgEcul38urey4Utl8skmSIv6O0CvLXpFhGCYnAwAAGZILjL8pokNZs0qtW9u2L1+WFi40NQ4AAADSkIIFC8pisST56dOnj9Nz5s6dqxIlSsjHx0dly5bVihUrUjExUqJEzhJa0n6JvN1tH1TDfw/XJ5s+MTkVAABA2kQRHZKkHj3+3eYGowAAALhlx44dOnPmjP0nIiJCktT61iyMO/z0009q3769evTooV27dql58+Zq3ry59uzZk5qxkQI1nqihqS2myiLb16YHbhqo8F3hJqcCAABIeyiiQ5JUq5ZUtKhte/166cgRc/MAAACYJjZWlj59FPD++1JsrNlpTBcYGKjcuXPbf5YtW6YiRYooJCQk2eO//vprNWzYUO+++65KliypwYMHq2LFiho9enQqJ0dKtC7dWp/X/9y+//Kyl7XmyBoTEwEAgAzHBcbfFNEhybZm/+03GJ040bwsAAAApjIM6eRJuZ0+bduGXVxcnKZNm6bu3bvL4uSmT9u2bVO9evUc2ho0aKBt27alRkQ8gLeefUt9K/eVJCVYE9RyTkv9fvZ3c0MBAICMwwXG3x5mB0DaERYmDRggJSZK4eHSoEGSBz0EAAAA/2/RokWKiopS165dnR5z9uxZ5cqVy6EtV65cOnv2rNNzYmNjFXvbrKPo6GhJktVqldVqfbjQSJHPn/9cJ66c0KKDi3Qt7poaTW+kH7v+KF/Dl2uAZFmtVhmGQf9AEvQNOEPfgFO3+sSt/pGKfSSl/ZESKexy55aaNpUWLZLOnJFWrrTtAwAAAJI0YcIEhYaGKm/evI/0eYcNG6ZBgwYlaT9//rxiYmIe6WvBuS9qfKGTl0/q18hfdebaGTWa3kiTQybLMAy5ufElZjiyWq26cuUK/QNJ0DfgDH0DTsXEKGtsrOITEnQpMlJufn6p9tJXr15N0XEU0eGgZ09bEV2y3WCUIjoAAAAk6fjx41q7dq0WLFhw1+Ny586tc+fOObSdO3dOuXPndnpO//791a9fP/t+dHS0ChQooMDAQAUEBDxccNyX5Z2Wq8akGjp86bAOXj6ot7e/rdWdV8vXy9fsaEhjrFarLBaLAgMDKYbBAX0DztA34FRMjOTtLUkKCgpK1SK6j49Pio6jiA4HDRpI+fJJp05Jy5dLp09Lj3iiEQAAAFxQeHi4goKC1Lhx47seV7VqVa1bt05vvvmmvS0iIkJVq1Z1eo63t7e8//+D0+3c3Nz4kJ3KcmXOpZUdV6rqhKq6cOOCtp7eqq5LumpGyxnydPc0Ox7SGIvFwp9TJIu+AWfoG0iWm5sMSTKhf6T0teixcODhIXXrZttOTJQmTzY3DwAAAMxntVoVHh6usLAwedxx05wuXbqof//+9v033nhDq1at0siRI3XgwAENHDhQO3fu1GuvvZbasfGAimYvqmXtl8nXwzb7fN7+eWozr41iE2LvcSYAAED6RBEdSXTv/u/2hAlp9qa4AAAAj4fFIgUFyZozp20bWrt2rU6cOKHutw8U/9+JEyd05swZ+361atU0Y8YM/fDDDypfvrzmzZunRYsWqUyZMqkZGQ+pSv4qmtt6rrzdbd8QWHRgkVrMbqGb8TdNTgYAANIdFxh/WwyDEum9REdHK0uWLLpy5UqGWZPx+eeltWtt2xs2SLVrmxpHkm0GVGRkpG1tJL72gzvQP+AMfQPO0DdwN2b1j4w47kwOv4e0wWq1at5v89R1dVfdTLAVz+sWqqvF7RYrk1cmk9PBbPx/FM7QN+AMfQN3k9bH3/RYJKtnz3+3x483LwcAAAAA89TKX0srOqyQv5e/JGnd0XUKnR6qq7FXTU4GAACQeiiiI1nNm0vZs9u2582TLl82NQ4AAAAAk9QKrqWIzhHK4p1FkrTlxBbVn1ZfUTFR5gYDAABIJRTRkSxvb6lzZ9t2bKw0fbq5eQAAAFJNXJwsb7+tzB9/LMXFmZ0GSBOezf+s1nVZp+y+tpk2P//zs+pOqauLNy6anAwAALg8Fxh/U0SHUz16/Ls9bhw3GAUAABmE1SodOiT3o0dt2wAkSU/nfVobwjYo0C9QkvTbmd9UZ3Idnbt2zuRkAADApbnA+JsiOpwqW1aqUsW2/ccf0q+/mpsHAAAAgLnK5SqnTV03KY9/HknSn5F/qvbk2joVfcrcYAAAAI8RRXTcFTcYBQAAAHC7koEltbnbZhUIKCBJOnDhgEImhejElRMmJwMAAHg8KKLjrtq2lTJlsm3PmCFdv25uHgAAAADmK5q9qDZ326xCWQtJko5cPqJa4bV05NIRk5MBAAA8ehTRcVeZM0vt2tm2r16V5s41Nw8AAACAtKFg1oLa3G2ziucoLkk6fuW4QiaF6OCFgyYnAwAAeLQoouOebl/SZcIE83IAAAAASFvyB+TXpq6bVCqwlCTp1NVTCpkUoj2Re0xOBgAA8OhQRMc9VakilS5t2/7xR+nAAXPzAAAAPHYBATL8/c1OAbiE3P65tTFso57K/ZQk6dz1c6o9qbZ2ndllbjAAAOA60vj4myI67sliYTY6AADIQHx8ZEybpivffSf5+JidBnAJgZkCta7LOj2T9xlJ0sWbF/XclOe0/dR2k5MBAIA0zwXG3xTRkSKdOkmenrbtyZOluDhz8wAAAABIW7L7ZldE5whVK1BNkhQVE6V6U+pp64mtJicDAAB4OBTRkSI5c0otWti2z5+Xli41Nw8AAACAtCeLTxat7rRadQrWkSRdjbuqBtMaaMPRDSYnAwAAeHAU0ZFity/pMn68eTkAAAAeq7g4WT74QP5DhvD1O+AB+Hv5a3mH5WpQpIEk6Xr8dTWa0UirD682ORkAAEiTXGD8TREdKVa3rhQcbNtevVo6ccLcPAAAAI+F1Srt2SOPgwdt2wDum6+nrxa3W6ymxZtKkmISYtRsVjMtObjE5GQAACDNcYHxN0V0pJibm9Sjh23bMKTwcHPzAAAAAEi7vD28Na/NPLUq1UqSFJcYp5ZzWmrevnkmJwMAALg/FNFxX7p2tRXTJWniRCkx0dQ4AAAAANIwL3cvzWw5Ux3LdpQkJVgT1HZeW03/Y7rJyQAAAFKOIjruS4ECUsOGtu0TJ6S1a83NAwAAACBt83Dz0OTmk9X9qe6SJKthVeeFnTVx10STkwEAAKQMRXTct9tvMDphgnk5AAAAALgGdzd3jWs2Tr0q9ZIkGTLUY0kPfbfjO5OTAQAA3BtFdNy3Jk2koCDb9qJF0vnzpsYBAAAA4ALcLG76ttG3euvZt+xtfVb00RfbvjAxFQAAwL1RRMd98/S0rY0uSfHx0tSppsYBAAB49Ly9JS8vs1MA6Y7FYtHI+iP1QY0P7G1vr3lbQ7cMNTEVAAAwXRoff1NExwPp0ePf7fHjJcMwLwsAAMAj5eMjY+5cRY0fL/n4mJ0GSHcsFouG1B2iT2p/Ym8bsH6APtrwkQw+WAAAkPG4wPibIjoeSPHiUs2atu39+6Vt28zNAwAAAMC1fBjyoT6r95l9f/DmwXpv7XsU0gEAQJpDER0P7PYbjI4fb14OAAAAAK7p3erv6puG39j3R/w0Qm+sekNWw2piKgAAAEcU0fHAWrWSAgJs27NnS9HR5uYBAAB4JOLipE8+UaaRI23bAB6r16u8rrFNxsoiiyRp1PZRenXZqxTSAQDIKFxg/E0RHQ/Mz0/q2NG2feOGNGuWuXkAAAAeCatVlp075bl7t2SliAekhpefflmTmk+Sm8X2EXXcb+PUbXE3JVgTTE4GAAAeOxcYf1NEx0NhSRcAAAAAj0KX8l0048UZcre4S5Km7J6iTgs6KT4x3uRkAAAgo6OIjodSsaJUoYJte8cO6Y8/zM0DAAAAwHW1LdNWc1vPlaebpyRp9t7ZajOvjWITYk1OBgAAMjKK6Hhot89GnzDBvBwAAAAAXF+Lki20qN0iebt7S5IWHVikFrNb6Gb8TZOTAQCAjIoiOh5ahw6Sj49te+pUKSbG3DwAAAAAXFujYo20rMMy+Xr4SpJWHl6ppjOb6nrcdZOTAQCAjIgiOh5a1qxS69a27cuXpYULTY0DAAAAIB2oV7ieVnVaJX8vf0nSuqPrFDo9VFdjr5qcDAAAZDQU0fFIcINRAAAAAI9areBaiugcoSzeWSRJW05sUf1p9RUVE2VuMAAAkKFQRMcjUbOmVKyYbXv9eunIEXPzAAAAPDAfHxlLlihqypR/16wDYJpn8z+rdV3WKbtvdknSz//8rLpT6urijYsmJwMAAI+EC4y/KaLjkbBYpB49/t2fONG8LAAAAADSl6fzPq0NYRsU6BcoSfrtzG+qM7mOzl07Z3IyAACQEVBExyMTFia5u9u2w8OlhARz8wAAAABIP8rlKqdNXTcpj38eSdKfkX+q9uTaOn31tLnBAABAukcRHY9M7txS06a27TNnpJUrzc0DAADwQOLipOHDlWnUKNs2gDSjZGBJbe62WQUCCkiSDlw4oFrhtXTiygmTkwEAgAfmAuNviuh4pLjBKAAAcHlWqyxbt8pzxw7JajU7DYA7FM1eVJu7bVahrIUkSUcuH1Gt8Fo6cokbMwEA4JJcYPztckX0ggULymKxJPnp06dPssdPmjQpybE+aXSB+vSgQQMpXz7b9vLlthnpAAAAAPAoFcxaUJu7bVbxHMUlScevHFfIpBAdvHDQ5GQAACA9crki+o4dO3TmzBn7T0REhCSpdevWTs8JCAhwOOf48eOpFTfD8fCQunWzbScmSpMnm5sHAAAAQPqUPyC/NnXdpFKBpSRJp66eUsikEO2J3GNyMgAAkN64XBE9MDBQuXPntv8sW7ZMRYoUUUhIiNNzLBaLwzm5cuVKxcQZT/fu/26PHy8ZhnlZAAAAAKRfuf1za2PYRj2V+ylJ0rnr51R7Um3tOrPL3GAAACBd8TA7wMOIi4vTtGnT1K9fP1ksFqfHXbt2TcHBwbJarapYsaKGDh2q0qVLOz0+NjZWsbGx9v3o6GhJktVqlTWNrsuTlgQHS3XrWrRunUVHjkgbNlhVu/bDP6/VapVhGFwDJIv+AWfoG3CGvgGnbvWJW/0jFfsI/RG4f4GZArW+y3o1mNZAO07v0MWbF/XclOe0utNqVc5X2ex4AAAgHXDpIvqiRYsUFRWlrl27Oj3mySef1MSJE1WuXDlduXJFn3/+uapVq6a9e/cqf/78yZ4zbNgwDRo0KEn7+fPnFRMT86jip2utW/to3bqskqRvv41VqVJXHvo5rVarrly5IsMw5Obmcl+iwGNG/4Az9A04Q9+AUzExyhobq/iEBF2KjJSbn1+qvfTVq1dT7bXux6lTp/Tee+9p5cqVunHjhooWLarw8HBVqlTJ6TnTp0/XZ599pkOHDilLliwKDQ3ViBEjlCNHjlRMjowim282re2yVo2mN9LWk1sVFROlelPqaWXHlar+RHWz4wEAABdnMQzXXWyjQYMG8vLy0tKlS1N8Tnx8vEqWLKn27dtr8ODByR6T3Ez0AgUK6PLlywoICHjo3BlBbKyUP79Fly5Z5O1t6NQpQ9myPdxzWq1WnT9/XoGBgRQ7kAT9A87QN+AMfQNOxcRIbdooLjZWHgsXpmoRPTo6WtmyZdOVK1fSzLjz8uXLqlChgurUqaNevXopMDBQhw4dUpEiRVSkSJFkz9m6datq1aqlL7/8Uk2bNtWpU6f06quvqnjx4lqwYME9XzM6OlpZsmRJU7+HjMhqtSoyMlJBQUEu8/fktbhrajazmTYc2yBJyuSZSUvbL1WdQnVMTpb+uGL/QOqgb8AZ+gaciomR0aqVYuPi5LVoUaqPv1My7nTZmejHjx/X2rVrUzQIv52np6cqVKigw4cPOz3G29tb3t7eSdrd3Nz4Q55Cvr5S587S119LsbEWzZxp0WuvPfzzWiwWrgOcon/AGfoGnKFvIFm+vrLOmaOoyEgF+fqmav9Ii31x+PDhKlCggMLDw+1thQoVuus527ZtU8GCBdW3b1/78a+88oqGDx/+WLMC/l7+Wt5huVrMbqHVR1brevx1NZrRSIvaLlKDog3MjgcAAJLj7S3j1vg7mZpsWpD2RukpFB4erqCgIDVu3Pi+zktMTNSff/6pPHnyPKZkuKVHj3+3x43jBqMAAMBFWCySj4/t5y733ckolixZokqVKql169YKCgpShQoVNG7cuLueU7VqVZ08eVIrVqyQYRg6d+6c5s2bp0aNGqVSamRkvp6+WtxusZoWbypJikmIUbNZzbTk4BKTkwEAgGS5wPjbJWeiW61WhYeHKywsTB4ejm+hS5cuypcvn4YNGyZJ+uSTT/Tss8+qaNGiioqK0ogRI3T8+HH17NnTjOgZStmyUpUq0i+/SH/8If36q3SXZTMBAACQBv39998aM2aM+vXrpw8++EA7duxQ37595eXlpbCwsGTPqV69uqZPn662bdsqJiZGCQkJatq0qb799ttkj09uOUXJNu7nZqvmceUbMHu6eWpOqznqtLCT5u+fr7jEOLWc01LTW0xXq1KtzI6XLrhy/8DjRd+AM/QN3I1Z/SOlr+eSRfS1a9fqxIkT6t69e5LHTpw44fA12MuXL+ull17S2bNnlS1bNj399NP66aefVKpUqdSMnGH17GkrokvS+PEU0QEAgAuIj5dGj5bf1avSe+9JafQrpanFarWqUqVKGjp0qCSpQoUK2rNnj77//nunRfR9+/bpjTfe0EcffaQGDRrozJkzevfdd/Xqq69qwoQJSY4fNmyYBg0alKT9/PnziomJebRvCCmWHm7A/FWNr2TEG1pweIESrAlqv6C9zl8+r5bFWpodzeWlh/6Bx4O+AWfoG3AqPl6+EyfKEhOjyFdflVsqjr+vXr2aouNc+saiqYUbGz24q1elPHmk69elzJmlM2ekTJke7Lm4AQXuhv4BZ+gbcIa+Aadc4MZGqSk4OFjPP/+8xo8fb28bM2aM/ve//+nUqVPJntO5c2fFxMRo7ty59rYff/xRNWvW1OnTp5MsrZjcTPQCBQro8uXLaeb3kBGllxswJ1oT9cryVxT+u21df4ss+qHpD+r+VNJJWUi59NI/8OjRN+AMfQNOxcRIbdooLjZWHgsXpvr4O1u2bOn3xqJwDZkzS+3aSRMm2Arq8+ZJTiYsAQAAIA2qXr26Dh486ND2119/KTg42Ok5N27cSLLsoru7uyQpuTk83t7e8k5mxhE3/jVfergBs5ubm8Y3Gy8fDx+N2TlGhgy9tPQlxSXGqfczvc2O59LSQ//A40HfgDP0DSTLzU2GJJnQP1L6WvRYPHa3Lz9/2wQmAAAAuIC33npLP//8s4YOHarDhw9rxowZ+uGHH9SnTx/7Mf3791eXLl3s+02bNtWCBQs0ZswY/f3339q6dav69u2rypUrK2/evGa8DWRwbhY3fdvoW7317Fv2tj4r+ujLbV+amAoAALgKiuh47KpUkUqXtm3/+KN04IC5eQAAAJByzzzzjBYuXKiZM2eqTJkyGjx4sL766it17NjRfsyZM2d04sQJ+37Xrl31xRdfaPTo0SpTpoxat26tJ598UgsWLDDjLQCSbLMfR9YfqQ9qfGBv67emn4ZuGWpiKgAA4ApYzgWPncVim43+1v9P+pgwQRoxwtxMAAAASLkmTZqoSZMmTh+fNGlSkrbXX39dr7/++mNMBdw/i8WiIXWHyMfDRx9t/EiSNGD9AMUkxGhQ7UGyWCwmJwQAAGkRM9GRKjp1kry8bNuTJ0txcebmAQAAAJBxfRjyoT6r95l9f/DmwXpv7XvJrtkPAABAER2pImdOqXlz2/b589LSpabGAQAAAJDBvVv9XX3T8Bv7/oifRuiNVW/IalhNTAUAANIiiuhINdxgFAAAuARvbxlTp+rK6NGSt7fZaQA8Rq9XeV1jm4yVRbZlXEZtH6VXl71KIR0AgNTkAuNviuhINXXrSsHBtu3Vq6Xb7j0FAACQdlgsUpYsMgICbNsA0rWXn35Zk5pPkpvF9vF43G/j1G1xNyVYE0xOBgBABuEC42+K6Eg1bm5Sjx62bcOQwsPNzQMAAAAAktSlfBfNeHGG3C3ukqQpu6eo04JOik+MNzkZAABICyiiI1V17WorpkvSxIlSYqKpcQAAAJKKj5fGjJHv5Mm2bQAZQtsybTW39Vx5unlKkmbvna0289ooNiHW5GQAAKRzLjD+poiOVFWggNSwoW37xAlp3Tpz8wAAACSRmCjLypXyXreOf/EHMpgWJVtoUbtF8na3rce66MAitZjdQjfjb5qcDACAdMwFxt8U0ZHquMEoAAAAgLSqUbFGWtZhmXw9fCVJKw+vVNOZTXU97rrJyQAAgFkooiPVNWkiBQXZthctks6fNzUOAAAAADioV7ieVnVaJX8vf0nSuqPrFDo9VFdjr5qcDAAAmIEiOlKdp6dtbXTJtszR1KmmxgEAAACAJGoF11JE5whl8c4iSdpyYouen/q8omKizA0GAABSHUV0mKJHj3+3x4+XDMO8LAAAAACQnGfzP6t1XdYpu292SdIvp35R3Sl1dfHGRZOTAQCA1EQRHaYoXlyqVcu2vX+/tG2buXkAAAAAIDlP531aG8I2KNAvUJL025nfVGdyHUVejzQ5GQAASC0U0WGaO2ejAwAAAEBaVC5XOW3qukl5/PNIkv6M/FO1J9XWmatnTE4GAABSA0V0mKZVKykgwLY9e7YUHW1uHgAAAEmSt7eM8eMV/cUXkre32WkApBElA0tqc7fNKhBQQJK0/8J+1ZpUSyevnDQ5GQAALs4Fxt8U0WEaPz+pY0fb9o0b0qxZ5uYBAACQJFksUlCQrDlz2rYB4P8VzV5Um7ttVsGsBSVJhy8dVq1JtXT08lFzgwEA4MpcYPxNER2m6tnz322WdAEAAACQ1hXMWlCbu25W0exFJUnHoo4pZFKIDl08ZHIyAADwuFBEh6kqVpQqVLBt79gh/fGHuXkAAACUkCBNnCjfmTNt2wBwhwJZCmhz180qmbOkJOlk9EmFTArR/vP7TU4GAIALcoHxN0V0mO722egTJpiXAwAAQJKUkCDLokXyXrkyzQ7iAZgvT+Y82th1o8oGlZUknbl2RiGTQvTHOWYGAQBwX1xg/E0RHabr0EHy8bFtT50qxcSYmwcAAAAAUiIoU5A2hG1QxTwVJUnnb5xXncl19NuZ30xOBgAAHiWK6DBd1qxS69a27cuXpYULTY0DAAAAACmWwy+H1nVZpyr5qkiSLt28pOcmP6df/vnF5GQAAOBRoYiONIEbjAIAAABwVVl9siqic4RqPlFTknQl9orqTa2nLce3mJwMAAA8ChTRkSbUrCkVK2bbXr9eOnLE3DwAAAAAcD8ye2fWyo4r9Vyh5yRJ1+KuqeH0hlp/dL3JyQAAwMOiiI40wWKRevT4d3/iRPOyAAAAAMCDyOSVScvaL1PDog0lSTfib6jxjMZadXiVyckAAMDDoIiONCMsTHJ3t22Hh6fZm/ECAAAAgFO+nr5a1HaRmj3ZTJIUkxCjF2a9oCUHl5icDAAAPCiK6EgzcueWmja1bZ85I61caW4eAACQQXl7yxg9WtFDh0re3manAeCCvD28Na/1PLUu1VqSFJcYp5ZzWmrevnkmJwMAIA1ygfE3RXSkKdxgFAAAmM5ikZ54Qtb8+W3bAPAAPN09NaPlDHUs21GSlGBNUNt5bTX9j+kmJwMAII1xgfE3RXSkKQ0aSPny2baXL7fNSAcAAAAAV+Th5qHJzSer+1PdJUlWw6rOCztr4i5uAgUAgCuhiI40xcND6tbNtp2YKE2ebG4eAACQASUkSDNmyGfBAm7SAuChubu5a1yzcepVqZckyZChHkt66Pud35ucDACANMIFxt8U0ZHmdO/+7/b48ZJhmJcFAABkQAkJssyaJZ9Fi9LsIB6Aa3GzuOnbRt/qzSpv2tt6Le+lr3/+2rxQAACkFS4w/qaIjjSnUCGpXj3b9pEj0qZN5uYBAAAAgIdlsVj0RYMv9F719+xtb65+U59t/czEVAAAICUooiNN4gajAAAAANIbi8WiYXWH6eOQj+1t7619T59s+kQGX8EFACDNooiONKl5cyl7dtv2vHnS5cumxgEAAACAR8JisWhg7YEa+txQe9vHGz/Wf9f/l0I6AABpFEV0pEne3lKXLrbt2Fhp+nRz8wAAAADAo9S/Zn99Uf8L+/7QH4fqnTXvUEgHACANooiONKtHj3+3x43jBqMAAAAA0pe3qr6lbxt9a9//4ucv9NqK12Q1rCamAgAAd6KIjjSrTBmpShXb9h9/SL/+am4eAAAAAHjUej/TW+ObjpdFFknSdzu/0ytLX1GiNdHkZAAA4BaK6EjTbr/B6IQJFvOCAACAjMPLS8bIkbo6cKDk5WV2GgAZQI+KPTSlxRS5WWwf0cfvGq9ui7spwZpgcjIAAFKBC4y/U7WIfvLkSf3zzz/2/e3bt+vNN9/UDz/8kJox4ELatpUyZbJtz5ol3bhBIR0AADxmbm5SsWJKLFzYtu2iGHsDrqVTuU6a1XKWPNw8JElT/5iqjgs6Kj4x3uRkAAA8Zi4w/k7VVB06dNCGDRskSWfPntXzzz+v7du3a8CAAfrkk09SMwpcRObMUrt2tu3oaIuWLfM2NxAAAICLYOwNuJ7WpVtrXut58nTzlCTN2TtHree2VmxCrMnJAADI2FK1iL5nzx5VrlxZkjRnzhyVKVNGP/30k6ZPn65JkyalZhS4kNuXdJk+3c+8IAAAIGNISJAWLJD38uW2bRfF2BtwTS+UeEGL2y2Wt7ttAtHig4vVYnYL3Yy/aXIyAAAeExcYf6dqET0+Pl7e3raBwNq1a9WsWTNJUokSJXTmzJnUjAIXUqWKVLq0bXv7di8dOGBuHgAAkM4lJMgyaZJ8Z89Os4P4lGDsDbiu0GKhWt5huXw9fCVJKw+vVLNZzXQ97rrJyQAAeAxcYPydqkX00qVL6/vvv9eWLVsUERGhhg0bSpJOnz6tHDlypGYUuBCLxXE2+rBhFsXybUYAAIC7YuwNuLa6hetqVadV8vfylySt/XutGs1opKuxV01OBgBAxpOqRfThw4dr7Nixql27ttq3b6/y5ctLkpYsWWL/qimQnE6dJC8vQ5I0bZpFpUpJc+dKhmFyMAAAgDTqUY69T506pU6dOilHjhzy9fVV2bJltXPnzrueExsbqwEDBig4OFje3t4qWLCgJk6c+MDvB8iIagXX0ppOaxTgHSBJ2nx8s+pPq6+omChzgwEAkMF4pOaL1a5dWxcuXFB0dLSyZctmb3/55Zfl58da13AuZ05p2DBD77wjGYZFf/8ttWkjVa0qjRxp+y8AAAD+9ajG3pcvX1b16tVVp04drVy5UoGBgTp06JDDcyanTZs2OnfunCZMmKCiRYvqzJkzslqtD/x+gIyqaoGqWtdlnepPra/LMZf18z8/q96UelrTeY2y+2Y3Ox4AABlCqhbRb968KcMw7APu48ePa+HChSpZsqQaNGiQmlHggt58UypX7qKGDcuh9estkqRt26Rq1WwF9WHDpMKFzc0IAACQVjyqsffw4cNVoEABhYeH29sKFSp013NWrVqlTZs26e+//1b27LYiX8GCBe//TQCQJFXKW0kbwjao3tR6unDjgn4986vqTK6jtZ3XKjBToNnxAABI91K1iP7CCy/oxRdf1KuvvqqoqChVqVJFnp6eunDhgr744gv16tUrNePABZUpk6A1awytWmXRu+9K+/fb2ufMkRYtkl5/XRowQLrHxCgAAIB071GNvZcsWaIGDRqodevW2rRpk/Lly6fevXvrpZdeuus5lSpV0meffaapU6cqU6ZMatasmQYPHixfX98kx8fGxir2tpveREdHS5KsViuz101ktVplGAbXII0oG1RWG7ps0PPTntfZa2f1x7k/VHtSbUV0jlBu/9ypnof+AWfoG3CGvgGnbvWJW/0jFftISvtjqhbRf/vtN3355ZeSpHnz5ilXrlzatWuX5s+fr48++ogiOlLEYpEaN5YaNJDGj5c++kg6f16Ki7Mt7RIebmvr1Uvy8jI7LQAAgDke1dj777//1pgxY9SvXz998MEH2rFjh/r27SsvLy+FhYU5PefHH3+Uj4+PFi5cqAsXLqh37966ePGiw4z2W4YNG6ZBgwYlaT9//rxiYmLu413jUbJarbpy5YoMw5CbW6reTgtO5FROzWs8T62XtdaZ62e078I+1ZpYS3OazFFe/7ypmoX+AWfoG3CGvgGnYmKUNTZW8QkJuhQZKbdUXPb76tWU3bDbYhipd2tGPz8/HThwQE888YTatGmj0qVL6+OPP9bJkyf15JNP6saNG6kV5b5ER0crS5YsunLligICAsyOk2FZrVZFRkYqKCjI4S/b6Ghp+HDpiy+k2z9jFSlia3/xRVvhHembs/4B0DfgDH0DTlmtsv75py5duqTsNWvKzSP15p08ynHnoxp7e3l5qVKlSvrpp5/sbX379tWOHTu0bdu2ZM+pX7++tmzZorNnzypLliySpAULFqhVq1a6fv16ktnoyc1EL1CggC5fvsz420RWq1Xnz59XYGAgf0+mMX9f/lv1ptbT8SvHJUmFshbS2s5rVTBrwVTLQP+AM/QNOEPfgFP/P/6+fPmystWokerj72zZst1z/J2qM9GLFi2qRYsWqUWLFlq9erXeeustSVJkZCSDYzywgABpyBDp1VdtS7lMnWprP3JEatVKqlHDNkO9cmVzcwIAABfh5iaVLauEyEjbtot6VGPvPHnyqFSpUg5tJUuW1Pz58+96Tr58+ewF9FvnGIahf/75R8WKFXM43tvbW97e3kmex83NjQ/ZJrNYLFyHNKhojqLa3G2znpv8nI5cPqKjUUdVZ0odreuyTkWzF021HPQPOEPfgDP0DSTLzU0qX16JkZFy8/BI1f6R0tdK1R770Ucf6Z133lHBggVVuXJlVa1aVZK0Zs0aVahQITWjIB0qUECaMkX69Vepdu1/23/8UapSRWrfXjp2zKx0AAAAqetRjb2rV6+ugwcPOrT99ddfCg4Ovus5p0+f1rVr1xzOcXNzU/78+e/znQBIzhNZntDmbptVImcJSdKJKycUMilEBy4cMDkZAADpT6oW0Vu1aqUTJ05o586dWr16tb29bt269vUagYdVsaK0fr20ZIn05JP/ts+aZdv/z3+kqCjT4gEAgLQuIUFavlzea9fatl3Uoxp7v/XWW/r55581dOhQHT58WDNmzNAPP/ygPn362I/p37+/unTpYt/v0KGDcuTIoW7dumnfvn3avHmz3n33XXXv3j3ZG4sCeDB5M+fVxrCNKhNURpJ0+upphUwK0Z7IPSYnAwDgPrjA+DvVvzuRO3duVahQQadPn9Y///wjSapcubJKlCiR2lGQjlksUtOm0p9/St9+K+XMaWuPi5NGjJCKFpVGjZLi483NCQAA0qCEBFnGjpXvlClpdhCfUo9i7P3MM89o4cKFmjlzpsqUKaPBgwfrq6++UseOHe3HnDlzRidOnLDv+/v7KyIiQlFRUapUqZI6duyopk2b6ptvvnl0bw6AJCmXfy5tCNugCrlt3zCJvB6p2pNqa9eZXSYnAwAghVxg/J2qRXSr1apPPvlEWbJkUXBwsIKDg5U1a1YNHjxYVqs1NaMgg/D0lHr3lg4flt5/X7q11ObFi1LfvlLp0tKiRVLq3V4XAAAgdTzKsXeTJk30559/KiYmRvv379dLL73k8PikSZO0ceNGh7YSJUooIiJCN27c0MmTJzVy5EhmoQOPSU6/nFrXZZ0q57PdCOrizYt6bspz2n5qu8nJAABIH1K1iD5gwACNHj1an376qXbt2qVdu3Zp6NChGjVqlD788MPUjIIMJksWadgw6eBB6bZJUzp0SGrRwraG+s6dpsUDAAB45Bh7AxlLNt9siugcoeoFqkuSomKiVG9KPW09sdXkZAAAuL5ULaJPnjxZ48ePV69evVSuXDmVK1dOvXv31rhx4zRp0qTUjIIMKjhYmjZN2r5dqlnz3/bNm6VnnpE6dZKOHzcvHwAAwKPC2BvIeAK8A7Sq0yrVKVhHknQ17qoaTGugDUc3mJwMAADXlqpF9EuXLiW7/mKJEiV06dKl1IyCDO6ZZ6RNm6SFC6Vixf5tnz7ddvPR/v2lK1fMywcAAPCwGHsDGZO/l7+Wd1iuBkUaSJKux19XoxmNtPrw6nucCQAAnEnVInr58uU1evToJO2jR49WuXLlUjMKIItFat5c2rtX+uYbKUcOW3tsrPTpp7abj373HTcfBQAAromxN5Bx+Xr6alG7RWpSvIkkKSYhRs1mNdOyv5aZnAwAANfkkZov9tlnn6lx48Zau3atqlatKknatm2bTp48qRUrVqRmFMDO01N6/XWpc2dp6FDp66+luDjpwgWpTx9bgf2zz6SmTW2FdwAAAFfA2BvI2Hw8fDS/zXy1n99eC/YvUFxinFrMbqFZLWepZamWZscDAMClpOpM9JCQEP31119q0aKFoqKiFBUVpRdffFF79+7V1KlTUzMKkETWrLZi+cGDUrt2/7YfPCi98IL03HPSr7+aFg8AAKQWT08ZH36oa/362f613UUx9gbg5e6l2a1mq32Z9pKkBGuC2s5rq5l/zjQ5GQAAt3GB8bfFMAzD7BC7d+9WxYoVlZiYaHaUZEVHRytLliy6cuWKAgICzI6TYVmtVkVGRiooKEhubo//339++UV6+21p6x03s+/cWRoyRCpQ4LFHwH1I7f4B10HfgDP0DdyNWf0jNcadaX3sLTH+Tiv4ezL9SLQmqufSnpr0+yRJkkUWTXxhoro+1fWBn5P+AWfoG3CGvoG7Sevjb3os4ESVKtKWLdL8+VKRIv+2T50qFS8uDRggRUeblw8AAAAAUsLdzV0Tmk3QyxVfliQZMtRtcTf98OsPJicDAMA1UEQH7sJikV58Udq3T/rySylbNlt7TIxt/fRixaTvv5cSEszNCQAAHqGEBGndOnlt2cL/5AGkG24WN33f5Hv1rdzX3vbKslc06pdRJqYCAEAuMf6miA6kgJeX9Oab0pEj0u3LM0VGSr16SeXKScuXS+YvjgQAAB5aQoIsX38tv3Hj0uwgHgAehMVi0VcNv9K71d61t/Vd1Vef//S5iakAABmeC4y/PVLjRV588cW7Ph4VFZUaMYCHli2bNHKk1KeP1L+/NGeOrX3/fqlJE9vNR0eOlJ56ytSYAAAgA2PsDeBuLBaLhtcbLh8PHw3ePFiS9G7Eu4pJiNF/a/3X5HQAAKRNqVJEz5Ilyz0f79KlS2pEAR6JwoWl2bNts9Pfflvats3Wvn69VLGi1KWL7eaj+fKZGhMAAGRAjL0B3IvFYtEndT6Rt7u3/rvBVjj/cMOHikmI0eA6g2WxWExOCABA2pIqRfTw8PDUeBkg1VWtKm3dKs2bJ733nnT0qG1Jl8mTbbPU33lHevddKXNms5MCAICMgrE3gJQaUGuAfDx89E7EO5KkIVuGKDYhVp89/xmFdAAAbuNya6IXLFhQFoslyU+fPn2cnjN37lyVKFFCPj4+Klu2rFasWJGKiZHeWSxS69a2JV1GjpSyZrW137wpDR5su/loGl7SCQAAAEAG9na1tzU6dLR9//Ntn6vvyr6yGlYTUwEAkLa4XBF9x44dOnPmjP0nIiJCktS6detkj//pp5/Uvn179ejRQ7t27VLz5s3VvHlz7dmzJzVjIwPw9rbddPTIEdsyL7duPnrunPTyy7Z10leu5OajAAAAANKWPpX76IcmP8gi2+zz0TtG69Vlr1JIBwDg/7lcET0wMFC5c+e2/yxbtkxFihRRSEhIssd//fXXatiwod59912VLFlSgwcPVsWKFTV69OhkjwceVvbs0pdfSvv2SS1b/tu+d6/UqJHUoIG0e7d5+QAAAADgTi89/ZImNZ8kN4utTDDut3HqtribEq2JJicDAMB8qbIm+uMSFxenadOmqV+/fk7Xa9u2bZv69evn0NagQQMtWrTI6fPGxsYqNjbWvh8dHS1Jslqtslr5l3izWK1WGYbhMtegcGHbuug//ii9+65F27fb+mhEhFShgqGuXaVPPjGUN6+5OdMLV+sfSD30DThD34BT7u6yvvOOrkVFKau7u5SKfYT+CMBMXcp3kbe7tzou6KhEI1FTdk9RXGKcpjSfIk93T7PjAQDSK09PGf/5j65fviwvz7T5/xuXLqIvWrRIUVFR6tq1q9Njzp49q1y5cjm05cqVS2fPnnV6zrBhwzRo0KAk7efPn1dMTMwD58XDsVqtunLligzDkJub63yJonhxadEiafFiHw0Zkln//OMuw7AoPFyaNctQ79431KvXdWXKxDovD8NV+wceP/oGnKFv4G6sxYvrypUrirt4MVX7x9WrV1PttQAgOW3LtJWXu5fazmureGu8Zu2ZpdiEWM1qNUte7l5mxwMApEfu7lKNGoqPjLRtp0EuXUSfMGGCQkNDlfcRT+Xt37+/w+z16OhoFShQQIGBgQoICHikr4WUs1qtslgsCgwMdMlix8svS126SKNHWzV0qEVXrlh086abRo7014wZmTRokG12ehr9uyLNc/X+gceHvgFn6Bu4G7P6h4+PT6q9FgA406JkCy1su1At57RUbGKsFh5YqBdnv6h5bebJx4O/pwAAGY/LFtGPHz+utWvXasGCBXc9Lnfu3Dp37pxD27lz55Q7d26n53h7e8vb2ztJu5ubGx+yTWaxWFz6Ovj5Sf/5j9S9u/TJJ9KYMVJCgnTmjEUvv2zRqFHS559L9eubndQ1uXr/wOND34Az9A0kKzFR+ukneV2+LLdGjVK1f9AXAaQVjYs31pL2S9R8VnPdTLip5YeWq9nMZlrUbpH8PP3MjgcASE8SE6WtW+V5+bLthoJpcEyc9hKlUHh4uIKCgtS4ceO7Hle1alWtW7fOoS0iIkJVq1Z9nPGAu8qZU/rmG9vNRlu0+Lf9zz9tNx5t2FDas8e8fAAAZGjx8bJ89pkyffutFB9vdhoAME39IvW1ouMKZfLMJEmK+DtCjaY30rW4ayYnAwCkKy4w/nbJIrrValV4eLjCwsLk4eE4mb5Lly7q37+/ff+NN97QqlWrNHLkSB04cEADBw7Uzp079dprr6V2bCCJ4sWlBQukTZukSpX+bV+9WipfXnrpJenMGfPyAQAAAMjYahesrTWd1yjA27a06abjm9RgWgNdiblicjIAAFKPSxbR165dqxMnTqh79+5JHjtx4oTO3FZ1rFatmmbMmKEffvhB5cuX17x587Ro0SKVKVMmNSMDd1WrlvTLL9L06dITT9jarFZp/HipWDHb0i/Xr5ubEQAAAEDGVK1ANa3tvFZZfbJKkn46+ZPqT6uvyzGXzQ0GAEAqcckiev369WUYhooXL57ksY0bN2rSpEkOba1bt9bBgwcVGxurPXv2qFGjRqmUFEg5NzepQwfp4EHp00+lW/ewvX5d+vhj26z18HDbMlEAAAAAkJqeyfeMNoRtUA7fHJKknWd2qvmS5tp9drfJyQAAePxcsogOpGc+PtJ770mHD0t9+kju7rb206dtNyR9+mlp7VpzMwIAAADIeJ7K/ZQ2dt2oXJlySZL+uvyXqkyooiGbhyjBmmByOgAAHh+K6EAaFRgojR5tu8Fos2b/tu/eLT3/vO1mxXv3mpcPAAAAQMZTJqiMNnfbrLJBZSVJ8dZ4/XfDf1V9YnUduHDA5HQAADweFNGBNK5ECWnxYmn9eqlixX/bV66UypWT+vaVoqJMiwcAAAAggymeo7h+6fGLXn/qdblZbGWF7ae2q8LYCvrq569kNawmJwQA4NGiiA64iDp1pB07pClTpPz5bW1WqzRq1L/rpVsZqwIA8PA8PGS88YZuvPSS5OFhdhoASJO8Pbz1QZUPtKXrFhXPYbtfWUxCjN5a/Zaem/ycjl4+anJCAIDLcIHxN0V0wIW4uUmdO0t//SUNHiz5+traz5+3rZderZq0c6e5GQEAcHkeHlLduoqrWTPNDuIBIK14Nv+z2vXKLvWt3Nfetun4JpX7vpx++PUHGYZhYjoAgEtwgfE3RXTABfn6Sv/9r3TggNSq1b/tv/wiVa4svfKKdOGCefkAAAAAZBx+nn76OvRrre+yXsFZgiVJ1+Ku6ZVlr6jRjEY6FX3K5IQAADwciuiAC3viCWnuXCkiwrZ2uiQZhvTDD9KTT0rffy8lJpqbEQAAl5OYKO3YIY/ff+d/pABwH+oUqqM/ev2hnhV62ttWHV6lMmPKaNof05iVDgBInguMvymiA+lAvXrS7t3S559L/v62tkuXpF69pGeekbZtMzcfAAAuJT5elsGD5f/FF1J8vNlpAMClBHgHaFyzcVreYbny+OeRJEXFRKnzws5qNbeVIq9HmpwQAJDmuMD4myI6kE54eUlvv21bL71Tp3/bd+2yrZXetat07pxp8QAAAABkII2KNdKe3nvUoWwHe9uC/QtU5rsyWrB/gYnJAAC4fxTRgXQmTx5p6lRp82apXLl/2ydPlooXl776Ks3+ox4AAACAdCS7b3ZNf3G65raeq5x+OSVJ52+cV8s5LdV5YWddvnn5/9i77/imqv+P46900tLFaKEMoeyyR0FZAoKUpYBMQYYgyhZwMAQBUfwJXxBFBUEtIuBARFFAQJagsmTLlrJH2S2UztzfH7EpoQ0UaJuWvp+Px33QnHtv7ifpIf3kk5NzHByhiIhI2qiILvKQql8f/v4bpk8HPz9LW2QkDB0K1avDunWOjE5ERERERHKK9uXbs7ffXlqXbW1tm7d7HhVnVOTXI786MDIREZG0URFd5CHm4gIDB8LBg9C7d3L73r3QqBE8+yycOuW4+EREREREJGco4FWAxZ0WM7fNXHzdfQE4E3WG5vOb89LPLxEVG+XgCEVEROxTEV0kBwgIgM8+g82bISQkuf2bb6BcOXjvPYiLc1x8IiIiIiLy8DOZTHSr0o29/ffStGRTa/us7bOoPLMy64+td2B0IiIi9qmILpKD1KplKaTPmgX58lnabtyAESOgUiVYscKx8YmIiIiIyMOviE8Rfu36KzNaziC3a24Ajl09RqMvGzH016HcjL/p4AhFRERsqYguksM4OUGfPnDoEPTvb7kNltvNmkHbtnDsmENDFBERcSwXF4yXXuJm9+6WudFERCTdmUwm+ob0ZVffXdR/pD4ABgbTNk+j2qfV2HJ6i4MjFBGRTJMN8m8V0UVyqLx54eOPYds2qFMnuf3HHyE4GN56C25qAIiIiORELi7QsiWxTZpk2SReRORhUTJvSdb2WMuUplNwd3YH4OClg9T+vDaj14wmLlHzToqIPPSyQf6tIrpIDletGmzcCHPnQoEClraYGBg7FipUgCVLwDAcG6OIiIiIiDy8nJ2cGVZ7GDte2kFIIcsiTmbDzDsb3qHm7JrsOrfLwRGKiEhOpyK6iGAyQbdulildhg0DZ2dLe3g4tG4NLVvC4cOOjVFERCTTmM2wZw8u+/dbfhZOnz7Nc889R758+fDw8KBSpUps27YtTef+8ccfuLi4ULVq1YwNUkSyvWD/YP7q/RcTGk3AxckyEnH3+d3UnF2TiRsmkmBOcHCEIiKSIbJB/q0iuohY+fjAlCmwaxc0apTcvnw5VKwIb7xhWYhURETkoRYXh+mNN/B6912I0zQCV65coW7duri6urJ8+XL27dvHlClTyJMnz13PvXr1Kt27d6dx48aZEKmIPAxcnFwY/fhotrywhUoBlQCIN8fzxpo3qPtFXQ5cPODgCEVEJN1lg/xbRXQRSaFCBVi9Gr79FooUsbTFxcHEiVCuHCxcqCleREREcor33nuPokWLEhYWRq1atQgKCqJp06aULFnyruf27duXLl26ULt27UyIVEQeJtUCq7G1z1ZG1huJk8lSuthyegvVPq3GtE3TMBtZc6SiiIg8nLLmTO0i4nAmE3TsCC1aWIrn//sfxMfDqVOW9ieegOnToXx5R0cqIiIiGWnJkiWEhobSoUMH1q9fT+HChenfvz99+vS543lhYWEcPXqUefPm8fbbb9/x2NjYWGJjY623IyMjATCbzZiz6Fd6cwKz2YxhGPodSKoyo3+4OrnydqO3aVW6Fc//9DyHLh8iJiGGoSuG8uOBH/n8qc8JyhOUYdeX+6PXDrFHfUPsSuoTSf0jE/tIWvujiugickdeXpYies+e8PLL8OuvlvY1a6BKFRg82LIIqY+PQ8MUERGRDHL06FFmzJjBsGHDGDVqFFu3bmXw4MG4ubnRo0ePVM85fPgwI0aMYMOGDbi43P0tx7vvvsv48eNTtF+4cIGYmJgHfgxyf8xmM9euXcMwDJyc9CVmsZWZ/aOEWwmWt1nOxC0T+Xzv5wCsP76eKp9WYVztcXQt1xWTyZShMUja6bVD7FHfELtiYvCLjSU+IYHLERE4eXpm2qWjoqLSdJyK6CKSJmXKwLJlsGQJDBkCx45BQgJMnQoLFsDkydC1q2UEu4iIiDw8zGYzISEhTJw4EYBq1aqxd+9eZs6cmWoRPTExkS5dujB+/HjKlCmTpmuMHDmSYcOGWW9HRkZStGhR/P398dEn9Q5jNpsxmUz4+/ur2CEpOKJ/zGo7i2erPUvvJb05fu04N+Jv8Nrvr/Hb6d+Y3Wo2hX0KZ0occmd67RB71DfErpgYcHcHICAgIFOL6Lly5UrTcSqii0iamUzQujU0bQqTJsH//Z/lde7cOejWDT791DLFS9Wqjo5URERE0ktgYCDlb5u/LTg4mEWLFqV6fFRUFNu2bWPHjh0MHDgQSP76touLCytXruSJJ56wOcfd3R33/9443crJyUlvsh3MZDLp9yB2OaJ/NC7RmN39dvPKilf4bMdnAKz4dwWVP63MR80/okulLhqVngXotUPsUd+QVDk5YQA4oH+k9VrqsSJyzzw8LFO47NsHbdokt2/cCDVqwMCBcOWKw8ITERGRdFS3bl0OHjxo03bo0CGKFSuW6vE+Pj7s2bOHnTt3Wre+fftStmxZdu7cyaOPPpoZYYvIQ8zH3YfZT89maZelBHoFAnA15irPLX6O9gvbc+HGBQdHKCIiDxsV0UXkvgUFweLFsHw5lC5taTOb4eOPLdO/fPZZpq4FISIikj5cXDB69uRmp06Qhvm8H3ZDhw5l06ZNTJw4kSNHjrBgwQJmzZrFgAEDrMeMHDmS7t27A5bRPBUrVrTZAgICyJUrFxUrViR37tyOeigi8pBpUboFe/vvpUulLta2H/b/QIVPKrB4/2IHRiYiIvckG+TfKqKLyANr1gz27LFM75L0vvjiRejTBx57DLZudWx8IiIi98TFBZ55htiWLbNsEp+ZatasyeLFi/n666+pWLEiEyZMYNq0aXTt2tV6zNmzZzlx4oQDoxSRnCqvR17mPzOfhR0Wks8jHwAXoi/wzHfP0G1xN67c1FdkRUSyvGyQf6uILiLpwt0dhg+HAwegU6fk9q1b4dFHLQX1C/pWpYiISLbUqlUr9uzZQ0xMDPv376dPnz42++fMmcO6devsnj9u3Dh27tyZsUGKSI7Wvnx7/un/D63Ltra2zds9j0ozKrHiyAoHRiYiIg8DFdFFJF0VKQLffANr1kCFCpY2w7BM7VKmjGWql4QEx8YoIiJyR2YzHD6M89GjmpdMRCQbKeBVgMWdFvNlmy/xdfcF4HTUaZrNb0bfX/oSFRvl4AhFRCRV2SD/VhFdRDJEo0awYwe8/z74+Fjarl61LDoaEgJ//OHQ8EREROyLi8P0yit4jxsHcXGOjkZERO6ByWSie5Xu7Om3hydLPGlt//TvT6kyswq/H//dgdGJiEiqskH+rSK6iGQYV1cYMgQOHoQePZLbd+2CevWge3c4e9Zh4YmIiIiIyEOqqG9RVjy3ghktZ5Db1bJwU/jVcBrOaciwFcO4GX/TwRGKiEh2oiK6iGS4ggVhzhzYuBGqVk1u/+orKFsWpk6F+HhHRSciIiIiIg8jk8lE35C+7Oq7i/qP1AfAwOD9Te9T7dNqbDm9xcERiohIdqEiuohkmrp1Yds2+OQTyJPH0hYVBa+8AlWqwOrVjo1PREREREQePiXzlmRtj7VMaToFd2d3AA5eOkidz+swes1o4hKz5tQBIiKSdaiILiKZytkZ+vWDQ4fgxRfBZLK0798PTZpAx45w8qRjYxQRERERkYeLs5Mzw2oPY8dLOwgpFAJAopHIOxveodbsWuw+v9vBEYqISFamIrqIOET+/PDpp7BlCzz6aHL7woVQrhy8+y7ExjouPhERERERefgE+wfzZ68/eavhW7g4uQCw6/wuQmaF8O6Gd0kwJzg4QhERyYpURBcRhwoJgT//hC++AH9/S1t0NIwaBRUrwvLljo1PREREREQeLq7OroxpMIYtL2yhYkBFAOLN8YxaM4p6X9Tj4MWDDo5QRESyGhXRRcThnJzg+efh4EEYNMhyG+DIEWjRAlq3hqNHHRujiIjkIC4uGJ07E9OmDbi4ODoaERHJINUCq7GtzzZG1B2Bk8nyJmTz6c1U/bQqH2z6ALNhdnCEIiI5RDbIv1VEF5EsI08e+PBD2LED6tdPbl+yBMqXh3Hj4OZNh4UnIiI5hYsLdOlCzDPPZNkkXkRE0oe7izvvNnmXjc9vpHTe0gDEJMQwZMUQGs9tTPiVcAdHKCKSA2SD/FtFdBHJcipXhvXrYf58CAy0tMXGwvjxlmL6jz+CYTg0RBEREREReYjULlqbnX13MrjWYGvbumPrqDyzMrP/no2hNyAiIjmaiugikiWZTNCli2WKl9deS/4g8tgxaNsWmje37BMREUl3hgEnTuB06pQ+tRURyUE8XT35oPkHrOm+hmK+xQC4HnedF395kZYLWnI68rSDIxQReUhlg/xbRXQRydK8vWHSJNi9G5o0SW5fsQIqVYIRI+D6dcfFJyIiD6HYWEwDB+IzapTlq1AiIpKjNApqxO5+u3mh2gvWtuVHllNxRkXm756vUekiIuktG+TfKqKLSLYQHAwrV8L330PRopa2+Hh47z0oVw6+/TbLflgpIiIiIiLZjI+7D7Ofns3SLksJ9LLMMXk15irPLX6ODgs7cOHGBQdHKCIimUlFdBHJNkwmaNcODhyA0aPBzc3Sfvo0dOniRJs2eVm7VsV0ERERERFJHy1Kt2Bv/710qdTF2rZo/yIqfFKBHw/86LjAREQkU6mILiLZjqcnTJgA//wDrVolt2/Z4kaTJk40bIiK6SIiIiIiki7yeuRl/jPzWdhhIfk88gFwIfoCbb9tS/fF3bkac9WxAYqISIZTEV1Esq1SpeDnny1buXLJFfPff4cnnkDFdBERERERSTfty7fnn/7/8HTZp61tX+3+ioqfVGTFkRUOjExERDKaiugiku21agW7dxt88slVFdNFRERERCTDFPAqwI+dfuTLNl/i6+4LwOmo0zSb34xui7txKvKUgyMUEZGMoCK6iDwUnJ2hbdsYdu82WLDAsthoEhXTRUREREQkvZhMJrpX6c6efnt4ssST1vZ5u+dRZnoZxq4dy424Gw6MUERE0puK6CLyUHF2hmefhb17UTFdRETuj4sLRps2xDZvDi4ujo5GRESyqKK+RVnx3ApmtpxJXo+8ANxMuMlbv79FmY/K8OXOLzEbZgdHKSKSDWSD/FtFdBF5KKmYLiIi983FBXr14uazz2bZJF5ERLIGk8nESyEvcWTQEYY8OgQXJ8vfjTNRZ+j5U09qza7FhuMbHByliEgWlw3ybxXRReShpmK6iIiIiIhktDweeXi/2fspFh79++zfPD7ncdp/156jV446MEIREXkQKqKLSI6gYrqIiKSZYUBEBE4XL+qPgoiI3JMy+crwU+efWN19NZULVLa2L9q/iOCPg3l91etci7nmwAhFRLKgbJB/q4guIjmKiukiInJXsbGYXngBn2HDIDbW0dGIiEg29ETQE2x/cTuzn5pNgdwFAIhLjGPyn5MpPb00M7fNJMGc4OAoRUSyiGyQf6uILiI5korpIiIiIiKSkZydnHmh+gscHnSYkfVG4u7sDsCF6Av0W9qPqjOrsvLflQ6OUkRE0kJFdBHJ0VRMFxERERGRjOTt7s3ExhM5MPAAnSp0srb/c+EfQueF0nJBS/Zf2O/ACEVE5G5URBcRQcV0ERERERHJWMX9ivNN+2/4o9cf1Cpcy9q+7PAyKs2oxKBlg7gUfcmBEYqIiD0qoouI3ELFdBERERERyUh1itbhr95/Ma/tPIr4FAEg0Ujko60fUWp6Kd7/633iEuMcHKWIiNxKRXQRkVSomC4iIiIiIhnFyeRE18pdOTjwIG81fAtPV08ArsZcZdjKYVT8pCI/HfgJQ282RESyBBXRRUTuQMV0ERERERHJKJ6unoxpMIbDgw7Ts2pPTJgAOHz5MG2+bUPjuY3ZeW6nY4MUEREV0UVE0kLFdBGRHMTZGaN5c2IbN7b8ARAREclghbwLEdY6jG0vbuPxYo9b29ceW0v1T6vzwpIXOHf9nAMjFBHJQNkg/1YRXUTkHqSlmN6gAaxZo2K6iEi25eoK/fpxs0cPy88iIiKZpHpgddb1WMeijosokacEAAYGn+/4nNLTSzNxw0Ruxt90cJQiIuksG+TfKqKLiNyHOxXTN2yAxo1VTBcRERERkXtnMpl4JvgZ9vXfx+QnJ+Pj7gPA9bjrvLHmDcp9XI5v9n6j+dJFRDKRiugiIg9AxXQRkYeQYcC1a5giI/XiLSIiDuPu4s6rdV7lyKAj9Avph5PJUsI5ce0Ezy56lrpf1GXzqc0OjlJEJB1kg/xbRXQRkXSgYrqIyEMkNhZTt274DhwIsbGOjkZERHI4/9z+fNLyE3b33U1oyVBr+1+n/uKxzx+j6w9dOXHthAMjFBF5QNkg/1YRXUQkHamYLiIiIiIiGaFCQAV+fe5XlnddTnD+YGv7gj0LKPtRWcasGcP1uOsOjFBE5OGlIrqISAZQMV1ERERERDJCs1LN2N1vNx+3+Jh8HvkAiEmI4e0Nb1NmehnCdoRhNswOjlJE5OGiIrqISAZSMV1ERERERNKbi5ML/Wv258jgI7xS+xVcnVwBOHv9LL2W9CJkVgjrj613cJQiIg8PFdFFRDKBiukiIiIiIpLe/HL58b+m/2PfgH20LdfW2r7j3A4aftmQZ759hiOXjzguQBGRh4SK6CIimUjFdBERERERSW+l8pbih04/sLbHWqoVrGZtX3xgMeU/Ls+rK1/lasxVxwUoIpLNqYguIuIAKqaLiEh2cvr0aZ577jny5cuHh4cHlSpVYtu2bXaP/+GHH3jyySfx9/fHx8eH2rVrs2LFikyMWEQkZ2pYvCFb+2zli6e/oKBXQQDizfFM+WsKpaeX5pOtn5BgTnBwlCIi2U+2LKLfaxK/bt06TCZTiu3cuXOZGLWISEoqpouIZEHOzhiNGxNXr57lhTqHu3LlCnXr1sXV1ZXly5ezb98+pkyZQp48eeye8/vvv/Pkk0+ybNky/v77bxo1asRTTz3Fjh07MjFyEZGcydnJmeerPc/hQYcZXX80uVxyAXAx+iIDlg2gyswq/HrkVwdHKSJyi2yQf7s4OoB7lZTEN2rUiOXLl+Pv78/hw4fvmMQnOXjwID4+PtbbAQEBGRmqiEiaJRXTO3aE776Dt96CAwcs+5KK6fXrw7hx0KgRmEwODVdE5OHm6govv0x0RARerq6Ojsbh3nvvPYoWLUpYWJi1LSgo6I7nTJs2zeb2xIkT+emnn/j555+pVq1a6ieJiEi68nLzYsITE+hTow8jV49kwZ4FAOy7sI/m85vTrFQzpjSdQnn/8g6OVERyvGyQf2e7kei3JvG1atUiKCiIpk2bUrJkybueGxAQQMGCBa2bk1O2e/gi8pDTyHQREclqlixZQkhICB06dCAgIIBq1aoxe/bse7oPs9lMVFQUefPmzaAoRUTEnkd8H2H+M/P5q/dfPFbkMWv7r0d+pfKMygxYOoCL0RcdGKGISNaX7UaiL1myhNDQUDp06MD69espXLgw/fv3p0+fPnc9t2rVqsTGxlKxYkXGjRtH3bp1MyFiEZF7p5HpIiIOZBgQE2PZ9IklR48eZcaMGQwbNoxRo0axdetWBg8ejJubGz169EjTffzvf//j+vXrdOzYMdX9sbGxxMbGWm9HRkYCluK72Wx+8Ach98VsNmMYhn4Hkir1j+ynVqFabOy5kW//+ZaRa0Zy4toJEo1EPtn2CfP3zGd0/dEMqDkAdxf3B7qO+obYo74hdhkG5ps3MW7exJyYmKmXTmt/NBlG9npnkCuXZS6vYcOG0aFDB7Zu3crLL7/MzJkz7SbxBw8eZN26dYSEhBAbG8tnn33GV199xebNm6levXqK41NL4osWLcqVK1dspoORzGU2m7lw4QL+/v76FoGk8LD3j8RESzH97bdNHDhgWzGvX9/gzTcNFdPteNj7htw/9Q2xKyYGOnYkLjYWl8WLcfL0zLRLR0ZGkidPHq5du5Zl8k43NzdCQkL4888/rW2DBw9m69at/PXXX3c9f8GCBfTp04effvqJJk2apHrMuHHjGD9+fIr2Q4cO4e3tff/BywMxm81cu3YNX19fvU5KCuof2dvNhJvM2j2L6TuncyP+hrW9uE9xxjw2hubFm2O6zzcX6htij/qG2BUTg1+fPsQnJBD1+eeZmn9HRUVRpkyZu+bf2a6I/qBJfJIGDRrwyCOP8NVXX6XYpyQ+a9KLrdxJTukfiYmwZEkupk714sgR2y8TPfpoHK++ep26deNUTL9FTukbcu/UN8SubJDEZ6ZixYrx5JNP8tlnn1nbZsyYwdtvv83p06fveO4333xDr169WLhwIS1btrR7nAaxZE36sFHuRP3j4XDu+jnGrB1D2M4wDJLLQw2KNWDKk1OoFnjv61iob4g96htiVzYYxJLtpnMJDAykfHnbRS+Cg4NZtGjRPd1PrVq12LhxY6r7Ro4cybBhw6y3k5J4f39/JfEOZDabMZlMerGVVOWk/vHSS/DCC/Ddd2abkembN7vRoUNe6tc3GDPG4IknNDIdclbfkHujviF2xcSAu+Wr7AEBAZmaxCd96zIrqVu3LgcPHrRpO3ToEMWKFbvjeV9//TW9evXim2++uWMBHcDd3R1395TTBzg5Oen/p4OZTCb9HsQu9Y/sr5BPIT5v/TmDHh3EsBXDWHtsLQDrj6+n5mc16Vm1J+888Q6B3oH3dL/qG2KP+oakysnJ8jGeA/pHWq+V7Yro95vE327nzp0EBqb+R0BJfNalF1u5k5zUP5ycoGtX6Nw5tTnTTTRtaqJcOXjuOejSBYKCHBuvo+WkviH3Rn1DUpUNkvjMNHToUOrUqcPEiRPp2LEjW7ZsYdasWcyaNct6zMiRIzl9+jRz584FLFO49OjRgw8++IBHH32Uc+fOAeDh4YGvr69DHoeIiNhXtWBVVndfzZKDS3h11ascuXwEA4OwnWF89893jKg3gldqv4KHq4ejQxURcYisl6XfxdChQ9m0aRMTJ07kyJEjLFiwgFmzZjFgwADrMSNHjqR79+7W29OmTeOnn37iyJEj7N27lyFDhrBmzRqbc0REsqOkBUj37oUFC6BcueR9Bw7A6NFQogTUqwczZ8KlS46LVUREsqeaNWuyePFivv76aypWrMiECROYNm0aXbt2tR5z9uxZTpw4Yb09a9YsEhISGDBgAIGBgdbt5ZdfdsRDEBGRNDCZTLQu15p/+v/D1KZT8cvlB8CN+BuMWTuGsh+VZcGeBWSzWYFFRNJFtiui308SHxcXxyuvvEKlSpVo0KABu3bt4rfffqNx48aOeAgiIunu1mL6119Dgwa2+//4A/r1g8BAaNMGvv/eMluBiIhIWrRq1Yo9e/YQExPD/v376dOnj83+OXPmsG7dOuvtdevWYRhGim3OnDmZG7iIiNwzN2c3htYeyuFBhxlYcyDOJmcATkaepOsPXan9eW3+Opn2NelERB4G2W5hUUeIjIzE19c3Sy3wlBOZzWYiIiIsc5Nmwa86i2Opf6R0/LiloP7VV7BvX8r9Pj7Qvr1lypcGDSxTxDyM1DfEHvUNsSsmBqN9e2Lj4nD78cdMX9hIeaeeh6xCr5NyJ+ofOcf+C/t5ddWrLDu8zKa9U4VOvNfkPYr52U6vq74h9qhviF3ZIP9WjxUReUgVKwYjRlhGp+/YAa+8YhmJniQyEr74Ap54wnLs8OGwZ4/j4hURyTKcnDDq1iW+Zs2H9xNGERGRNAr2D2Zpl6WseG4FFfwrWNu//edbyn5UllGrRxEVG+XACEUk28sG+XfWjEpERNKNyQRVq8L//gcnT8Jvv0HPnuDllXzMqVMwaRJUrgxVqsDkyZY2EZEcyc0Nhg/nxqBBlp9FRESEpiWbsrPvTma0nEF+z/wAxCbG8u7Gdyk9vTSfbf+MRHOig6MUkWwpG+TfKqKLiOQgzs7QuDGEhcH58/DNN9CqFbi4JB+zeze8/jo88ohllPoXX8C1a46LWUREREREsgYXJxf6hvTlyKAjvFbnNdycLcWu8zfO0+fnPtSYVYM14WscHKWISPpTEV1EJIfy9IROneDnn+HMGfjoI6hdO3m/YcDatdC7NxQoAB07wpIlEBfnuJhFRERERMTxfHP5MunJSezrv492we2s7bvO7+LJeU/SbXk3lh1eRoI5wYFRioikHxXRRUQEf38YMAD+/BMOH4bx46F06eT9sbGwcCG0bm2ZV71fP/jjD0uhXUTkoRMTg+npp/Hr3h1iYhwdjYiISJZVMm9Jvu/4Pet7rqd6YHVr+28nfuOpb57ikfcfYcRvIzh48aADoxSRLC8b5N8qoouIiI1SpeDNN+HgQdi8GQYNshTZk1y+DDNnQr16ULIkjBkDBw44Ll4REREREXGsx4s9ztY+W5nTeg6FvQtb289eP8t7f7xHuY/LUefzOsz+ezaRsZEOjFRE5P6oiC4iIqkymaBWLfjwQzh9GpYuhS5dwMMj+ZjwcHj7bQgOhpAQmDYNzp1zWMgiIiIiIuIgTiYnelTtwb+D/mVO6Bxal22Ni1Py4kt/nfqLF395kYL/K0i3xd1YE74Gs2F2YMQiImmnIrqIiNyVqyu0aAHz51sWJJ07F5o2Badb/or8/TcMHQqFC0OzZjBvHly/7riYRUREREQk87k6uxJaPJQfOv7A6WGnmdp0KpUCKln330y4ybzd82g8tzElPyzJuHXjOHb1mOMCFhFJAxXRRUTknnh7Q7dusGIFnDoFU6dC9eTpDzGbLfu6dbMsSNq1KyxfDglaU0hEREREJEcJyB3A0NpD2dV3F9v6bGNAzQHkyZXHuv/Y1WOMXz+eoA+CaDy3MfN2zyM6PtqBEYuIpE5FdBERuW+BgZbR53//Dfv2wRtvQLFiyfujo2HBAsso9sKF4eWXYetWLUgqIiIiIpKTmEwmahSqwUctPuLMK2f4tv23NCvVDCdTcllqTfgaui3uRuCUQF78+UX+OvkXht44iEgWoSK6iIiki+Bgy/zoR4/Chg3w0kuQJ3mQCRERlvnVa9WCcuVgwgTLsSIiIiIiknPkcslFxwodWd51OSeGnGDiExMpnbe0dX9kbCSzt8+mzhd1KP9Jed7b+B5nos44MGIRERXRRUQknTk5Qb16MHMmnD0LixdD+/bg7p58zKFD8OabULIk1KkDn3wCFy86LmYRERtOThghIcRXqWK7+IOIiIikq8I+hRlZfyQHBx5k4/Mb6VW1F15uXtb9By4eYMTqERR9vygtF7Tk+33fE5sQ68CIRSRDZIP822TouzF3FRkZia+vL9euXcPHx8fR4eRYZrOZiIgIAgICcMqi/6HEcdQ/sr6rV2HRIsuCo+vWpdzv4gLNm1vmUH/6afDwSJ/rqm+IPeobcieO6h/KOy30PGQNep2UO1H/EHsetG/ciLvB9/u+J2xnGOuPr0+xP69HXrpW6srzVZ+nWmC19AhZMoleN+ROsnr+rR4rIiKZws8PeveGtWvh+HH4v/+DihWT9yckwM8/Q+fOlgVJn38eVq+GxESHhSwiIiIiIpkst1tuelTtwbqe6/h38L+MeXwMj/g+Yt1/+eZlpm+ZTvVZ1ak6syofbPqAi9H6WquIZCyNRE8DjYTJGvSJpdyJ+kf2tXu3ZXT6/PlwJpWpDgsVgi5dLCPUq1QBk+ne7l99Q+xR35A7yeojYR52aX0eEhMTiY+Pz8TIchaz2cylS5fIly+fXif/4+rqirOzs6PDyBL0d1TsyYi+YTbMrAlfQ9jOMH7Y/wMxCTE2+12dXHm67NM8X/V5QkuF4uLkki7XlfSl1w25k6yef6uIngZ6M5M16MVW7kT9I/tLTIT16y0F9e+/h6iolMdUqADPPWcpqj/ySMr9qVHfEHvUN8SumBiMrl2JjY3F7bvvcPL0zLRLK++0uNvzYBgG586d4+rVq5kfXA5iGAZmsxknJydM9/op9kPMz8+PggUL5vjnRH9HxZ6M7htXY67y7d5vCdsZxubTm1PsD/QKpFvlbjxf7XnK5S+X7teX+6fXDbErG+Tf+mhORESyBGdneOIJy/bxx/DLL5aC+rJllqleAP75B0aOtGwNGlgK6u3bW6aKERFJV7GxEBfn6CjEjqQCekBAAJ6enjm+mJlRDMMgISEBFxcXPcdYno/o6GgiIiIACAwMdHBEIjmTXy4/Xgp5iZdCXmLfhX3M2TmHubvmcv7GeQDOXj/LpD8nMenPSTxW5DGer/o8nSp0wjeXr4MjF5E7yuL5t4roIiKS5Xh4QIcOlu3iRVi40FJQ//PP5GPWr7dsAwZAq1aWgnqLFuDu7ri4RUQk4yUmJloL6Pny5XN0OA81FdFT8vhv5fOkkZSa2kXEscr7l2fSk5N454l3WPHvCsJ2hvHzwZ+JN1um+tp0ahObTm1iyK9DeCb4GZ6v+jyNghrhZNIoaBG5N3rVEBGRLC1/fujXD/74A/79FyZMgDJlkvfHxcEPP8Azz0DBgvDSS7BhA5jNjotZREQyTtIc6J6Z+DVfkVsl9T3Nxy+Sdbg6u9KqTCsWdVzE6WGnmRY6jcoFKlv330y4yfw982nyVRNKfFCCsWvHEn4l3IERi0h2oyK6iIhkGyVKwOjRcOAAbN0KL78MAQHJ+69ehVmz4PHHLceOGgX79jksXBERyUAaGS2Oor4nkrX55/bn5cdeZudLO/n7xb8ZWHMgeT3yWvcfv3act35/ixIflqDRl434atdXRMdHOzBiEckOVEQXEZFsx2SCkBCYNg1On4bly6FrV7h1UOLx4/Duu1CpkhNPPJGPESNMrFljmWZNREQku2rYsCFDhgxxdBgiIlmeyWSiemB1preYzplhZ/iu/Xc0L9XcZiqXdcfW0f3H7hT8X0H6LOnDnyf/xDAMB0YtIlmViugiIpKtubhAs2aWOdPPn7f826wZ3LrY+/79rkyebKJxY8iXzzKH+vTpcOgQKEcWEZHM8NRTT9GsWbNU923YsAGTycTu3bsf+Dpz5szBTytui4jYcHdxp0OFDizruowTQ07wbuN3KZMveY7IqLgoPtvxGXW/qEvwx8H838b/40zUGQdGLCJZjYroIiLy0PDysoxIX77cMkJ92jSoWdO2Sn7jBixdCoMHQ9mylmlf+vaFxYvh2jXHxC0iWYyTE1SsSELZsrafyIk8gN69e7Nq1SpOnTqVYl9YWBghISFUrlw5lTNFRCQ9FfYpzIh6Izgw4AB/9PqDF6q9gLebt3X/wUsHGbl6JEXfL0qL+S1Y+M9CYhP0dVaRDJUN8u+sGZWIiMgDKljQMmf6pk0Ge/ZEMHeumW7doEAB2+OOHYNPP7UsTJovH9Svb1m8dMsWSEx0SOgi4mhubhgTJ3L9jTfAzc3R0chDolWrVvj7+zNnzhyb9uvXr7Nw4UJ69+7NpUuXePbZZylcuDCenp5UqlSJr7/+Ol3jOHHiBK1bt8bLywsfHx86duzI+fPnrft37dpFo0aN8Pb2xtfXl0cffZRt27YBcPz4cZ566iny5MlD7ty5qVChAsuWLUvX+EREMovJZKJO0TrMfno2Z185y9w2c2lUvJF1v9kws/zIcjp+35FCUwsxaNkgtp/druleRDJCNsi/VUQXEZGHXv78Zrp2hblz4cwZ2LED/u//oFEjcHVNPi4xETZuhDffhEcftRTcO3eGsDDLeSIiIvfLxcWF7t27M2fOHJsCzMKFC0lMTOTZZ58lJiaGGjVqsHTpUvbu3cuLL75It27d2LJlS7rEYDabad26NZcvX2b9+vWsWrWKo0eP0qlTJ+sxXbt2pUiRImzdupVt27bx2muv4frfH8sBAwYQGxvL77//zp49e3jvvffw8vJKl9hERBwpt1tuulXpxpoeazg6+ChvPv4mxXyLWfdfvnmZj7Z+RI1ZNaj6aVWmbZrGhRsXHBixiGQ2F0cHICIikpmcnKBqVcs2fLhlepd162DFCst26FDysZcuwbffWjaAihUhNNSy1a8PuXI54AGIiEiqQmaFcO76uUy/bkGvgmx7cVuaju3VqxeTJ09m/fr1NGzYELBM5dKuXTt8fX3x9fXl1VdftR4/aNAgVqxYwXfffUetWrUeONbVq1ezZ88ewsPDKVq0KABz586lQoUKbN26lZo1a3LixAlee+01ypUrh2EYBAUF4eJiedt44sQJ2rVrR6VKlQAoUaLEA8ckIpLVBOUJYnyj8YxtOJa14WsJ2xnGov2LiEmIAWD3+d0MXTGU11e9TqsyrehVrRfNSjXDxUklNpGHmf6Hi4hIjpY7N7RsadnAMr1LUkF99WqIjEw+du9eyzZlCnh4QIMGyUX1cuXAZHLIQxCR9BYTg6lXL3xjYiyrFXt6OjoiSYNz189xOuq0o8O4o3LlylGnTh2++OILGjZsyJEjR9iwYQNvvfUWAImJiUycOJHvvvuO06dPExcXR2xsLJ7p1Af3799P0aJFrQV0gPLly+Pn58f+/fupWbMmw4YN44UXXuCrr76icePGtG3blrJlywIwePBg+vXrx8qVK2nSpAnt2rXTPO4i8tByMjnRuERjGpdozMcxH/PtP98StjOMTac2ARBvjmfxgcUsPrCYgl4F6Va5G89XfZ5g/2AHRy6SDWWD/FtFdBERkVsULw4vvWTZ4uNh8+bkovq2bZD0DfybN+HXXy0bQNGi0LSppaDepAnkyeOwhyAi6SEyElNcnKOjkHtQ0Ktgtrhu7969GTRoEB9//DFhYWGULFmSBg0aADB58mQ++OADpk2bRqVKlcidOzdDhgwhLhP74rhx4+jSpQtLly5l+fLljBs3jq+//ppnnnmGF154gdDQUJYuXcrKlSt59913mTJlCoMGDcq0+EREHME3ly8v1niRF2u8yP4L+5mzcw5zd8+1fgPq3PVzTP5zMpP/nMyjhR/l+arP07liZ3xz+To4cpFsJIvn3yqii4iI2OHqCvXqWbYJE+DiRfjtN1i50lJUv3We9JMn4fPPLZuTE9SqlTxKvWZNcNFfXBGRDJXWKVUcrWPHjrz88sssWLCAuXPn0q9fP0z/fZXpjz/+oHXr1jz33HOAZQ7zQ4cOUb58+XS5dnBwMCdPnuTkyZPW0ej79u3j6tWrNtcoU6YMZcqUYciQIXTu3Jk5c+bwzDPPAFC0aFH69u1L3759GTlyJLNnz1YRXURylGD/YN578j3eafwOK46sIGxnGEsOLiHeHA/A5tOb2Xx6M0NWDOGZ4Gd4vurzNCreCGcnZwdHLiIPQm/pRURE0ih/fstCo507W0ak//NP8ij133+H2FjLcWYzbNpk2caPBz8/y+j0pKL6Ld+iFxGRHMbLy4tOnToxcuRIIiMj6dmzp3Vf6dKl+f777/nzzz/JkycPU6dO5fz58/dcRE9MTGTnzp02be7u7jRp0oRKlSrRtWtXpk2bRkJCAv3796dBgwaEhIRw8+ZNXnvtNdq3b09QUBAnT57k77//thbQhwwZQvPmzSlTpgxXrlxh7dq1BAdr2gIRyZlcnFxoWaYlLcu05GL0RRbsWUDYzjB2ntsJQExCDAv2LGDBngXk9chLo+KNaBxkmR6mdN7S1g9QRSR7UBFdRETkPphMloVGK1aEV16B6GhLIT2pqL5/f/KxV6/C999bNoDg4OSC+uOPZ8np3kREJAP17t2bzz//nBYtWlCoUCFr++jRozl69CihoaF4enry4osv0qZNG65du3ZP93/9+nWqVatm01ayZEmOHDnCTz/9xKBBg3j88cdxcnKiWbNmTJ8+HQBnZ2cuXbpE9+7dOX/+PPnz56dNmzaMHz8esBTnBwwYwKlTp/Dx8aFZs2a8//77D/hsiIhkf/k98zP40cEMfnQwO87uIGxnGPP3zOfyzcsAXL55mUX7F7Fo/yIAivgUoUmJJpaielBjAr0DHRm+iKSByTCSZncVeyIjI/H19eXatWv4+Pg4Opwcy2w2ExERQUBAAE5OTo4OR7IY9Q+xx1F94+RJSzF95UrLFDBXrqR+nLs71K+fXFSvWFELlGYWvW6IXTExGO3bExsXh9uPP+KUiZ90Ke+0uNPzEBMTQ3h4OEFBQeTKlctBEeYMhmGQkJCAi4uLRkzeQn3QQn9HxR71DYvYhFh+PvQz3+z9hjXha7gSY+cNARCcP9g6Sr1h8Yb45fLLvEAzkfqG2JUN8m+NRBcREckARYvCCy9YtsRE2Lo1eZT65s2WKV/AMgXMb79Zttdeg0KFkhcoffJJyJfPsY9DRERERETunbuLO+3Lt6d9+fYkmhPZcW4Hq4+uZnX4ajae2MjNhJvWY/df3M/+i/v5aOtHOJmcCCkUYh2lXveRuuRyybkf2IlkFSqii4iIZDBnZ3jsMcs2dqxlVPrq1clF9ZMnk489cwbmzLFsJhOEhCSPUn/sMS1QKpIpnJygdGkSo6MtP4uIiIg8AGcnZ0IKhRBSKITh9YYTmxDLX6f+YvXR1fwW/htbT28l0UgEwGyY2XJ6C1tOb+Hdje/i7uxOvUfqWUeq1wisoUVK5eGTDfJvvRUXERHJZHnyQPv2ls0w4MCB5IL6+vVw879BKYZhGcG+dSu8/Tb4+EDjxslF9eLFHfowRB5ebm4YU6YQFRGBh5ubo6MRERGRh4y7izsNizekYfGGTGACkbGRrD+2ntXhlpHqeyP2Wo+NTYy1trMGfN19aVi8IY2DGtOkRBPK5S+nKbck+8sG+beK6OkoMTGR+Ph4R4fx0DKbzcTHxxMTE6O5s/7j6uqKs7M+gRbJzkwmy0KjwcEwZAjExMDGjclF9T17ko+NjITFiy0bQJkyyQX1Bg3Ay8shD0FERERERB6Aj7sPT5V9iqfKPgXAuevnWBO+xjr9y/Frx63HXou9xk8Hf+Kngz8BEOgVSOMSja3TvxT1LeqQxyDysFMRPR0YhsG5c+e4evWqo0N5qBmGgdlsJioqSp+y3sLPz4+CBQvqORF5SOTKBU2aWLbJky3Tu6xcaSmor1oFly4lH3vokGWbPh1cXaFeveSiepUqWqBURERERCQ7KuhVkC6VutClUhcMw+DolaP8dvQ3VoevZk34Gi7dTH5TcPb6Webtnse83fMAKJOvjLWg3iioEXk98jrqYYg8VFRETwdJBfSAgAA8PT1VzMwghmGQkJCAi4uLnmMsz0d0dDQREREABAYGOjgiEckIhQpBz56WLTERtm9PHqX+11+WNoD4eFi71rKNGAEFCtguUBoQ4MhHIZLNxMZi6tcPn5gY+Pxz8PBwdEQiIiKSQ5lMJkrmLUnJvCV5KeQlzIaZ3ed3W0eprz++nuj4aOvxhy4d4tClQ8zYNgMTJqoFVqNJUBMal2hMvUfq4enq6cBHI2JHNsi/VUR/QImJidYCer58+RwdzkNNRfSUPP57UYmIiCAgIEBTu4g85JydoWZNyzZ6NFy7ZimaJxXVw8OTjz1/Hr76yrIBVK+ePEr90UctI95FxA7DgIgInOLiLD+LiIiIZBFOJieqFqxK1YJVeaXOK8QlxrH51GbrvOmbTm0iwZwAgIHB9rPb2X52O5P+nISbsxu1i9S2LlJas1BNXJ1dHfyIRMgW+beK6A8oaQ50T099kieOkdT34uPjVUQXyWF8faFNG8tmGHDkSHJBfe1auHEj+djt2y3bu+9aivHly0O1apbierVqULWqZeFSERERERHJPtyc3ahfrD71i9VnXMNxXI+7zu/Hf7eOVN91fpf12LjEONYfX8/64+t5c92beLt506B4A+v0LxUDKmrQoogdKqKnE73IiKOo74kIWOY/L13asg0cCLGx8OefyUX1nTuTj01MtCxYumcPzJ2b3F6qlG1hvVo1TQMjIhanT59m+PDhLF++nOjoaEqVKkVYWBghISF2z1m3bh3Dhg3jn3/+oWjRoowePZqePXtmXtAiIiI5kJebFy1Kt6BF6RYAXLhxgbXH1rL66Gp+C/+No1eOWo+Niovil0O/8MuhXwAIyB1gLag3LtGY4n7FHfEQRLIkFdElXRUvXpwhQ4YwZMgQR4ciIpKjubtDo0aW7f/+zzK9y6pVsHq1ZUT6P/8kz6ee5MgRy7ZwYXJb4cIpC+uPPKJFS0VykitXrlC3bl0aNWrE8uXL8ff35/Dhw+TJk8fuOeHh4bRs2ZK+ffsyf/58Vq9ezQsvvEBgYCChoaGZGL1kdff7/uHSpUsEBwezZcsWihcvnm7xjBgxghs3bjB9+vR0u08REUfyz+1Pxwod6VihIwDHrh6zjlJfHb6aiBsR1mMjbkTw9d6v+Xrv1wCUyFOCxkGNaVKiCY2KN8I/t79DHoNIVuDk6ADEMUwm0x23cePG3df9bt26lRdffPGBYmvYsKGK8CIi6axAAXjuOQgLg1274Pp12LoVZs2Cvn3tz5N++jT88gu89Ra0bQvFi0P+/NCkCbz+Onz9NRw4kLIgLyIPj/fee4+iRYsSFhZGrVq1CAoKomnTppQsWdLuOTNnziQoKIgpU6YQHBzMwIEDad++Pe+//34mRp719OzZ0ybnzpcvH82aNWP37t3pdo1x48ZRtWrVNB2X2vuAcuXKpVssGemdd96hdevWNgX0wYMHU6NGDdzd3e0+B7t376Z+/frkypWLokWLMmnSJJv9r776Kl9++SVHjx5N9XwRkeyuuF9xelfvzYJ2Czj3yjl2993N+6Hv06pMK7zdvG2OPXrlKLO3z6bT950I+F8AVWdW5ZUVr7Ds8DKux1130CMQcQyNRM+hzp49a/3522+/5c033+TgwYPWNi8vL+vPhmGQmJiIi8vdu4u/vz6VFBHJDnLlgpAQy5YkIQEOHoQdOyyj1XfssGzXrtmee/myZUT76tXJbblzQ5UqtiPWK1QAN7fMeTwiknGWLFlCaGgoHTp0YP369RQuXJj+/fvTp08fu+f89ddfNGnSxKYtNDRUAyWAZs2aERYWBsC5c+cYPXo0rVq14sSJE5keS4UKFfjtt99s2tKS8ztadHQ0n3/+OStWrEixr1evXmzevDnVDyYiIyNp2rQpTZo0YebMmezZs4devXrh5+dnHQiUP39+QkNDmTFjBpMnT87wxyIi4kgmk4lKBSpRqUAlhjw2hPjEeLad2cZvR39jdfhq/jr1F3GJcdbjd53fxa7zu5i6aSouTi48VuQx6/QvjxZ5FDdnJf/y8NJI9ByqYMGC1s3X1xeTyWS9feDAAby9vVm+fLl1JMfGjRv5999/ad26NQUKFMDLy4uaNWumSLqLFy/OtGnTrLdNJhOfffYZbdu2xdPTk9KlS7NkyZIHin3RokVUqFABd3d3ihcvzpQpU2z2f/LJJ5QuXZpcuXJRoEAB2rdvb933/fffU6lSJTw8PMiXLx9NmjThxq0r74mI5GAuLpbC93PPwdSplsVJr1yBf/+F77+HUaOgeXPLqPbb3bhhmYP9o4+gd29LMd3Ly/LvCy/Axx9b9uslV7IFkwmKFsVcqJDmLgKOHj3KjBkzKF26NCtWrKBfv34MHjyYL7/80u45586do8BtLxYFChQgMjKSmzdvpjg+NjaWyMhImw3AbDanuhmGkS03AHd3dwoUKECBAgWoUqUKw4cP5+TJk0RERFiPO3HiBB07dsTPz4+8efPSunVrwsPDrfvXrl1LrVq1yJ07N35+ftStW5djx44RFhbG+PHj2bVrl3VkeVhYmN14XFxcrLEkbfny5bPuDwoK4p133qFLly7kzp2bwoUL89FHH9ncx/Hjx2ndujVeXl74+PjQsWNHzp07Z3PMkiVLqFmzJrly5SJ//vy0bdvW5jm5ceMGzz//PN7e3jzyyCN8+umnd3wely5diru7O48++qhN+wcffED//v0JCgoCSHHevHnziIuL4/PPP6d8+fJ06tSJQYMGMXXqVJvjWrVqxTfffHPX36e9/pmTNj0P2uxt6hvZc3M2OfNo4Ud5o/4brOm+hkuvXWJ5l+W8Vvs1qhesjonkvCjBnMDGExsZv348j895nLzv5aXZvGb874//sf3MdhISE9Q3tKV9MwyMIkVILFQIswP6SFpk/WEG4jAjRozgf//7HyVKlCBPnjycPHmSFi1a8M477+Du7s7cuXN56qmnOHjwII888ojd+xk/fjyTJk1i8uTJTJ8+na5du3L8+HHy5s17zzFt376dTp06MW7cODp16sSff/5J//79yZcvHz179mTbtm0MHjyYr776ijp16nD58mU2bNgAWEbfP/vss0yaNIm2bdsSFRXFhg0brMm7iIikZDJBiRKWrV275PazZ1OOWA8Ptz03Pj553633V7as7Yj1atXgPv4kiGQcd3eMjz8mMiKCXO7ujo7G4cxmMyEhIUycOBGAatWqsXfvXmbOnEmPHj3S5Rrvvvsu48ePT9F+4cIFYmJibNri4+Mxm80kJCSQkJBge8Jtx9pwcrL9ekx6HJvaPFh3kPRGLSnu69ev89VXX1GqVCl8fX1JSEggPj6e0NBQHnvsMdasWYOLiwvvvvsuzZo1Y/v27Tg5OdG2bVt69+7N3LlziYuLY+vWrSQmJtKuXTv27NnDihUr+PXXXwGs95taLIZhpLrvVlOnTmX48OGMHj2aVatWMWTIEEqWLEmTJk0wm83WAvrq1atJSEhg8ODBdOrUyTrYZtmyZbRr144RI0bw+eefExcXx6+//mpz3alTpzJu3Dhef/11fvjhB/r370/dunUpW7ZsqjH9/vvvVK9e3W7s9h7bn3/+Sb169XBycrLua9KkCZMmTeLChQvWef6rV6/OqVOnOHLkSKrzrSckWApDly5dwtXV9Y7P38PMbDZz7do1DMPAyUnj8ySZ+sbDpap3VapWrsqwysO4EnOFP878wcbTG9l4eiP/XvvXetyN+Bus+HcFK/61fEsob6681C1Ul3qF61G/cH2K+xTHMAz1DbHL/OabXLt2Dd9r13CKisq060al8VoqomeQkBA4dy7zr1uwIGzblj739dZbb/Hkk09ab+fNm5cqVapYb0+YMIHFixezZMkSBg4caPd+evbsybPPPgvAxIkT+fDDD9myZQvNmjW755imTZtG48aNGTNmDABlypRh3759TJ48mZ49e3LixAly585Nq1at8Pb2plixYlSrVg2wFNETEhJ45plnKFasGACVKlW65xhERAQCAy1bixbJbVeuwM6dyYX17dst08Pc+sG+YVjmUD9wABYsSG4vVsy2sF69uuX+NQhYxPECAwMpX768TVtwcDCLFi2ye07BggU5f/68Tdv58+fx8fHBw8MjxfEjR45k2LBh1tuRkZEULVoUf39/fHx8bI6NiYkhKioKFxeXlFOPdOli/4HUqAFjxybffv55iI1N/dgKFeDdd5Nv9+0L/42Ot3GP37B0cnJi2bJl1mLtjRs3CAwM5Oeff8btv6J90gjozz//HNN/L4Jz5swhT548bNy4kZCQEK5du8ZTTz1lLTLfmtN6e3vj6upKkSJF7hrL3r17UywQ27VrV2bOnGm9Xbt2bUaNGgVA+fLl2bRpE9OnT6dZs2asWrWKvXv3cvToUYoWLQrA3LlzqVixIjt27KBmzZq89957dO7cmQkTJljvs0aNGjbXbNGihfX9xMiRI/nwww/ZsGEDFSpUSDX2kydPUqhQIbtTzzg5OWEymVLsj4iIoHjx4jbthQoVAuDixYvWqSmTBgidPn2aUqVKpbh/FxcXnJycyJcvH7nu8YOUh4nZbMZkMuHv769imNhQ33h4BRBA2UfK0oteAJy8dpLVx1azJnwNa8LXcPZ68tTBl2Mu8/PRn/n56M8AFPMtRqPijQjJG8IT+Z6gdL7SOJnUPySZo1470vq3XEX0DHLunGUxtuws5NaJcrGMlBk3bhxLly61FqRv3rx51/kbK1eubP05d+7c+Pj4EBERcYcz7Dtw4ACtW7e2aatbty7Tpk0jMTGRJ598kmLFilGiRAmaNWtGs2bNrFPJVKlShcaNG1OpUiVCQ0Np2rQp7du3T/HGQURE7k+ePNCokWVLEh0Nu3fbjljfswfi4mzPPX7csi1enNwWEJCysF6ihArrIpmtbt26NmvnABw6dMg6KCE1tWvXZtmyZTZtq1atonbt2qke7+7ujnsqo/6dnJxSvIlKKpAmbWlmMqX9BSStx97HC1KjRo2YMWMGAFeuXOGTTz6hRYsWbNmyhWLFirF7926OHDmS6ocHR48eJTQ0lJ49e9KsWTOefPJJmjRpQseOHQkMDPwvJJPNv/ZDN1G2bNkUUy36+PjYnPvYY4/Z3F/t2rWZNm0aJpOJAwcOULRoUZtvpVaoUAE/Pz8OHDhArVq12LlzJ3369LljPJUrV7aJu2DBgly4cMHuOTdv3sTDw8Pu/js9B7f3m1uPTfrZ09PTep073Udq/TOn0fMg9qhv5AzF8hSjV55e9KrWC8MwOHDxgHU+9XXH1nEtNnlxpePXjjNn1xzmMAfWgo+7D9UDq1MjsAYhhUKoEViDknlLqrCewznitSOt11IRPYMULJj9r5s7d26b26+++iqrVq3if//7H6VKlcLDw4P27dsTd3sl5Da3f8XRZDKleb6he+Xt7c327dtZt24dK1eu5M0332TcuHFs3boVPz8/Vq1axZ9//snKlSuZPn06b7zxBps3b7bOmygiIunL0xMee8yyJYmLg/37bQvrO3fC9eu250ZEwK+/WrYkPj6208BUrw7lylnmcxdJN7GxmIYMwefmTZgxA1IZOZ2TDB06lDp16jBx4kQ6duzIli1bmDVrFrNmzbIeM3LkSE6fPs3cuXMB6Nu3Lx999BGvv/46vXr1Ys2aNXz33XcsXbo0Y4NduND+vtvfIM2bl/ZjP//8/mO6Te7cuW1GN3/22Wf4+voye/Zs3n77ba5fv06NGjWYP39+inOTRkqHhYUxePBgfv31V7799lvrVCuP3fpimwZubm6pjrROT6l98+B29/p+IX/+/Fy5cuWeY7H3DYmkfUkuX74MJD/fIiJydyaTiWD/YIL9gxn06CASzAlsP7ud1UdXszp8NRtPbCQ2MfkbYJGxkaw7to51x9ZZ224trNcIrEGNQjUolbeUCus5QTbIv/WWM4Ok15QqWckff/xBz549adu2LWAZmX7s2LFMjaFcuXL8+eefKeIqU6YMzs7OgOXrlU2aNKFJkyaMHTsWPz8/1qxZwzPPPIPJZKJu3brUrVuXN998k2LFirF48WKbrw+LiEjGcnODKlUs2/PPW9rMZjhyxLawvn07XLpke25kJKxfb9mS5MoFlSvbFtYrVbrnqYpFkhkGnDyJU1yc5eccrmbNmixevJiRI0fy1ltvERQUxLRp0+jatav1mLNnz9p8OzEoKIilS5cydOhQPvjgA4oUKcJnn31GaGhoxgZ7L//xM+rYe5Q04ippwdXq1avz7bffEhAQkGI0+q2qVatGtWrVGDlyJLVr12bBggU89thjuLm5kZiYmG7xbdmyxeb2pk2bCA4OBizT+pw8eZKTJ09ap3PZt28fV69etU4BVLlyZVavXs3zSS/46aBatWrMu9OHIHbUrl2bN954g/j4eGvhftWqVZQtW9bm26l79+7F1dXV7nQyIiJydy5OLtQqXItahWsxsv5IbsbfZOOJjaw6sIoDkQfYfnY7p6Nsp3BQYT0Hywb5t4rokmalS5fmhx9+4KmnnsJkMjFmzJgMG1F+4cIFdu7cadNWsGBBhg4dSu3atZkwYQKdOnXir7/+4qOPPuKTTz4B4JdffuHo0aM8/vjj5MmTh2XLlmE2mylbtiybN29m9erVNG3alICAADZv3syFCxesbwJERMRxnJygTBnL1rmzpc0w4NSplIX1U6dsz42JgS1bLFsSZ2cIDk6eDqZ6daha1TKSXUTuXatWrWjVqpXd/XPmzEnR1rBhQ3bcurKwABAbG8u5/xZPunLlCh999BHXr1/nqaeeAixzkk+ePJnWrVvz1ltvUaRIEY4fP84PP/zA66+/Tnx8PLNmzeLpp5+mUKFCHDx4kMOHD9O9e3cAihcvTnh4ODt37qRIkSJ4e3unOlUOWBbIPHfbQk4mk4kCBQpYb//5559MmjSJtm3bsmrVKhYuXGj9RkGTJk2oVKkSXbt2Zdq0aSQkJNC/f38aNGhgnRpy7NixNG7cmJIlS9K5c2cSEhJYtmwZw4cPv+/nMDQ0lJEjR3LlyhWb4veRI0e4fv06586d4+bNm9b3E+XLl8fNzY0uXbowfvx4evfuzfDhw9m7dy8ffPAB77//vs39b9iwgfr166dpFL2IiKSNh6sHjYMaUyl3JQICAnBycuL89fP8ffZv/j7zN9vObuPvM3+nubBerWA16zQwKqxLZlARXdJs6tSp9OrVizp16pA/f36GDx9OZGoLLKWDBQsWsODWFeewLHQ6YsQIvv32W8aOHcuECRMIDAzkrbfeomfPngD4+fnxww8/MG7cOGJiYihdujRff/01FSpUYP/+/fz+++9MmzaNyMhIihUrxpQpU2jevHmGPAYREXkwJhMULWrZbl0O48KF5KJ6UmH98GHbcxMTYe9ey/bf7BIAlCyZXFivWhWKFHFC39YXkcz066+/Wucv9/b2ply5cixcuJCGDRsClvm4f//9d4YPH84zzzxDVFQUhQsXpnHjxvj4+HDz5k0OHDjAl19+yaVLlwgMDGTAgAG89NJLALRr144ffviBRo0acfXqVcLCwqy58u3++ecfayxJ3N3diYmJsd4eMmQIf//9N2+99RY+Pj5MnTrV+o0Ck8nETz/9xKBBg3j88cdxcnKiWbNmTJ8+3Xp+w4YNWbhwIRMmTOD//u//8PHx4fHHH3+g57BSpUpUr16d7777zvq4AV544QXW3/JVpWrVqgEQHh5O8eLF8fX1ZeXKlQwYMIAaNWqQP39+3nzzTV588UWb+//mm28YN27cA8UoIiJ3V8CrAC1Kt6BF6RbWtlsL63+ftWynIm1H0UTGRrL++HrWH09+zU8qrCcV1UMKhaiwLunKZBhZdIx8FhIZGYmvry/Xrl1LdYGf8PBwgoKCcvTK7JnBMAwSEhJwcXG5t0WkHnLqgxZms5mIiAjrJ9oiSdQ3cobISNi1y7awvm8fJCTc/VxfX4MyZUyUKQOlSyePiC9dWiPXc6yYGIz27YmNi8Ptxx9x+m+Rwcxwp7wzJ1H+nTUUL16cQYMGMWzYsCyXfy9dupTXXnuNvXv3puvf9+XLl/PKK6+we/duXOwsuKE+aKEcS+xR3xB77rdvpKWwnhpvN+/kqWAKWaaDKZ2vtArrWVE2yL81El1ERESyPR8fqF/fsiWJibGMRL+1sL57N/w37bDVtWsmtm6FrVtT3m+BAqRaXC9ZMkuudSMikmO0bNmSw4cPc/r0aet87Onhxo0bhIWF2S2gi4hI5rM3Yn372e1sO7PNbmE9Ki4qxYh1FdblfikzEBERkYdSrlwQEmLZkiQkwKFDloL69u0Gf/8dx4kTbhw/bkp1/Zrz5y3bhg227SYTPPJIyuJ6mTJQvDio9iIikvGGDBmS7vfZvn37dL9PERFJfwW8CtC8dHOal06eojfiRoR1tHpScT2thfVqgZapYJLmWVdhXW6nt3giIiKSY7i4QPnylq1LF4OIiCsEBAQQF2fi6FFLgf3QIcsc60k/37bmHmBZ9PT4ccv2228pr1GiRMriepkyUKiQZRFVyeJMJggIwBwTY/lZJIcKDw8nIS3zYomIiGQBAbkD7lhYTyqup1ZY//347/x+/Hdr262F9aTiugrrGSgb5N8qoouIiEiOlytXcnH9dpGRcORIyuL6oUNw9WrK45NGux86lHKfh0dyUf32Uez582fZfDHncXfH+OwzIiMiyOXu7uhoREREROQ+paWw/veZvzkZedLmvLQU1msUqkGZfGVUWE8P2SD/VhFdRERE5A58fKB6dct2K8OAS5dSFteTfr597nWwtO3ebdlu5+eXenFdC5yKiIiIiKQfe4V1mznW01hY93LzSp5jXYX1h5qK6CIiIiL3wWSyjB7Pnx/q1LHdZzbDmTOpF9f//dcyWv12V6/Cli2W7XYFC6Y+/3rJkpZR9CIiIiIicv8CcgfQrFQzmpVqZm1LKqz/feZvtp3dlmph/Xrc9VQL69UK3jLHugrrDwUV0UVERETSmZMTFCli2Ro1st2XkGCZSz21+ddPnCDVBU7PnbNs9hY4TSqu31pgL1ZMC5zet7g4TMOH4x0dDR98oE8qRERERHKg1ArrF25csI5UT5pjPbXC+oYTG9hwIjl5v7WwXqOQpbiuwvotskH+rbdWIiIiIpnIxcUygrxkSWje3HZfTIxlpHpqBfbz51Pe160LnK5aZbvP1fXOC5xq/vU7MJvh8GGc4+IsP4uIiIiIAP65/e9aWP/77N+cuHbC5jx7hfUK/hUombckJfP8t/33c0GvgphyUsKeDfJvFdFFREREsohcuaBCBct2u8jI5KL67QucXruW8vj4eDh40LLdztMz9QVOg4IgIMAykl5ERERERO7uQQrrm09vZvPpzSnu09PVkxJ5SqQorpfMW5JivsVwdXbN8McltlREF7lF8eLFGTJkCEOGDLmn8y5dukRwcDBbtmyhePHi6RbPiBEjuHHjBtOnT0+3+xQRkezJxwdq1LBstzIMuHgx9QVODx9OfYHT6GjYtcuy3c7NDQoXtkxFU7Ro8r+3/uzvr5Hs8pBIWiH4+nXw8oJ8+dS570HPnj25evUqP/74o6NDERERyVLsFdZvXbx0+9ntHL92PNXzo+Oj2Ruxl70Re1PsczY584jvI6mOYC+ZtyRebl4Z9rhyMhXRc7CePXvy5ZdfWm/nzZuXmjVrMmnSJCpXrpwu1xg3bhw//vgjO3fuvOtx48ePT9FetmxZDhw4kC6xZKR33nmH1q1b2xTQBw8ezB9//MHevXsJDg5O9TnYvXs3AwYMYOvWrfj7+zNo0CBef/116/5XX32VEiVKMHToUEqUKJEJj0RERLIbk8lS1Pb3h7p1bfeZzXD6dOoLnB49mvoCp3FxEB5u2exxc0tZZL/93/z5VYuULOzqVfjyS5g+3TKHUpKSJWHQIOjRA/z80v2yWSX/HjRoEL/99hv79+9Pse/EiRMEBQWxePFinnrqqQeKZd26dTRq1IgrV67glwHPp4iISHbin9uf0FKhhJYKtbbdjL9J+NVw/r38L/9e+Tf53yv/En4lnHhzfIr7STQSCb8aTvjVcH7jtxT7A3IH2BbW85SkVN5SlMxbEn9P/6w5TYxhWOZFv3nTMkKoaNEs92ZCRfQcrlmzZoSFhQFw7tw5Ro8eTatWrThx4sRdzkx/FSpU4LffbP/zu2SDFdGio6P5/PPPWbFiRYp9vXr1YvPmzezevTvFvsjISJo2bUqTJk2YOXMme/bsoVevXvj5+fHiiy8CkD9/fkJDQ5kxYwaTJ0/O8MciIiIPFyen5FHkTzxhuy8+PuUCp8ePw8mTcOoUXL5s/37j4ixF+KNH7R/j7m6/wJ70swb9ikOsWAHt2lm+knG7o0dh6FB44w1YtAhCQ1Me84CyQv7du3dvPvroI/7880/q1Kljs2/OnDkEBATQokWLTItHREQkp/Jw9aC8f3nK+5dPsS/RnMipyFMpiutJP0fGRqZ6nxE3Ioi4EcFfp/5Ksc/LzStFgT3p56K+RXFxyuQ6XNLAhg8/xHT0KO4AxYpl+MCG+5H1K5SSodzd3SlYsCAABQsWZMSIEdSvX58LFy7g7+8PwMmTJ3nllVdYuXIlTk5O1K9fnw8++MA66nrdunW8/vrr/PPPP7i6ulKhQgUWLFjA2rVrraPLkz7lCgsLo2fPnqnG4uLiYo0lNUFBQfTs2ZODBw+yZMkS/Pz8GDVqFAMGDLAec+LECQYNGsTq1atxcnKiWbNmTJ8+nQIFCliP+fnnn3nrrbfYs2cPXl5e1K9fn8WLF1v3R0dH06tXLxYuXEiePHkYPXq0taidmmXLluHu7s5jjz1m0/7hhx8CcOHChVSL6PPnzycuLo4vvvgCNzc3KlSowM6dO5k6darN9Z566ineeOMNFdFFRCRdubpCqVKWLbVa2Y0blmL6qVPJhfVb/z150pLz2hMbaxnge+sg39vlypX6dDG3tuXJo0K7pKMVK6BlS8toJ8NIuT+p7eZNy3FLl6Z7IT0r5N9Vq1alevXqfPHFFzZFdMMwmDNnDj169MBkMtG7d2/Wrl3LuXPneOSRR+jfvz8vv/xyuj0XV65c4eWXX+bnn38mNjaWBg0a8OGHH1K6dGkAjh8/zsCBA9m4cSNxcXEUL16cyZMn06JFC65cucLAgQNZuXIl169fp0iRIowaNYrnn38+3eITERFxJGcnZ4r5FaOYXzGeCLIdEWMYBpduXkp1BPu/l//l7PWzqd7n9bjr7Dq/i13nU87r6OLkQnG/4qlOEVMiTwk8XT3T9wE6eGDDvVIRPSPFxNjf5+Rk+S50eh6bK9e9xXeb69evM2/ePEqVKkW+fPkAiI+PJzQ0lNq1a7NhwwZcXFx4++23adasGbt378bJyYk2bdrQp08fvv76a+Li4tiyZQsmk4lOnTqxd+9efv31V+sIc19f3weKcerUqYwcOZLx48ezYsUKXn75ZcqUKcOTTz6J2WymdevWeHl5sX79ehISEhgwYACdOnVi3bp1ACxdupS2bdvyxhtvMHfuXOLi4li2bJnNNaZMmcKECRMYNWoU33//Pf369aNBgwaULVs21Zg2bNhAjdsnqE2Dv/76i8cffxy3W363oaGhvPfee1y5coU8efIAUKtWLU6dOsWxY8fSdb51ERGRO8mdG8qWtWz2XL9uW2hPrdie2qKnSWJi4MgRy2aPp6elqH6nOdr9/DKg0O7jg3Gn/Eyyn6tXLW/UDMMy19GdmM2WHLxdO0tnzqARUI7Mv3v37s2IESP44IMPyJ07N2ApzoeHh9OrVy/MZjNFihTh66+/JiAggL/++osXX3yRwMBAOnbsmC6Pv2fPnhw+fJglS5bg4+PD8OHDadGiBfv27cPV1ZUBAwYQFxfH77//Tu7cudm3bx9eXpZ5XseMGcO+fftYvnw5+fPn58iRI9xMbREIERGRh5DJZCK/Z37ye+bn0SKPptgfHR/N0StHUy2yH7t6jARzynkdE8wJHLl8hCOXU0/OA70C7c7Dns8j371NE5MFBjbcKxXRM1KHDvb3hYTA2LHJt597zjJkKzUVK8K77ybf7t0bIlP5ysbPP99ziL/88os1Eb1x4waBgYH88ssvODk5AfDtt99iNpv57LPPbEaz+Pn5sW7dOkJCQrh27RqtWrWiZMmSAAQHB1vv38vL664jzJMkjQy/1XPPPcfMmTOtt+vUqcOIESMwmUyUKVOGP/74g/fff58nn3yS1atXs2fPHsLDwylatCgAc+fOpUKFCmzdupWaNWvyzjvv0LlzZ5v516tUqWJzzRYtWtC/f38Ahg8fzvvvv8/atWvtFtGPHz9OoUKF7vr4bnfu3DmCgoJs2pJGzJ87d85aRE+67+PHj6uILiIiWYqXF5QrZ9nsiYqyP5o96d/U0pok0dHJU87Ykzv33edo9/W9h0J7rlwY8+ZxLSKCgAccpCBZyJdfWjpUam/UUmM2W46fOxcGD063MLJK/t2lSxdeeeUVFi5caB2pHhYWRr169ShTpgwA48ePJyEhARcXF0qUKMFff/3Fd999ly5F9KTi+R9//GEdDT9//nyKFi3Kjz/+SIcOHThx4gTt2rWjUqVKADZrBJ04cYJq1aoREhICoDxZRETkFp6unlQMqEjFgIop9iWYEzh57aTdaWKux11P9T7PXj/L2etn2XhiY4p9Pu4+dqeJKeJTBGcn5+SDs+DAhrRQET2Ha9SoETNmzAAsX6f85JNPaN68OVu2bKFYsWLs2rWLI0eO4O3tbXNeTEwM//77L02bNqVnz56Ehoby5JNP0qRJEzp27EhgYOA9x1K2bFmWLFli0+bj42Nz+9FHbT9dq127NtOmTQNg//79FC1a1FpAByhfvjx+fn7s37+fmjVrsnPnTvr06XPHOG5d1MlkMlGwYEEiIiLsHn/z5k1yZeAbbA8PD8AyzYyIiEh24+0NwcGWzZ7IyNSni7n15+up5/KAZeqZgwctmz1eXncezV6kiKXQLg8pw7AsIno/PvzQMidnOn3dIavk335+fjzzzDN88cUX9OzZk8jISBYtWsTHH39sPebjjz/miy++4OTJk9y8eZO4uDiqVq36wM8BWHJ3FxcXm/w+X758lC1b1rrg6eDBg+nXrx8rV66kSZMmtGvXzpqr9+vXj3bt2rF9+3aaNm1KmzZtUszvLiIiIim5OLkQlCeIoDxBNCnRxGafYRhciL5gd5qY8zfOp3qfkbGR7Di3gx3ndqTY5+bsZjNNTLtVp2gQHY3JwQMb7pWK6Blp4UL7+/4baWI1b17aj/388/uP6Ta5c+emVKlS1tufffYZvr6+zJ49m7fffpvr169To0YN5s+fn+LcpDkbw8LCGDx4ML/++ivffvsto0ePZtWqVSnmCL8bNzc3m1gyQlJB+k5cXV1tbptMJsx3+GQsf/78XLly5Z5jKViwIOfP2774JN2+deTQ5f9Wdkt6vkVERB42Pj5Qvrxls+fatTtPG3PypKWYbs/163DggGWzx9v71qK6iTx5vBg/3jLSXbK5S5fuPEG/PYZhOe/yZctKuOkgK+XfvXv3pnHjxhw5coS1a9fi7OxMh/++TfvNN9/w2muvMWnSJOrWrYuPjw+TJ09m8+bND/Do780LL7xAaGgoS5cuZeXKlbz77rtMmTKFQYMG0bx5c44fP86yZctYtWoVjRs3ZsCAAfzvf//LtPhEREQeNiaTiYDcAQTkDqB20dop9l+Pu253mpjjV4+TaCSmOCcuMY5Dlw5x6NIhMODlLy0p1j0PT0jngQ33KlsW0U+fPs3w4cNZvnw50dHRlCpVirCwMOtX+VKzbt06hg0bxj///EPRokUZPXq03QUu0829jE7OqGPvkclkwsnJyTqfYPXq1fn2228JCAhIMSr8VtWqVaNatWqMHDmS2rVrs2DBAh577DHc3NxITEz5H+h+bdmyxeb2pk2brF9fDQ4O5uTJk5w8edI6Gn3fvn1cvXqV8v+9K69cuTKrV69O1wWHqlWrxrw7fQhiR+3atXnjjTeIj4+3Fu5XrVpF2bJlrVO5AOzdu9e6YJSIiEhO5etr2ez9OTQMS6H9TtPGnDyZ+rpFSaKiYP9+OLI/jvGMxclk4DZ+HKApXbK9O32VIS2iotKtiH47R+bfjRo1IigoiLCwMNauXUvnzp2t86MnTbPSt29fXFxcMJlM/Hs/H0TYERwcTEJCAps3b7aOIL906RIHDx605u4ARYsWpW/fvvTt25eRI0cye/ZsBg0aBFg+VOjRowc9evSgfv36vPbaayqii4iIZCAvNy8qF6hM5QKVU+yLT4znxLUTdqeJiY6PJl80lLr3cagZMrDhXmW7IvqVK1eoW7cujRo1Yvny5fj7+3P48GGbouPtwsPDadmyJX379mX+/PmsXr2aF154gcDAQEKzwOqujhQbG8u5c+cAy3P70Ucfcf36dZ566ikAunbtyuTJk2ndujVvvfUWRYoU4fjx4/zwww+8/vrrxMfHM2vWLJ5++mkKFSrEwYMHOXz4MN27dwcscxOGh4ezc+dOihQpgre3N+7u7qnGkpCQYI0liclkss4TDvDnn38yadIk2rZty6pVq1i4cCFLly4FoEmTJlSqVImuXbsybdo0EhIS6N+/Pw0aNLB+wDJ27FgaN25MyZIl6dy5MwkJCSxbtozhw4ff93MYGhrKyJEjbRYDBThy5AjXr1/n3Llz3Lx5k507dwKWKWbc3Nzo0qUL48ePp3fv3gwfPpy9e/fywQcf8P7779vc/4YNG6hfv36aRtGLiIjkVCaTZYpEPz/4b/rkFAzDMgWjvWljkv413zRTkb3kcjNwNt1lnkbJHm5bd+ee3Ta1yoPISvm3yWSiV69eTJ06lStXrtjkoaVLl2bu3LmsXLmSUqVKMW/ePLZu3ZpiTZ+02LNnj830NCaTiSpVqtC6dWv69OnDp59+ire3NyNGjKBw4cK0bt0agCFDhtC8eXPKlCnDlStXWLt2rXUAzZtvvkmNGjWoUKECsbGx/PLLLzZzw4uIiEjmcnV2tcyDnrcklLTdZxgG52+c59SujTD5DmtI3k0GDmy4KyObGT58uFGvXr17Ouf11183KlSoYNPWqVMnIzQ0NE3nX7t2zQCMa9eupdh38+ZNY9++fcbNmzfvKaasoEePHgZg3by9vY2aNWsa33//vc1xZ8+eNbp3727kz5/fcHd3N0qUKGH06dPHuHbtmnHu3DmjTZs2RmBgoOHm5mYUK1bMePPNN43ExETDMAwjJibGaNeuneHn52cARlhYWKqxjB071iaWpM3d3d16TNJ9d+jQwfD09DQKFixofPDBBzb3c/z4cePpp582cufObXh7exsdOnQwzp07Z3PMokWLjKpVqxpubm5G/vz5jWeeecbmGu+//77N8VWqVDHGjh17x+eyVq1axsyZM23aGjRokOpjCg8Ptx6za9cuo169eoa7u7tRuHBh4//+7/9S3HfZsmWNr7/+2u61s3MfTE+JiYnG2bNnrX1PJIn6htijviGpMZsN49Lpm8bV+i2NszWbGok3bmTq9e+Ud+Yk6Z5/m82GUbKkYZhMhmH5PCVtm8lkOc9sTpfHlZXy7yQnT540nJycUrxfiomJMXr27Gn4+voafn5+Rr9+/YwRI0YYVapUsXk8rVu3tnvfa9euTTUfdnZ2NgzDMC5fvmx069bN8PX1NTw8PIzQ0FDj0KFD1vMHDhxolCxZ0nB3dzf8/f2Nbt26GRcvXjQMwzAmTJhgBAcHGx4eHkbevHmN1q1bG0ePHr3bryBdKP+20N9RsUd9Q+xR3xDjwoV7y8Vu3/7LA9JTWvNvk2GkdRb3rKF8+fKEhoZy6tQp1q9fT+HChenfv/8dF4t8/PHHqV69unUBSrDMIzhkyBCuXbuW4vjY2FhiY2OttyMjIylatChXrlxJ8ZXKmJgYjh07RlBQUIYuLikQFBTEwIEDeeWVVxwdSgpLly7l9ddfZ8+ePTjdPof9A1i+fDmvvvoqu3btwsUl9S+OxMTEEB4eTvHixXN0HzSbzVy4cAF/f/90/R1I9qe+Ifaob4hdMTHQsSNxsbG4LF6Mk6dnpl06MjKSPHnycO3atTtO5fGwi4yMxNfXN9XnISn3uef8+4MPYOhQy1uwtDKZYNo0hy5i5UiGYZCQkGCdzkUs7rsPPmTMZjMREREEBATo76jYUN8Qe9Q3BMOA0qXh6NF7z8lKlIDDh9N9TvQ75Z23ynbTuRw9epQZM2YwbNgwRo0axdatWxk8eDBubm706NEj1XPOnTtnMyUIQIECBYiMjOTmzZsppsl49913GT9+fIr7uXDhAjExMTZt8fHxmM1mEhISSEhIeMBHJ3djGAbx8fFZLokPDQ3l4MGDHD9+3Dofe3qIjIxk9uzZAHb7V0JCAmazmUuXLqVYFDUnMZvNXLt2DcMw9MdYbKhviD3qG2JXTAx+sbHEJyRwOSIiU4voUVFRmXatHKdHD3jjDbh5E+6waLyVkxN4eMB/06SIiIiIyAMymSyLgw4deu/nDh7ssEVFIRsW0c1mMyEhIUycOBGwLKizd+9eZs6cabeIfq9GjhzJsGHDrLeTRqL7+/unOhImKioKFxcXuyOFJf2YTKYsWyi+tc+kl06dOt31GBcXF5ycnMiXL1+OHwljMpk0olRSUN8Qe9Q3xK6YGPhvDumAgIBMLaLn5L/lGc7PDxYtgpYtLQXyOxXSnZwsb9J++MFynoiIiIikj2w6sCHbVX0DAwNtVmsHy8ruixYtsntOwYIFOX/+vE3b+fPn8fHxSXWxRnd391QX33FyckrxJtvJyQmTyWTdJOOEh4dbR2PruU6W1PdS6585jZ4HsUd9Q+xR35BUOTlhADigf6gvZrDQUFi6FNq1g+hoS9utXyVOyjE9PCwF9KZNMz9GERERkYdZNh3YkO2y9Lp163Lw4EGbtkOHDlGsWDG759SuXZvVq1fbtK1atYratWtnSIwiIiIiks25u4Obm6OjkIwQGgqnTlnmOi9RwnZfiRKW9tOnVUAXERERyShJAxs8PCxF8tsHyya1eXjAsmVZIi/LdkX0oUOHsmnTJiZOnMiRI0dYsGABs2bNYsCAAdZjRo4cSfdbhvj37duXo0eP8vrrr3PgwAE++eQTvvvuO4bez/w7IiIiIvJwy5ULY+FCrn72GWh6lYeTn59lXs3Dh+HiRQgPt/x7+LCl3dfX0RGKiIiIPNyy2cCGbDedS82aNVm8eDEjR47krbfeIigoiGnTptG1a1frMWfPnuXEiRPW20FBQSxdupShQ4fywQcfUKRIET777DNCQ0PTLS7jXlaUFUlH6nsiIiKSE6VLDmQyQb58lk0kjZR/i4iIpJOkgQ2DBmG+eJFLx46Rr3hxnPLnd+gioqnJdkV0gFatWtGqVSu7++fMmZOirWHDhuzYsSPdY0la5DI6OjrV+dVFMlr0f/N5ZtUFV0VERETSk/JvcTTl3yIiIunsv4ENiYmJlsENWayADtm0iJ6VODs74+fnR0REBACenp5a9DKDGIZBQkICLi4ueo6xPB/R0dFERETg5+eHs7Ozo0MSERF5OMTFwcSJ5L5xAyZM0JQuWYzy78yj/NuW8m8REZEMkg3ybxXR00HBggUBrIm8ZAzDMDCbzTg5OSmJv4Wfn5+1D4qIiEg6MJsxbduGa1wcmM2OjkZSofw7cyj/Tp3ybxERkXSWDfJvFdHTgclkIjAwkICAAOLj4x0dzkPLbDZz6dIl8uXLh5NTtlsTN0O4urpqBIyIiIjkOMq/M4fy75SUf4uIiORMKqKnI2dnZyVUGchsNuPq6kquXLmUxIuIiIiI8u8MpvxbRERExEKZkIiIiIiIiIiIiIiIHSqii4iIiIiIiIiIiIjYoSK6iIiIiIiIiIiIiIgdmhM9DQzDACAyMtLBkeRsZrOZqKgozckoqVL/EHvUN8Qe9Q2xKyYGIz6e2IQE3CIjcUpIyLRLJ+WbSflnTqX8O2vQ66TcifqH2KO+Ifaob4hd2SD/VhE9DaKiogAoWrSogyMRERERkUwVGOiQy0ZFReHr6+uQa2cFyr9FREREcqgsmn+bjJw+zCUNzGYzZ86cwdvbG5PJ5OhwcqzIyEiKFi3KyZMn8fHxcXQ4ksWof4g96htij/qG3Imj+odhGERFRVGoUKEcPUJL+XfWoNdJuRP1D7FHfUPsUd+QO8nq+bdGoqeBk5MTRYoUcXQY8h8fHx+92Ipd6h9ij/qG2KO+IXfiiP6Rk0egJ1H+nbXodVLuRP1D7FHfEHvUN+ROsmr+nXOHt4iIiIiIiIiIiIiI3IWK6CIiIiIiIiIiIiIidqiILtmGu7s7Y8eOxd3d3dGhSBak/iH2qG+IPeobcifqHyL6fyB3pv4h9qhviD3qG3InWb1/aGFRERERERERERERERE7NBJdRERERERERERERMQOFdFFREREREREREREROxQEV1ERERERERERERExA4V0SXLe/fdd6lZsybe3t4EBATQpk0bDh486OiwJAv6v//7P0wmE0OGDHF0KJJFnD59mueee458+fLh4eFBpUqV2LZtm6PDEgdLTExkzJgxBAUF4eHhQcmSJZkwYQJaJibn+f3333nqqacoVKgQJpOJH3/80Wa/YRi8+eabBAYG4uHhQZMmTTh8+LBjghXJRMq/Ja2Uf8vtlH9LapR/S5LsnH+riC5Z3vr16xkwYACbNm1i1apVxMfH07RpU27cuOHo0CQL2bp1K59++imVK1d2dCiSRVy5coW6devi6urK8uXL2bdvH1OmTCFPnjyODk0c7L333mPGjBl89NFH7N+/n/fee49JkyYxffp0R4cmmezGjRtUqVKFjz/+ONX9kyZN4sMPP2TmzJls3ryZ3LlzExoaSkxMTCZHKpK5lH9LWij/ltsp/xZ7lH9Lkuycf5sMfewj2cyFCxcICAhg/fr1PP74444OR7KA69evU716dT755BPefvttqlatyrRp0xwdljjYiBEj+OOPP9iwYYOjQ5EsplWrVhQoUIDPP//c2tauXTs8PDyYN2+eAyMTRzKZTCxevJg2bdoAllEwhQoV4pVXXuHVV18F4Nq1axQoUIA5c+bQuXNnB0YrkrmUf8vtlH9LapR/iz3KvyU12S3/1kh0yXauXbsGQN68eR0ciWQVAwYMoGXLljRp0sTRoUgWsmTJEkJCQujQoQMBAQFUq1aN2bNnOzosyQLq1KnD6tWrOXToEAC7du1i48aNNG/e3MGRSVYSHh7OuXPnbP62+Pr68uijj/LXX385MDKRzKf8W26n/FtSo/xb7FH+LWmR1fNvF0cHIHIvzGYzQ4YMoW7dulSsWNHR4UgW8M0337B9+3a2bt3q6FAkizl69CgzZsxg2LBhjBo1iq1btzJ48GDc3Nzo0aOHo8MTBxoxYgSRkZGUK1cOZ2dnEhMTeeedd+jataujQ5Ms5Ny5cwAUKFDApr1AgQLWfSI5gfJvuZ3yb7FH+bfYo/xb0iKr598qoku2MmDAAPbu3cvGjRsdHYpkASdPnuTll19m1apV5MqVy9HhSBZjNpsJCQlh4sSJAFSrVo29e/cyc+ZMJfE53Hfffcf8+fNZsGABFSpUYOfOnQwZMoRChQqpb4iI3Eb5t9xK+bfcifJvsUf5tzwMNJ2LZBsDBw7kl19+Ye3atRQpUsTR4UgW8PfffxMREUH16tVxcXHBxcWF9evX8+GHH+Li4kJiYqKjQxQHCgwMpHz58jZtwcHBnDhxwkERSVbx2muvMWLECDp37kylSpXo1q0bQ4cO5d1333V0aJKFFCxYEIDz58/btJ8/f966T+Rhp/xbbqf8W+5E+bfYo/xb0iKr598qokuWZxgGAwcOZPHixaxZs4agoCBHhyRZROPGjdmzZw87d+60biEhIXTt2pWdO3fi7Ozs6BDFgerWrcvBgwdt2g4dOkSxYsUcFJFkFdHR0Tg52aZAzs7OmM1mB0UkWVFQUBAFCxZk9erV1rbIyEg2b95M7dq1HRiZSMZT/i32KP+WO1H+LfYo/5a0yOr5t6ZzkSxvwIABLFiwgJ9++glvb2/rPEi+vr54eHg4ODpxJG9v7xRzc+bOnZt8+fJpzk5h6NCh1KlTh4kTJ9KxY0e2bNnCrFmzmDVrlqNDEwd76qmneOedd3jkkUeoUKECO3bsYOrUqfTq1cvRoUkmu379OkeOHLHeDg8PZ+fOneTNm5dHHnmEIUOG8Pbbb1O6dGmCgoIYM2YMhQoVok2bNo4LWiQTKP8We5R/y50o/xZ7lH9Lkuycf5sMwzAcHYTInZhMplTbw8LC6NmzZ+YGI1lew4YNqVq1KtOmTXN0KJIF/PLLL4wcOZLDhw8TFBTEsGHD6NOnj6PDEgeLiopizJgxLF68mIiICAoVKsSzzz7Lm2++iZubm6PDk0y0bt06GjVqlKK9R48ezJkzB8MwGDt2LLNmzeLq1avUq1ePTz75hDJlyjggWpHMo/xb7oXyb7mV8m9JjfJvSZKd828V0UVERERERERERERE7NCc6CIiIiIiIiIiIiIidqiILiIiIiIiIiIiIiJih4roIiIiIiIiIiIiIiJ2qIguIiIiIiIiIiIiImKHiugiIiIiIiIiIiIiInaoiC4iIiIiIiIiIiIiYoeK6CIiIiIiIiIiIiIidqiILiIiIiIiIiIiIiJih4roIiKSpZhMJn788UdHhyEiIiIikiMo/xYRuTsV0UVExKpnz56YTKYUW7NmzRwdmoiIiIjIQ0f5t4hI9uDi6ABERCRradasGWFhYTZt7u7uDopGREREROThpvxbRCTr00h0ERGx4e7uTsGCBW22PHnyAJaves6YMYPmzZvj4eFBiRIl+P77723O37NnD0888QQeHh7ky5ePF198kevXr9sc88UXX1ChQgXc3d0JDAxk4MCBNvsvXrxI27Zt8fT0pHTp0ixZsiRjH7SIiIiIiIMo/xYRyfpURBcRkXsyZswY2rVrx65du+jatSudO3dm//79ANy4cYPQ0FDy5MnD1q1bWbhwIb/99ptNkj5jxgwGDBjAiy++yJ49e1iyZAmlSpWyucb48ePp2LEju3fvpkWLFnTt2pXLly9n6uMUEREREckKlH+LiDieyTAMw9FBiIhI1tCzZ0/mzZtHrly5bNpHjRrFqFGjMJlM9O3blxkzZlj3PfbYY1SvXp1PPvmE2bNnM3z4cE6ePEnu3LkBWLZsGU899RRnzpyhQIECFC5cmOeff56333471RhMJhOjR49mwoQJgOWNgZeXF8uXL9fckCIiIiLyUFH+LSKSPWhOdBERsdGoUSObJB0gb9681p9r165ts6927drs3LkTgP3791OlShVrAg9Qt25dzGYzBw8exGQycebMGRo3bnzHGCpXrmz9OXfu3Pj4+BAREXG/D0lEREREJMtS/i0ikvWpiC4iIjZy586d4uud6cXDwyNNx7m6utrcNplMmM3mjAhJRERERMShlH+LiGR9mhNdRETuyaZNm1LcDg4OBiA4OJhdu3Zx48YN6/4//vgDJycnypYti7e3N8WLF2f16tWZGrOIiIiISHal/FtExPE0El1ERGzExsZy7tw5mzYXFxfy588PwMKFCwkJCaFevXrMnz+fLVu28PnnnwPQtWtXxo4dS48ePRg3bhwXLlxg0KBBdOvWjQIFCgAwbtw4+vbtS0BAAM2bNycqKoo//viDQYMGZe4DFRERERHJApR/i4hkfSqii4iIjV9//ZXAwECbtrJly3LgwAEAxo8fzzfffEP//v0JDAzk66+/pnz58gB4enqyYsUKXn75ZWrWrImnpyft2rVj6tSp1vvq0aMHMTExvP/++7z66qvkz5+f9u3bZ94DFBERERHJQpR/i4hkfSbDMAxHByEiItmDyWRi8eLFtGnTxtGhiIiIiIg89JR/i4hkDZoTXURERERERERERETEDhXRRURERERERERERETs0HQuIiIiIiIiIiIiIiJ2aCS6iIiIiIiIiIiIiIgdKqKLiIiIiIiIiIiIiNihIrqIiIiIiIiIiIiIiB0qoouIiIiIiIiIiIiI2KEiuoiIiIiIiIiIiIiIHSqii4iIiIiIiIiIiIjYoSK6iIiIiIiIiIiIiIgdKqKLiIiIiIiIiIiIiNihIrqIiIiIiIiIiIiIiB0qoouIiIiIiIiIiIiI2KEiuoiIiIiIiIiIiIiIHSqii4iIiIiIiIiIiIjYoSK6iIiIiIiIiIiIiIgdKqKLiIiIiIiIiIiIiNihIrqIZBnHjh3DZDIxZ84ca9u4ceMwmUxpOt9kMjFu3Lh0jalhw4Y0bNgwXe8zu0rt95PeevbsSfHixTPs/iX9mUwmBg4c6OgwREREJIfJ6Xmj3juJiGQuFdFF5L48/fTTeHp6EhUVZfeYrl274ubmxqVLlzIxsnu3b98+xo0bx7FjxxwditW6deswmUzWzdXVlRIlStC9e3eOHj3q6PAyTXR0NOPGjWPdunWODsVhbu0Ht299+/Z1dHgiIiIiNu6Uu9y65aT8Tu+dMlbSe6fvv//e0aGIyEPMxdEBiEj21LVrV37++WcWL15M9+7dU+yPjo7mp59+olmzZuTLl+++rzN69GhGjBjxIKHe1b59+xg/fjwNGzZMMZpl5cqVGXrtuxk8eDA1a9YkPj6e7du3M2vWLJYuXcqePXsoVKiQQ2PLCLNnz8ZsNltvR0dHM378eIAcParlySefTPX/WZkyZRwQjYiIiIh9X331lc3tuXPnsmrVqhTtwcHBD3Sd2/PGrEzvnUREsj8V0UXkvjz99NN4e3uzYMGCVBPBn376iRs3btC1a9cHuo6LiwsuLo57qXJzc3PYtQHq169P+/btAXj++ecpU6YMgwcP5ssvv2TkyJEPdN83btwgd+7c6RFmunF1dXV0CJkuJiYGNzc3nJzsfzmsTJkyPPfcc5kYlYiIiMj9uT1n2bRpE6tWrbprLhMdHY2np2ear5Od8ka9dxIRyf40nYuI3BcPDw+eeeYZVq9eTURERIr9CxYswNvbm6effprLly/z6quvUqlSJby8vPDx8aF58+bs2rXrrtdJbV6/2NhYhg4dir+/v/Uap06dSnHu8ePH6d+/P2XLlsXDw4N8+fLRoUMHm68ezpkzhw4dOgDQqFGjFF8vTW1ev4iICHr37k2BAgXIlSsXVapU4csvv7Q5JmmOwv/973/MmjWLkiVL4u7uTs2aNdm6detdH7c9TzzxBADh4eHWtuXLl1O/fn1y586Nt7c3LVu25J9//rE5r2fPnnh5efHvv//SokULvL29rUl6w4YNqVixIn///Td16tTBw8ODoKAgZs6cmaaYDhw4QPv27cmbNy+5cuUiJCSEJUuWWPdHRETg7+9Pw4YNMQzD2n7kyBFy585Np06dbOJMGtFy7Ngx/P39ARg/frz1dzNu3DjCwsIwmUzs2LEjRTwTJ07E2dmZ06dP3zHuHTt20Lx5c3x8fPDy8qJx48Zs2rTJun/btm2YTKYUv1uAFStWYDKZ+OWXX6xtp0+fplevXhQoUAB3d3cqVKjAF198YXNe0ldNv/nmG0aPHk3hwoXx9PQkMjLyjrGmxb38HtPShwHMZjMffPABlSpVIleuXPj7+9OsWTO2bduW4tgff/yRihUrWh/7r7/+arM/KiqKIUOGULx4cdzd3QkICODJJ59k+/btD/zYRUREJHu5NW95/PHH8fT0ZNSoUYCloNyyZUsKFSqEu7s7JUuWZMKECSQmJtrcx+1zoj9I/n0ved/95DR67+SY9063O3r0KB06dCBv3rx4enry2GOPsXTp0hTHTZ8+nQoVKuDp6UmePHkICQlhwYIF1v3Ka0VyJo1EF5H71rVrV7788ku+++47m4UFL1++zIoVK3j22Wfx8PDgn3/+4ccff6RDhw4EBQVx/vx5Pv30Uxo0aMC+ffvueVqSF154gXnz5tGlSxfq1KnDmjVraNmyZYrjtm7dyp9//knnzp0pUqQIx44dY8aMGTRs2JB9+/bh6enJ448/zuDBg/nwww8ZNWqU9Wul9r5eevPmTRo2bMiRI0cYOHAgQUFBLFy4kJ49e3L16lVefvllm+MXLFhAVFQUL730EiaTiUmTJvHMM89w9OjR+xo98++//wJYv+b51Vdf0aNHD0JDQ3nvvfeIjo5mxowZ1KtXjx07dti8sUhISCA0NJR69erxv//9z2akz5UrV2jRogUdO3bk2Wef5bvvvqNfv364ubnRq1cvu/H8888/1K1bl8KFCzNixAhy587Nd999R5s2bVi0aBFt27YlICCAGTNm0KFDB6ZPn87gwYMxm8307NkTb29vPvnkk1Tv29/fnxkzZtCvXz/atm3LM888A0DlypUJCgpiwIABzJ8/n2rVqtmcN3/+fBo2bEjhwoXvGHf9+vXx8fHh9ddfx9XVlU8//ZSGDRuyfv16Hn30UUJCQihRogTfffcdPXr0sDn/22+/JU+ePISGhgJw/vx5HnvsMesim/7+/ixfvpzevXsTGRnJkCFDbM6fMGECbm5uvPrqq8TGxt511E5MTAwXL15M0e7j42Nzblp+j/fSh3v37s2cOXNo3rw5L7zwAgkJCWzYsIFNmzYREhJiPW7jxo388MMP9O/fH29vbz788EPatWvHiRMnrH21b9++fP/99wwcOJDy5ctz6dIlNm7cyP79+6levfodH7+IiIg8fC5dukTz5s3p3Lkzzz33HAUKFAAshVovLy+GDRuGl5cXa9as4c033yQyMpLJkyff9X7vJ/++l7zvfnMavXfK/PdOtzp//jx16tQhOjqawYMHky9fPr788kuefvppvv/+e9q2bQtYpgkaPHgw7du35+WXXyYmJobdu3ezefNmunTpAiivFcmxDBGR+5SQkGAEBgYatWvXtmmfOXOmARgrVqwwDMMwYmJijMTERJtjwsPDDXd3d+Ott96yaQOMsLAwa9vYsWONW1+qdu7caQBG//79be6vS5cuBmCMHTvW2hYdHZ0i5r/++ssAjLlz51rbFi5caADG2rVrUxzfoEEDo0GDBtbb06ZNMwBj3rx51ra4uDijdu3ahpeXlxEZGWnzWPLly2dcvnzZeuxPP/1kAMbPP/+c4lq3Wrt2rQEYX3zxhXHhwgXjzJkzxtKlS43ixYsbJpPJ2Lp1qxEVFWX4+fkZffr0sTn33Llzhq+vr017jx49DMAYMWJEqo8RMKZMmWJti42NNapWrWoEBAQYcXFxNo/p1t9P48aNjUqVKhkxMTHWNrPZbNSpU8coXbq0zXWeffZZw9PT0zh06JAxefJkAzB+/PFHm2N69OhhFCtWzHr7woULKX6vt95foUKFbPrW9u3bU8SYmjZt2hhubm7Gv//+a207c+aM4e3tbTz++OPWtpEjRxqurq42v8PY2FjDz8/P6NWrl7Wtd+/eRmBgoHHx4kWb63Tu3Nnw9fW19sWk32uJEiVS7Z+pAexuX3/9tfW4tP4e09qH16xZYwDG4MGDU8RkNptt4nNzczOOHDlibdu1a5cBGNOnT7e2+fr6GgMGDEjTYxYREZGHx4ABA4zbSw9JecvMmTNTHJ9ajvTSSy8Znp6eNjnn7Xnjg+bfac377jen0Xsni4x877Rw4UK7xwwZMsQAjA0bNljboqKijKCgIKN48eLW57x169ZGhQoV7ng95bUiOZOmcxGR++bs7Eznzp3566+/bL7mt2DBAgoUKEDjxo0BcHd3t873nJiYyKVLl/Dy8qJs2bL3/JW3ZcuWAZYFN291+0hfsHxtMkl8fDyXLl2iVKlS+Pn53fdX7ZYtW0bBggV59tlnrW2urq4MHjyY69evs379epvjO3XqRJ48eay369evD1i+SpgWvXr1wt/fn0KFCtGyZUtu3LjBl19+SUhICKtWreLq1as8++yzXLx40bo5Ozvz6KOPsnbt2hT3169fv1Sv4+LiwksvvWS97ebmxksvvURERAR///13qudcvnyZNWvW0LFjR6KioqzXv3TpEqGhoRw+fNhmSpWPPvoIX19f2rdvz5gxY+jWrRutW7dO0/OQmu7du3PmzBmbxzl//nw8PDxo166d3fMSExNZuXIlbdq0oUSJEtb2wMBAunTpwsaNG63Tq3Tq1In4+Hh++OEH63ErV67k6tWr1mloDMNg0aJFPPXUUxiGYfO7CA0N5dq1ayn6W48ePWz65920bt2aVatWpdgaNWpkc1xafo9p7cOLFi3CZDIxduzYFPHc/jXhJk2aULJkSevtypUr4+PjY9PP/fz82Lx5M2fOnEnz4xYREZGHl7u7O88//3yK9ltzpKQcs379+kRHR3PgwIG73u/95t9pyfvg/nMavXeyyMj3TneLpVatWtSrV8/a5uXlxYsvvsixY8fYt28fYPn9njp16o7TyCivFcmZVEQXkQeSNK920hxxp06dYsOGDXTu3BlnZ2fAMq/y+++/T+nSpXF3dyd//vz4+/uze/durl27dk/XO378OE5OTjYFO4CyZcumOPbmzZu8+eabFC1a1Oa6V69evefr3nr90qVLp1gEMukrjMePH7dpf+SRR2xuJyWFV5U1VGMAAOmLSURBVK5cSdP13nzzTVatWsWaNWvYvXs3Z86coVu3bgAcPnwYsMyT7u/vb7OtXLkyxXyLLi4uFClSJNXrFCpUKMUio2XKlAGwSfJvdeTIEQzDYMyYMSmun1R4vTWGvHnz8uGHH7J79258fX358MMP0/Qc2PPkk08SGBjI/PnzAUs/+/rrr2ndujXe3t52z7tw4QLR0dGp9png4GDMZjMnT54EoEqVKpQrV45vv/3Wesy3335L/vz5rfPTX7hwgatXrzJr1qwUz0PSG8PbfxdBQUH39FiLFClCkyZNUmxJX3tOkpbfY1r78L///kuhQoXImzfvXeO7vZ+Dpa/f2s8nTZrE3r17KVq0KLVq1WLcuHHp8oZIREREsqfChQunOqXdP//8Q9u2bfH19cXHxwd/f3/roqRpyeHvN/9OS94HD5bT6L2TRUa9d7pbLPby/1tjGT58OF5eXtSqVYvSpUszYMAA/vjjD5tzlNeK5EyaE11EHkiNGjUoV64cX3/9NaNGjeLrr7/GMAybleUnTpzImDFj6NWrFxMmTCBv3rw4OTkxZMgQzGZzhsU2aNAgwsLCGDJkCLVr18bX1xeTyUTnzp0z9Lq3SkqGb2fcssDmnVSqVIkmTZqkui/pMXz11VcULFgwxX4XF9uX+FtHtaSHpOu/+uqr1jkib1eqVCmb2ytWrAAsifCpU6fw8/O77+s7OzvTpUsXZs+ezSeffMIff/zBmTNnrG+y0kunTp145513uHjxIt7e3ixZsoRnn33W+vwmPQ/PPfdcijk0k1SuXNnm9r2MQs8O0tLPO3bsSP369Vm8eDErV65k8uTJvPfee/zwww80b948s0IVERGRLCK1fOjq1as0aNAAHx8f3nrrLUqWLEmuXLnYvn07w4cPT1MO/yD5993yPniwnEbvne7sQd87pYfg4GAOHjzIL7/8wq+//sqiRYv45JNPePPNNxk/fjygvFYkp1IRXUQeWNeuXRkzZgy7d+9mwYIFlC5dmpo1a1r3f//99zRq1IjPP//c5ryrV6+SP3/+e7pWsWLFMJvN/PvvvzYjCQ4ePJji2O+//54ePXowZcoUa1tMTAxXr161Oe72qSnudv3du3djNpttCtJJXy0tVqxYmu/rQSWNKAkICLBbaE+rM2fOcOPGDZtRzIcOHQKwWZz0VklTobi6uqbp+r/++iufffYZr7/+OvPnz6dHjx5s3rw5RbH/Vnf73XTv3p0pU6bw888/s3z5cvz9/e0W9JP4+/vj6emZap85cOAATk5O/D97dx5nY/n/cfx1zqy2GUszxjCWCGNfssu+byGylTWikEhRUUKijciWNdnlSyQZe5bsWypDyc4QZgxmPffvj5MzzY8RY8x9zsz7+Xicx+O6r3Ofc95n5mq6zsd1rjsoKMjR165dO0aMGMG3335Lzpw5iYiIoH379omeL0uWLMTHxz/y7+FRPcjv8UHHcMGCBfnxxx+5evXqA61GfxC5cuXilVde4ZVXXiEsLIxy5coxevRofdgQERERADZv3szff//N8uXLqVGjhqP/5MmTqfL6/zXvu+NR5jT67GTOZ6d8+fIlOf///1kyZcpEu3btaNeuHTExMbRu3ZrRo0czdOhQvL29Ac1rRdIjbeciIo/szsqJ4cOHc/DgwUQrKcC+ouD/rx5YunRpov2yH9SdScn/3wpk/Pjxd517r9edOHEi8fHxifruFBz//wTxXpo0acLFixcTfc0zLi6OiRMnkjlzZmrWrPkgbyNFNGzYEB8fHz788ENiY2Pvuv/y5csP/FxxcXFMmzbNcRwTE8O0adPw8/OjfPny93yMv78/tWrVYtq0aVy4cOG+r3/9+nVeeuklKlasyIcffsiMGTPYv38/H3744X1zZcyY0fH4eylVqhSlSpVixowZfPvtt7Rv3/6+RXmwj4sGDRqwcuXKRFvVXLp0iQULFlC9enV8fHwc/cHBwZQsWZLFixezePFicuXKlehDnZubG8899xzffvstv/zyy31/Do/bg/weH3QMP/fccxiG4Vhx828PuxooPj7+rq8B+/v7ExgYSHR09EM9l4iIiKRdd1Yi/3uuERMTw+TJk1Pl9f9r3pcScxp9djLns1OTJk3YvXs3O3fudPTdvHmT6dOnkz9/fooVKwbA33//nehxnp6eFCtWDMMwiI2N1bxWJB3TSnQReWQFChSgatWqrFy5EuCuiWCzZs344IMP6NatG1WrVuXIkSPMnz8/0UUdH1SZMmXo0KEDkydPJjw8nKpVq7JhwwZOnDhx17nNmjVj3rx5+Pr6UqxYMXbu3Mn69evJkSPHXc/p5ubG2LFjCQ8Px8vLizp16uDv73/Xc/bq1Ytp06bRtWtX9u3bR/78+Vm2bBnbt29n/Pjx992LO6X5+PgwZcoUXnzxRcqVK0f79u3x8/Pj9OnTfP/991SrVo1JkyY90HMFBgYyduxY/vrrLwoXLszixYs5ePAg06dPx8PDI8nHffnll1SvXp2SJUvSs2dPnnzySS5dusTOnTs5e/Yshw4dAuC1117j77//Zv369bi5udGoUSNeeuklRo0axbPPPkvp0qXv+fwZMmSgWLFiLF68mMKFC5M9e3ZKlChBiRIlHOd07tyZN954A+CBt3IZNWoUISEhVK9enVdeeQV3d3emTZtGdHQ048aNu+v8du3aMXz4cLy9venRo8dd2+J89NFHbNq0iUqVKtGzZ0+KFSvG1atX2b9/P+vXr+fq1asPlCspoaGhfPPNN3f158yZk/r16zuOH+T3+KBjuHbt2rz44ot88cUXHD9+nEaNGmGz2fjpp5+oXbs2ffv2feD8N27cIE+ePLRp04bSpUuTOXNm1q9fz549exKtdhIREZH0rWrVqmTLlo0uXbrQv39/LBYL8+bNS9XtPO4370uJOY0+Oz2+z07ffvvtPS8+26VLF4YMGcLChQtp3Lgx/fv3J3v27MydO5eTJ0/y7bffOn7PDRo0ICAggGrVqpEzZ05+++03Jk2aRNOmTcmSJQvXr1/XvFYkvTJERFLAl19+aQBGxYoV77ovKirKGDRokJErVy4jQ4YMRrVq1YydO3caNWvWNGrWrOk47+TJkwZgzJ4929H33nvvGf//T9Xt27eN/v37Gzly5DAyZcpkNG/e3Dhz5owBGO+9957jvGvXrhndunUznnjiCSNz5sxGw4YNjd9//93Ily+f0aVLl0TP+dVXXxlPPvmk4ebmZgDGpk2bDMMw7spoGIZx6dIlx/N6enoaJUuWTJT53+/l448/vuvn8f9z3sumTZsMwFi6dOl9z7tzbsOGDQ1fX1/D29vbKFiwoNG1a1dj7969jnO6dOliZMqU6Z6Pr1mzplG8eHFj7969RpUqVQxvb28jX758xqRJk+75nv7/e/3jjz+Mzp07GwEBAYaHh4eRO3duo1mzZsayZcsMwzCMlStXGoDx6aefJnpcRESEkS9fPqN06dJGTEyMI2e+fPkSnbdjxw6jfPnyhqen5z1/dhcuXDDc3NyMwoUL/+fP6t/2799vNGzY0MicObORMWNGo3bt2saOHTvuee7x48cNwACMbdu23fOcS5cuGa+++qoRFBRkeHh4GAEBAUbdunWN6dOnO855mN/rHXde9163f4/NB/093sn6X2PYMAwjLi7O+Pjjj42iRYsanp6ehp+fn9G4cWNj3759ifK9+uqrdz323/+dRUdHG4MHDzZKly5tZMmSxciUKZNRunRpY/LkyQ/8cxARERHX9Oqrr941n78zb7mX7du3G5UrVzYyZMhgBAYGGm+++abx448/JpqjG8bd88ZHnX/fcb95X0rNafTZaXaic1Lqs1NSt59++skwDPvnljZt2hhZs2Y1vL29jYoVKxqrV69O9FzTpk0zatSoYeTIkcPw8vIyChYsaAwePNgIDw83DEPzWpH0zGIYqfhPuiIi4nRq1arFlStX7rkViSu4cuUKuXLlYvjw4QwbNszsOKZx9d+jiIiIiIiIiLPSnugiIuLS5syZQ3x8PC+++KLZUUREREREREQkDdKe6CIi4pI2btzIr7/+yujRo2nZsiX58+c3O5KIiIiIiIiIpEEqoouIiEv64IMP2LFjB9WqVWPixIlmxxERERERERGRNEp7oouIiIiIiIiIiIiIJEF7oouIiIiIiIiIiIiIJEFFdBERERERERERERGRJGhP9Adgs9k4f/48WbJkwWKxmB1HRERERNIowzC4ceMGgYGBWK3pd72L5t8iIiIikhoedP6tIvoDOH/+PEFBQWbHEBEREZF04syZM+TJk8fsGKbR/FtEREREUtN/zb9VRH8AWbJkAew/TB8fH5PTpF82m43Lly/j5+eXrldmyb1pfEhSNDYkKRobkqSoKIwXXyQ6NhbPBQuwZsyYai8dERFBUFCQY/6ZXmn+7Rz0d1LuR+NDkqKxIUnR2JAkucD8W0X0B3DnK6Q+Pj6axJvIZrMRFRWFj4+P/tjKXTQ+JCkaG5IUjQ1JkqcnhocH0YaBp49Pqk7i70jvW5ho/u0c9HdS7kfjQ5KisSFJ0diQJLnA/FsjVkREREREREREREQkCSqii4iIiIiIiIiIiIgkQUV0EREREREREREREZEkqIguIiIiIiIiIiIiIpIEFdFFRERERERERERERJLgbnYAERERERGn4u6O0bIl0Tdu4Omu6bKIiIiIyGPlAvNv50wlIiIiImIWd3fo3p3bYWFkcdJJvIiIiIhImuEC829t5yIiIiIiIiIiIiIikgQV0UVERERE/s0wICwM65Ur9raIiIiIiDw+LjD/ds718SIiIiIiZomOxvLSS/jExMCKFZAxo9mJRERERETSLheYf2sluoiIiIiIiIiIiIhIElREFxERERERERERERFJgoroIiIiIiIiIiIiIiJJUBHdyV35/Qq/r/zd7BgiIiIiIunCqVOwfLnZKURERETEmejCok7KMAzW9F3Dvmn78MriRb4/8pEhewazY4mIiIiIpFldusA334CXF5w7B9mymZ1IRERERJyBVqI7KYvFQtztOIx4g6jrUWwdtdXsSCIiIiIiaVrWrGCzwe3bMHeu2WlERERExFmoiO7Eao+sjXsG+5cFdk/azbU/r5mcSERERCQdcHPDaNyY6Lp1wc3N7DSSivr0SWhPmQKGYV4WERERkXTDBebfKqI7MZ/cPlQZWAUAW6yNje9sNDmRiIiISDrg4QF9+nC7Sxd7W9KNokWhdm17OzQUNmr6LSIiIvL4ucD8W0V0J1ftzWpk9MsIwC+LfuHcnnMmJxIRERERSbv+vRp98mTzcoiIiIiI81AR3cl5+XhR872ajuOQN0Iw9L1SERERkcfHMCA8HEtEhPbzSIdatoSAAHt75Ur7BUZFRERE5DFygfm3iuguoHyv8mR/KjsAp7aeInR1qMmJRERERNKw6GgsL76Ib9++EB1tdhpJZR4e0LOnvR0fDzNmmJtHREREJM1zgfm3iuguwM3DjXof1XMcr39zPbY4m4mJRERERETSrl69Eq5pNX06xMaam0dEREREzKUiuoso2qooQdWCALjy+xX2z9xvciIRERERkbQpTx5o3tzePn8eVq0yN4+IiIiImEtFdBdhsVio/3F9x/Hm9zYTExljYiIRERERkbRLFxgVERERkTtURHchQVWCCH4uGICbl26y45MdJicSEREREUmb6tWDQoXs7Q0bIFSXJRIRERFJt1REdzF1x9TF6m7/te34eAc3LtwwOZGIiIiISNpjtULv3gnHU6eal0VEREREzKUiuovJ8VQOnu7zNACxt2LZ/N5mcwOJiIiIiKRRXbuCt7e9PXs23LplahwRERERMYmK6C6oxrAaePl4AXBg5gEu/3rZ5EQiIiIiaYibG0bdusRUrw5ubmanERPlyAHt2tnb16/D4sWmxhERERFJm1xg/q0iugvK5JeJakOqAWDYDNa/td7kRCIiIiJpiIcHvPYat3r1srclXXvllYS2LjAqIiIi8hi4wPxbRXQXVXlAZXzy+AAQujqUvzb/ZW4gEREREZE0qEIFKFfO3t67134TERERkfTF1CL61q1bad68OYGBgVgsFlasWJHkub1798ZisTB+/PhE/VevXqVTp074+PiQNWtWevToQWRkZKJzDh8+zDPPPIO3tzdBQUGMGzfuMbyb1OWRwYPao2o7jkMGh2DYDBMTiYiIiKQRhgFRUfaboflVemexQJ8+CcdTppiXRURERCRNcoH5t6lF9Js3b1K6dGm+/PLL+573v//9j59//pnAwMC77uvUqRNHjx4lJCSE1atXs3XrVnr16uW4PyIiggYNGpAvXz727dvHxx9/zPvvv8/06dNT/P2ktlIvlCJnqZwAnN97nl8W/2JyIhEREZE0IDoay/PPk7VXL4iONjuNOIEOHcDX195euBCuXTM3j4iIiEia4gLzb1OL6I0bN2bUqFG0atUqyXPOnTtHv379mD9/Ph7/b0+c3377jbVr1zJjxgwqVapE9erVmThxIosWLeL8+fMAzJ8/n5iYGGbNmkXx4sVp3749/fv357PPPnus7y01WN2s1P+4vuN449sbiYuOMzGRiIiIiEjakykTdOlib9++DXPnmptHRERERFKXu9kB7sdms/Hiiy8yePBgihcvftf9O3fuJGvWrDz99NOOvnr16mG1Wtm1axetWrVi586d1KhRA09PT8c5DRs2ZOzYsVy7do1s2bLd9bzR0dFE/+tfPSIiIhx5bDZbSr7FR1agXgGerP8kf4b8yfW/rrN70m4qv17Z7FiPhc1mwzAMp/sdiHPQ+JCkaGxIUjQ2JEl3xsSd8ZGKY0Tj0Xn16QNffGFvT5kCr71m3+pFRERERNI+py6ijx07Fnd3d/r373/P+y9evIi/v3+iPnd3d7Jnz87Fixcd5xQoUCDROTlz5nTcd68i+pgxYxgxYsRd/ZcvXyYqKipZ7+VxKvdWOf5c/ycYsHXUVnI3zY1XVi+zY6U4m81GeHg4hmFgteqauJKYxockRWNDkqKxIUmKiiJrdDSxcXFcDQvDmjFjqr30jRs3Uu215OEULQq1a8OmTRAaChs3Qt26ZqcSERERkdTgtEX0ffv2MWHCBPbv348llZd4DB06lIEDBzqOIyIiCAoKws/PDx8fn1TN8iD8/f0p9WIpDn99mOjr0fz+1e+JtnlJK2w2GxaLBT8/PxU75C4aH5IUjQ1JisaGJCkqCrzsCxL8/f1TtYju7e2daq8lD69PH3sRHWDyZBXRRURERNILpy2i//TTT4SFhZE3b15HX3x8PIMGDWL8+PH89ddfBAQEEBYWluhxcXFxXL16lYCAAAACAgK4dOlSonPuHN855//z8vLCy+vuldxWq9VpP2TXGVWHX5f8SlxUHHsm7aFSv0pkzZ/V7FgpzmKxOPXvQcyl8SFJ0diQpGhsyD1ZrRgAJowPjUXn1rIlBATAxYuwciWcOwe5c5udSkREREQeN6edpb/44oscPnyYgwcPOm6BgYEMHjyYH3/8EYAqVapw/fp19u3b53jcxo0bsdlsVKpUyXHO1q1biY2NdZwTEhJCkSJF7rmVi6vyDfKl0gD7e46PiWfjOxtNTiQiIiIikrZ4eEDPnvZ2fDzMmGFuHhERERFJHaYW0SMjIx0FcoCTJ09y8OBBTp8+TY4cOShRokSim4eHBwEBARQpUgSA4OBgGjVqRM+ePdm9ezfbt2+nb9++tG/fnsDAQAA6duyIp6cnPXr04OjRoyxevJgJEyYk2q4lrag+pDoZn7B/3fjIgiOc33ve5EQiIiIiLshqxahWjdgKFUArw+X/6dUL3Nzs7enT4V9rdUREREQkOVxg/m1qqr1791K2bFnKli0LwMCBAylbtizDhw9/4OeYP38+RYsWpW7dujRp0oTq1aszffp0x/2+vr6sW7eOkydPUr58eQYNGsTw4cPp1atXir8fs3n7elNjeA3HccjgEAzDMDGRiIiIiAvy9IS33uJmv372tsi/5MkDzZvb2+fPw6pV5uYRERERcXkuMP82dU/0WrVqPVSR96+//rqrL3v27CxYsOC+jytVqhQ//fTTw8ZzSU+//DS7v9jN1RNX+WvzXxxfc5zCTQubHUtEREREJM3o0wdWrLC3J0+G1q1NjSMiIiIij5lzro+XZHPzdKPumLqO4/VvrscWZzMxkYiIiIhI2lKvHhQqZG9v2AChoebmEREREZHHS0X0NCj4uWDyVMkDwOVfL3Ng9gGTE4mIiIi4kKgoLC1akLVzZ4iKMjuNOCGrFXr3TjieOtW8LCIiIiIuzwXm3yqip0EWi4X6H9d3HG8evpmYmzEmJhIRERERSVu6dgVvb3t79my4dcvUOCIiIiLyGKmInkblrZaXoq2KAhB5MZKdn+40OZGIiIiISNqRIwe0a2dvX78OixebGkdEREREHiMV0dOweh/Vw+pu/xVvH7edyIuRJicSEREREUk7+vRJaE+ebF4OEREREXm8VERPw3IUzkH5l8sDEHszls0jNpsbSEREREQkDalYEcqVs7f37rXfRERERCTtURE9jas5vCaeWTwB2P/Vfq78fsXkRCIiIiIiaYPFkng1+pQp5mURERERkcdHRfQ0LpN/Jqq9VQ0AI95g/VvrTU4kIiIiIpJ2dOgAvr729sKFcO2auXlEREREJOWpiJ4OVHm9ClkCswBw7LtjnNp6yuREIiIiIk7MasV4+mliS5cGq6bL8fHxDBs2jAIFCpAhQwYKFizIyJEjMQzjvo/bvHkz5cqVw8vLi0KFCjFnzpzUCZzKMmWCLl3s7du3Ye5cc/OIiIiIuBwXmH87ZypJUR4ZPag9srbjeN0b6/7zQ4+IiIhIuuXpCcOHc3PQIHs7nRs7dixTpkxh0qRJ/Pbbb4wdO5Zx48YxceLEJB9z8uRJmjZtSu3atTl48CADBgzgpZde4scff0zF5Kmnd++E9pQpoKm2iIiIyENwgfm3iujpROkupfEv6Q/A+T3nObrkqMmJRERERMQV7Nixg2effZamTZuSP39+2rRpQ4MGDdi9e3eSj5k6dSoFChTg008/JTg4mL59+9KmTRs+//zzVEyeeoKDofY/a1ZCQ2HjRnPziIiIiEjKUhE9nbC6Wak/rr7jeMPQDcRFx5mYSERERERcQdWqVdmwYQOhoaEAHDp0iG3bttG4ceMkH7Nz507q1auXqK9hw4bs3LnzsWY1ky4wKiIiIpJ2uZsdQFJPwYYFKVC3ACc3nOT6yevsnbKXygMqmx1LRERExLlERWHp1Ims0dGwZAlkzGh2IlMNGTKEiIgIihYtipubG/Hx8YwePZpOnTol+ZiLFy+SM2fORH05c+YkIiKC27dvkyFDhkT3RUdHEx0d7TiOiIgAwGazYbPZUvDdPD4tWkBAgIWLFy2sWGFw5oxB7txmp3o0NpsNwzBc5ncgqUvjQ5KisSFJ0diQJEVFwYsv4hsdjW3RolSdfz/oeFQRPR2xWCzU/7g+08tPBwO2jtxKma5l8M7qbXY0EREREecSHQ0xMWancApLlixh/vz5LFiwgOLFizv2OA8MDKTLnStqPqIxY8YwYsSIu/ovX75MVFRUirxGaujQITOff56Z+HgLX3wRyaBBN82O9EhsNhvh4eEYhoHVSS/yJebR+JCkaGxIUjQ2JElRUWSNiCAuLo5rYWFYU7GIfuPGjQc6T0X0dCZX2VyUeqEUh+cd5vbV2/z04U+JtnkREREREfm3wYMHM2TIENq3bw9AyZIlOXXqFGPGjEmyiB4QEMClS5cS9V26dAkfH5+7VqEDDB06lIEDBzqOIyIiCAoKws/PDx8fnxR8N4/XgAHwxRcG8fEWFizIzKhRmfDwMDtV8tlsNiwWC35+fip2yF00PiQpGhuSFI0NSVJUFHh5AeDv75+qRXRv7wdbXKwiejpUZ1Qdji45Snx0PLu+2EWFVyuQNV9Ws2OJiIiIiBO6devWXR903dzc7vvV1ypVqrBmzZpEfSEhIVSpUuWe53t5eeH1zwenf7NarS71ITtvXmjeHFasgPPnLXz/vYXWrc1O9WgsFovL/R4k9Wh8SFI0NiQpGhtyT1YrBoAJ4+NBX0sjNh3yzetLpdcqARAfHc+mdzeZnEhEREREnFXz5s0ZPXo033//PX/99Rf/+9//+Oyzz2jVqpXjnKFDh9K5c2fHce/evfnzzz958803+f3335k8eTJLlizh9ddfN+MtpKp/X2B08mTzcoiIiIhIylERPZ16ZugzZMhh/yrt4W8Oc2H/BZMTiYiIiIgzmjhxIm3atOGVV14hODiYN954g5dffpmRI0c6zrlw4QKnT592HBcoUIDvv/+ekJAQSpcuzaeffsqMGTNo2LChGW8hVdWrB4UK2dsbNkBoqLl5REREROTRqYieTnln9abGsBqO45DBIRiGYWIiEREREXFGWbJkYfz48Zw6dYrbt2/zxx9/MGrUKDw9PR3nzJkzh82bNyd6XK1atThw4ADR0dH88ccfdO3aNXWDm8Rqhd69E46nTjUvi4iIiIikDBXR07EKfSqQ7clsAJzceJITa0+YnEhERETECVitUKIEcUWK2NsiD6lrV7hzjarZs+HWLVPjiIiIiDg3F5h/O2cqSRVunm7UHVPXcbz+zfXY4pO+QJSIiIhIuuDpifHhh0S+8w78a7W1yIPKkQPatbO3r1+HxYtNjSMiIiLi3Fxg/q0iejpXrG0xclfMDUDYL2EcnHPQ3EAiIiIiImmALjAqIiIiknaoiJ7OWSwW6n9S33G8efhmYm7GmJhIRERERMT1VawI5crZ23v32m8iIiIi4ppURBfyPZOPIs8WAeDG+Rv8/PnPJicSERERMVFUFJYXXsD3lVcgKsrsNOKiLJbEq9GnTDEvi4iIiIhTc4H5t4roAkC9j+phcbMAsH3sdiIvRZqcSERERMREERFYIjUfkkfToQP4+trbCxfCtWvm5hERERFxWk4+/1YRXQB4ougTlOtp/75pTGQMWz7YYnIiERERERHXlikTdOlib9++DXPnmptHRERERJJHRXRxqPV+LTwz26+Au2/aPq4cu2JyIhERERER19a7d0J7yhQwDPOyiIiIiEjyqIguDplzZqbqm1UBMOINNgzZYHIiERERERHXFhwMtWvb26GhsHGjuXlERERE5OGpiC6JVBlYhcy5MgPw+4rfOb3ttMmJRERERERcmy4wKiIiIuLaVESXRDwzeVL7g9qO45DBIRj6zqmIiIiISLK1bAkBAfb2ihVw7pyZaURERETkYamILncp060MfsX9ADj781l+XfaryYlEREREUpHVCk89RXyBAva2yCPy8ICePe3t+HiYMcPcPCIiIiJOxQXm386ZSkxldbNSf1x9x/GGoRuIj4k3MZGIiIhIKvL0xPj0U26MGAGenmankTSiZ8+Ez4TTp0NsrLl5RERERJyGC8y/VUSXeyrUuBD5a+cH4Nof19g7da+5gUREREREXFhQELRoYW+fPw+rVpmbR0REREQenIrock8Wi4X6HyesRt/ywRairkeZmEhERERExLXpAqMiIiIirklFdElSYPlASnYqCcDtv2+z7aNtJicSERERSQXR0VheegmfgQMhOtrsNJKG1KsHhQrZ2+vXQ2iouXlEREREnIILzL9VRJf7qjOqDm6ebgD8PP5nwk+Hm5xIRERE5DEzDAgLw3rlir0tkkKsVujdO+F46lTzsoiIiIg4DReYf6uILveVNX9WKvavCEB8dDybhm0yOZGIiIiIiOvq2hW8ve3t2bPh1i1T44iIiIjIA1ARXf7TM28/g3c2+0z/0LxDXDx40eREIiIiIiKuKUcOaNfO3r5+HRYvNjWOiIiIiDwAFdHlP2XIloEa79awHxgQ8maIuYFERERERFzYvy8wOnmyeTlERERE5MGoiC4PpMKrFchaICsAf4b8yYkfT5gbSERERETERVWsCOXK2dt799pvIiIiIuK8VESXB+Lu5U7dD+s6jkMGh2CLt5mYSERERETENVksiVejT5liXhYRERER+W8qossDK/58cQIrBAIQdiSMQ18fMjmRiIiIyGNgsUBQELbAQHtb5DHo0AF8fe3thQvh2jVz84iIiIiYxgXm3yqiywOzWC3U/7i+43jTsE3E3oo1MZGIiIjIY+DlhfHll0R89BF4eZmdRtKoTJmgSxd7+/ZtmDvX3DwiIiIipnGB+beK6PJQ8tfMT+HmhQG4ce4GP4//2eREIiIiIiKuqXfvhPaUKWAY5mURERERkaSpiC4Prd7Yelis9q9WbPtoGzfDbpqcSERERETE9QQHQ+3a9nZoKGzcaG4eEREREbk3FdHlofkF+1H2pbIAxNyIYcvILSYnEhEREUlB0dFYXn0VnyFDIDra7DSSxukCoyIiIpLuucD8W0V0SZbaI2rjkckDgH1T9/F36N8mJxIRERFJIYYBZ85gPX9e+2vIY9eyJQQE2NsrVsC5c2amERERETGBC8y/VUSXZMkckJmqg6sCYIuzsWHoBpMTiYiIiIi4Hg8P6NnT3o6PhxkzzM0jIiIiIndTEV2SreqgqmQOyAzAb8t/48yOMyYnEhERERFxPT17gvWfT2bTp0NsrLl5RERERCQxFdEl2Twze1JrRC3H8bo31mE46VcuREREREScVVAQtGhhb58/D6tWmZtHRERERBJTEV0eSdnuZXki+AkAzu48y2/LfzM5kYiIiIiI69EFRkVEREScl4ro8kis7lbqja3nON4wZAPxMfEmJhIRERERcT316kGhQvb2+vUQGmpuHhERERFJoCK6PLLCzQqTr2Y+AK6euMq+6ftMTiQiIiLyCCwW8PfH9sQT9rZIKrBaoXfvhOOpU83LIiIiIpKqXGD+rSK6PDKLxUKDTxo4jreM2EJUeJSJiUREREQegZcXxowZRHz2GXh5mZ1G0pGuXROG3OzZcOuWqXFEREREUocLzL9NLaJv3bqV5s2bExgYiMViYcWKFY77YmNjeeuttyhZsiSZMmUiMDCQzp07c/78+UTPcfXqVTp16oSPjw9Zs2alR48eREZGJjrn8OHDPPPMM3h7exMUFMS4ceNS4+2lK4FPB1KiQwkAbl25xfax201OJCIiIiLiWnLkgPbt7e3r12HxYlPjiIiIiMg/TC2i37x5k9KlS/Pll1/edd+tW7fYv38/w4YNY//+/Sxfvpxjx47R4s5l6//RqVMnjh49SkhICKtXr2br1q306tXLcX9ERAQNGjQgX7587Nu3j48//pj333+f6dOnP/b3l97UGV0HN083AH7+/GcizkaYnEhERERExLXoAqMiIiIizsfdzBdv3LgxjRs3vud9vr6+hISEJOqbNGkSFStW5PTp0+TNm5fffvuNtWvXsmfPHp5++mkAJk6cSJMmTfjkk08IDAxk/vz5xMTEMGvWLDw9PSlevDgHDx7ks88+S1Rsl0eXrUA2KvStwM+f/UxcVBybhm3i2dnPmh1LRERE5OHExGB56y2y3LoFEyaAt7fZiSQdqVgRypWD/fthzx7Yuxf++agjIiIikja5wPzbpfZEDw8Px2KxkDVrVgB27txJ1qxZHQV0gHr16mG1Wtm1a5fjnBo1auDp6ek4p2HDhhw7doxr166lav70oMY7NfDOah/oB+ce5OKhiyYnEhEREXlINhscP47byZP2tkgqsli0Gl1ERETSGReYf5u6Ev1hREVF8dZbb9GhQwd8fHwAuHjxIv7+/onOc3d3J3v27Fy8eNFxToECBRKdkzNnTsd92bJlu+u1oqOjiY6OdhxHRNi3JbHZbNic9BfpLLyyelH97eqsf3M9GLD+zfV0/KFjijy3zWbDMAz9DuSeND4kKRobkhSNDUnSnTFxZ3yk4hjReBSADh3gjTcgPBwWLoRPPoF7fGwRERERkVTiEkX02NhYnn/+eQzDYEoqLMUYM2YMI0aMuKv/8uXLREVFPfbXd3X52uYj8xeZiTwbyR/r/mD/0v3kqZnnkZ/XZrMRHh6OYRhYrS71JQpJBRofkhSNDUmKxoYkKSqKrNHRxMbFcTUsDGvGjKn20jdu3Ei11xLnlSkTdOkCX3wBt2/D3LkwYIDZqURERETSL6cvot8poJ86dYqNGzc6VqEDBAQEEBYWluj8uLg4rl69SkBAgOOcS5cuJTrnzvGdc/6/oUOHMnDgQMdxREQEQUFB+Pn5JXp9SVq9MfVY8eIKAPaO2UuZ1mWwuj1agcJms2GxWPDz81OxQ+6i8SFJ0diQpGhsSJKiosDLCwB/f/9ULaJ7O+H+j2KO3r3tRXSwb+ny2mv2rV5EREREJPU5dRH9TgH9+PHjbNq0iRw5ciS6v0qVKly/fp19+/ZRvnx5ADZu3IjNZqNSpUqOc9555x1iY2Px8PAAICQkhCJFitxzKxcALy8vvP754PRvVqtVH7IfUKmOpdj1+S4u7L/ApUOXOLrwKKU7l37k57VYLPo9SJI0PiQpGhuSFI0NuSerFQPAhPGhsSh3BAdD7dqwaROEhsLGjVC3rtmpRERERNInU2fpkZGRHDx4kIMHDwJw8uRJDh48yOnTp4mNjaVNmzbs3buX+fPnEx8fz8WLF7l48SIxMTEABAcH06hRI3r27Mnu3bvZvn07ffv2pX379gQGBgLQsWNHPD096dGjB0ePHmXx4sVMmDAh0UpzSXkWq4X6n9R3HG98dyOxt2NNTCQiIiIi4lp0gVERERER52BqEX3v3r2ULVuWsmXLAjBw4EDKli3L8OHDOXfuHN999x1nz56lTJky5MqVy3HbsWOH4znmz59P0aJFqVu3Lk2aNKF69epMnz7dcb+vry/r1q3j5MmTlC9fnkGDBjF8+HB69eqV6u83vSlQuwBPNX0KgIgzEeyasMvkRCIiIiIPyMcHI3Nms1NIOteyJdzZgXLFCjh3zsw0IiIiIo+Rk8+/Td3OpVatWhiGkeT997vvjuzZs7NgwYL7nlOqVCl++umnh84nj67e2Hqc+OEEhs1g25htlO1Rlkx+mcyOJSIiIpI0b2+Mb74hPCwMf+1RLiby8ICePWHkSIiPhxkz4L33zE4lIiIiksJcYP6tTRflsfIv7k+Z7mUAiI6IZuuoreYGEhERERFxIT17wp2t8qdPh1jtkCgiIiKS6lREl8eu9ge18chov6jr3sl7uXriqsmJRERERERcQ1AQtGhhb58/D6tWmZtHREREJD1SEV0euyy5slDljSoA2OJsbBi6weREIiIiIvcRE4Pl7bfJPHo0/HNBexEz6QKjIiIikqa5wPxbRXRJFVXfqEomf/te6L8u+5WzP581OZGIiIhIEmw2+OUX3I8ds7dFTFavHhQqZG+vXw+hoebmEREREUlRLjD/VhFdUoVXFi9qjajlOF73xroHunCsiIiIiEh6Z7VC794Jx1OnmpdFREREJD1SEV1STbmXypGjSA4Azmw/w+8rfjc5kYiIiIiIa+jaFby87O3Zs+HWLVPjiIiIiKQrKqJLqrG6W6k3tp7jeMOQDcTHxpuYSERERETENeTIAe3b29vXr8PixabGEREREUlXVESXVFWkRRHyPpMXgL9D/2b/V/tNTiQiIiIi4hp0gVERERERc6iILqnKYrHQ4JMGjuPN728mOiLaxEQiIiIiIq6hYkUoV87e3rMH9u41N4+IiIhIeqEiuqS63BVzU/z54gDcunyL7eO2m5xIRERE5P/x8gJPT7NTiCRisWg1uoiIiKRRTj7/VhFdTFF3TF2sHvbht/OznUScizA5kYiIiMg/vL0xli7l+owZ4O1tdhqRRDp0AF9fe3vhQrh2zdw8IiIiIo/MBebfKqKLKbI9mY0Kr1YAIO52HJuGbzI5kYiIiIiI88uUCbp0sbdv34a5c83NIyIiIpIeqIgupqnxbg28fL0AODj7IJeOXDI5kYiIiIiI8+vdO6E9ZQoYhnlZRERERNIDFdHFNBlzZOSZt5+xHxiw/q315gYSERERAYiJgQ8+INOnn9rbIk4mOBhq17a3Q0Nh40Zz84iIiIg8EheYf6uILqaq1L8Svnntmzqe+OEEf2740+REIiIiku7ZbFj27sXj0CGw2cxOI3JPusCoiIiIpBkuMP9WEV1M5e7tTp3RdRzHIYNDMGz6PqqIiIiIyP20bAkBAfb2ihVw7pyZaURERETSNhXRxXQlO5YkoKz9E8DFAxc5suCIyYlERERERJybhwf07Glvx8fDjBnm5hERERFJy1REF9NZrBbqf1zfcbzxnY3ERcWZmEhERERExPn17AnWfz7RTZ8OsbHm5hERERFJq1REF6fwZN0nKdS4EADhp8PZ9cUukxOJiIiIiDi3oCBo0cLePn8eVq0yN4+IiIhIWqUiujiNemPrYbFaAPjpw5+49fctkxOJiIiISP78+bFYLHfdXn311XueP2fOnLvO9fb2TuXU6YcuMCoiIiLy+KmILk4jZ8mclO5aGoDo8Gi2jtpqciIRERER2bNnDxcuXHDcQkJCAGjbtm2Sj/Hx8Un0mFOnTqVW3HSnXj0oZP9CJ+vXQ2iouXlERERE0iIV0cWp1P6gNu4Z3AHY8+Uerv5x1eREIiIiku54e2N89x3Xv/4atIIaPz8/AgICHLfVq1dTsGBBatasmeRjLBZLosfkzJkzFROnL1Yr9O6dcDx1qnlZRERERJLFBebfKqKLU/HJ7UOVgVUAsMXa2Pj2RpMTiYiIiMgdMTExfPPNN3Tv3h2LxZLkeZGRkeTLl4+goCCeffZZjh49moop05+uXcHLy96ePRtuaVdEERERkRTlbnYAkf+v2pvV2Dd9H7cu3+LokqNUHliZPJXymB1LREREJN1bsWIF169fp2vXrkmeU6RIEWbNmkWpUqUIDw/nk08+oWrVqhw9epQ8ee49p4uOjiY6OtpxHBERAYDNZsNms6Xoe0iLsmWDdu0sfP21hevXYeFCG926Pfrz2mw2DMPQ70DuSeNDkqKxIUnR2JD7MWt8POjrqYguTsfLx4ta79dizatrAAgZHELXLV3NDSUiIiLpR0wMfPopmSIjYdgwp/1KqRlmzpxJ48aNCQwMTPKcKlWqUKVKFcdx1apVCQ4OZtq0aYwcOfKejxkzZgwjRoy4q//y5ctERUU9evB0oF07D77+OgcAkybF0bTpo2+LaLPZCA8PxzAMrFZ9iVkS0/iQpGhsSFI0NiRJMTFknDoVt6gowvr3x5qK8+8bN2480HkqootTKtezHLsm7OLv0L85/dNpjn13jMLNC5sdS0RERNIDmw3L9u14xMSAVko5nDp1ivXr17N8+fKHepyHhwdly5blxIkTSZ4zdOhQBg4c6DiOiIggKCgIPz8/fHx8kp05PWnYEMqVM9i/38LBg56cPu3P008/2nPabDYsFgt+fn4qdshdND4kKRobkhSNDUlSVBQcPoxndDTuTzyBNWPGVHtp7wcs2KuILk7JzcONuh/VZUnrJQCsf2s9hRoXMjmViIiISPo1e/Zs/P39adq06UM9Lj4+niNHjtCkSZMkz/Hy8sLrzqbe/2K1WvUh+yH06QM9e9rb06ZZqVjx0Z/TYrHo9yBJ0viQpGhsSFI0NuSerFYMABPGx4O+lkasOK2iLYsSVC0IgL+P/c2BGQdMTiQiIiKSPtlsNmbPnk2XLl1wd0+8Dqdz584MHTrUcfzBBx+wbt06/vzzT/bv388LL7zAqVOneOmll1I7drrToQP4+trbCxfCtWvm5hERERFJK1REF6dlsVho8EkDx/GWEVuIiYwxMZGIiIhI+rR+/XpOnz5N9+7d77rv9OnTXLhwwXF87do1evbsSXBwME2aNCEiIoIdO3ZQrFix1IycLmXKBF262Nu3b8PcuebmEREREUkrVEQXp5anch6KtbF/4LoZdpNDkw+ZnEhEREQk/WnQoAGGYVC48N3XqNm8eTNz5sxxHH/++eecOnWK6OhoLl68yPfff0/ZsmVTMW361rt3QnvKFDAM87KIiIiIpBUqoovTqzumLlYP+1A9PPUwN84/2FVzRURERETSm+BgqF3b3g4NhY0bzc0jIiIikhaoiC5OL3uh7Dzd52kA4m7Hsf6t9RhaUiMiIiIick99+iS0p0wxL4eIiIhIWqEiuriEmsNq4uXjBcAvC37h589/NjmRiIiIpFleXhhLlnB9+nTw8jI7jchDa9kSAgLs7RUr4Nw5M9OIiIiI/AcXmH+riC4uIeMTGWn8ZWPH8bo31vH7yt9NTCQiIiJplsUC3t72m8VidhqRh+bhAT172tvx8TBjhrl5RERERO7LBebfKqKLyyjZsSTlBpazHxiwvONyzu87b24oEREREREn1LMnWP/5tDd9OsTGmptHRERExJWpiC4u5ek3nqZ4++IAxN6KZWHzhUScjTA5lYiIiKQpsbEwYQIZVXkUFxYUBC1a2Nvnz8OqVebmEREREUmSC8y/VUQXl2KxWGgxswVBVYMAiLwQyYJmC4iJjDE5mYiIiKQZ8fFYNmzAc9s2+14YIi5KFxgVERERl+AC828V0cXluHu7025FO7IWyArApUOX+LbDt9jibeYGExERERFxIvXqQcGC9vb69RAaam4eEREREVelIrq4pEx+mej4fUe8fO1X7A1dHcq6N9aZnEpERERExHlYrYlXo0+dal4WEREREVemIrq4LL9gP57/9nms7vZhvGv8LvZM3mNyKhERERER59G1K3jZ150wZw7cumVmGhERERHXpCK6uLQn6z5J0ylNHcc/9PuBE2tPmJhIRERERMR55MgB7dvb29euweLF5uYRERERcUUqoovLK/dSOaq+WRUAw2aw9PmlXDpyyeRUIiIiIiLOQRcYFREREXk0KqJLmlBvTD2CWwcDEHMjhoXNFhJ5MdLkVCIiIiIi5qtYEcqWtbf37IG9e83NIyIiIuJqVESXNMFitdBqXisCnw4EIPx0OIueXUTsrViTk4mIiIjL8fLCmDeP8EmTEjaTFnFhFgu88krCsVaji4iIiFNxgfm3iuiSZnhk9KD9d+3xCfIB4Nzuc/yv8/8wbIbJyURERMSlWCzg64vh42Nvi6QBHTqAr6+9vXChfX90EREREafgAvNvFdElTcmSKwsdV3fEM7MnAL99+xsb3tlgcioREREREXNlygRdutjbt2/D3Lnm5hERERFxJSqiS5qTs1RO2ixug8Vq/5er7R9t58CsAyanEhEREZcRGwtTppBh7lx7WySN6N07oT1lChj6wqaIiIg4AxeYf6uILmnSU02eotGERo7j1S+v5uSmkyYmEhEREZcRH4/lhx/w2rAB4uPNTiOSYoKDoXZtezs0FDZuNDePiIiICOAS828V0SXNqti3IhX7VQTAFmdjSeslXDl2xeRUIiIiIiLm6dMnoa0LjIqIiIg8GBXRJU1r+FlDnmryFABR16NY0HQBt67cMjmViIiIiIg5WraEgAB7e8UKOHfOzDQiIiIirkFFdEnTrO5Wnlv0HDlL5QTg2h/XWNRyEXHRcSYnExERERFJfR4e0LOnvR0fDzNmmJtHRERExBWoiC5pnlcWLzqs7kDmgMwAnNl+hu96fIehKymJiIiISDrUsydY//kkOH26016/S0RERMRpqIgu6YJvkC8dVnXAPYM7AEfmH2HryK0mpxIRERERSX1BQdCihb19/jysWmVuHhERERFnpyK6pBuBTwfSen5rsNiPN7+3mSMLjpgbSkRERETEBLrAqIiIiMiDUxFd0pXgVsHUG1vPcbyy20pObz9tYiIRERFxOl5eGDNmEPHZZ+DlZXYaSWUXblwwO0KqqFcPCha0t9evh9BQc/OIiIhIOuYC829Ti+hbt26lefPmBAYGYrFYWLFiRaL7DcNg+PDh5MqViwwZMlCvXj2OHz+e6JyrV6/SqVMnfHx8yJo1Kz169CAyMjLROYcPH+aZZ57B29uboKAgxo0b97jfmjixqm9UpVzPcgDEx8SzuOVirv5x1eRUIiIi4jQsFvD3x/bEE/a2pBt/XvuTwpMK0/O7nkRER5gd57GyWhOvRp861bwsIiIiks65wPzb1CL6zZs3KV26NF9++eU97x83bhxffPEFU6dOZdeuXWTKlImGDRsSFRXlOKdTp04cPXqUkJAQVq9ezdatW+nVq5fj/oiICBo0aEC+fPnYt28fH3/8Me+//z7Tp09/7O9PnJPFYqHJl00oULcAALeu3GJhs4Xcvnbb5GQiIiIiYhabYaP7yu5ExkQy48AMSkwuQcgfIWbHeqy6dk1Y7DVnDty6ZWYaEREREeeVrCL67NmzuZUCM6zGjRszatQoWrVqddd9hmEwfvx43n33XZ599llKlSrF119/zfnz5x0r1n/77TfWrl3LjBkzqFSpEtWrV2fixIksWrSI8+fPAzB//nxiYmKYNWsWxYsXp3379vTv35/PPvvskfOL63LzcOP5Zc/zRNEnALjy+xWWtllKfGy8yclERETEdHFxMGsWGRYutLclXbBgoWPJjmT2zAzAmYgzNPimAb1X9+ZG9A2T0z0eOXJA+/b29rVrsHixuXlEREQknXKB+bd7ch40ZMgQXnvtNdq2bUuPHj2oWrVqSufi5MmTXLx4kXr1Evav9vX1pVKlSuzcuZP27duzc+dOsmbNytNPP+04p169elitVnbt2kWrVq3YuXMnNWrUwNPT03FOw4YNGTt2LNeuXSNbtmx3vXZ0dDTR0dGO44gI+1c5bTYbNpstxd+rPBibzYZhGCn2O/D08aT9qvbMqjKLW1ducXLjSVb3Xk2z6c2wOOlXRyRpKT0+JO3Q2JCkaGxIkmJiYMUKvKKjsfXsCe7JmjIni8ajeSwWC73K96JBwQb0+K4HG09uBGDavmmsPbGWWc/Ook6BOianTHl9+sDcufb2lCnQrZu5eURERCQdiovDsmIFXjEx8PLL8K86rrNI1ieCc+fOsWrVKubMmUOtWrV48skn6datG126dCEgICBFgl28eBGAnDlzJurPmTOn476LFy/i7++f6H53d3eyZ8+e6JwCBQrc9Rx37rtXEX3MmDGMGDHirv7Lly8n2kpGUpfNZiM8PBzDMLBaU2gnosxQf1Z9VrddTXx0PAdnHcQr0Isyr5ZJmeeXVPNYxoekCRobkhSNDUlSVBRZo6OJjYvjalgY1owZU+2lb9xImyueXUn+rPkJeTGEqXun8mbIm9yMvcmp8FPU/bourzz9CmPrj3WsVk8LKlaEsmXhwAHYswf27oV/rVESEREREZJZRHd3d6dVq1a0atWKS5cu8c033zB37lyGDRtGo0aN6NGjB82bN3fZD6RDhw5l4MCBjuOIiAiCgoLw8/PDx8fHxGTpm81mw2Kx4Ofnl6Jjy7+pP9aZVv73wv8A2DVqF0GlgwhuHZxiryGP3+MaH+L6NDYkKRobkqSoKMdG0f7+/qlaRPf29k6115KkWS1WXqnwCo0KNaL7yu5sObUFgMl7J/PDiR+Y/exsauavaXLKlGGxwCuvQM+e9uMpU2DmTHMziYiIiDibR/5uas6cOalevTqhoaGEhoZy5MgRunTpQrZs2Zg9eza1atVK1vPeWdF+6dIlcuXK5ei/dOkSZcqUcZwTFhaW6HFxcXFcvXrV8fiAgAAuXbqU6Jw7x0mtmvfy8sLrzhV2/sVqtepDtsksFstj+T2U6lSKa39eY/PwzQCs6LyCrPmykrtC7hR9HXm8Htf4ENensSFJ0diQe7JaMQBMGB8ai87lyWxPsrHLRibvmcxb69/iVuwtTl4/Sa25tehXsR9j6o4hk2cms2M+sg4d4I03IDwcFi6ETz6Be3xhV0RERCTdSvYs/dKlS3zyyScUL16cWrVqERERwerVqzl58iTnzp3j+eefp0uXLskOVqBAAQICAtiwYYOjLyIigl27dlGlShUAqlSpwvXr19m3b5/jnI0bN2Kz2ahUqZLjnK1btxIbG+s4JyQkhCJFitxzKxdJv2q8W4NSL5QCIO52HItaLCL8dLjJqURERETETFaLlb4V+3Ko9yGq563u6J+4eyKlp5bmp1M/mZguZWTKBHc+ut2+nbBHuoiIiIjYJauI3rx5c4KCgpgzZw49e/bk3LlzLFy40HER0EyZMjFo0CDOnDlz3+eJjIzk4MGDHDx4ELBfTPTgwYOcPn0ai8XCgAEDGDVqFN999x1Hjhyhc+fOBAYG0rJlSwCCg4Np1KgRPXv2ZPfu3Wzfvp2+ffvSvn17AgMDAejYsSOenp706NGDo0ePsnjxYiZMmJBouxYRsK9GbD6jOXmr5wUg8mIkC5otIDoi+j8eKSIiIiJpXaHshdjSdQufN/ycDO4ZAPjj2h/UnFOT19e+zq3YWyYnfDS9eye0p0wBwzAvi4iIiIizSVYR3d/fny1btvDLL78wYMAAsmfPftc5fn5+nDx58r7Ps3fvXsqWLUvZsmUBGDhwIGXLlmX48OEAvPnmm/Tr149evXpRoUIFIiMjWbt2baK9IufPn0/RokWpW7cuTZo0oXr16kyfPt1xv6+vL+vWrePkyZOUL1+eQYMGMXz4cHr16pWcty5pnLuXO+3+145sBe3fUgg7Esay9suwxdlMTiYiIiIiZrNarAyoPICDvQ9SNagqAAYG43eNp8zUMuw4s8PkhMkXHAx3duIMDYWNG02NIyIiIuJULIbx8GsMvv76a9q1a3fXvuExMTEsWrSIzp07p1hAZxAREYGvry/h4eG6sKiJbDYbYWFh9gt8Peb9Qq8cu8LMKjOJuhYFQIW+FWgyscljfU15NKk5PsS1aGxIUjQ2JEmGge3UKa5cucITZctidXNLtZfWvNPOFX4O8bZ4JuyawDsb3yEqzj5ntGBhYJWBjKw9kgweGUxO+PCWLoXnn7e3n3sOlizR30lJmv4/KknR2JCkaGxIklxg/p2sEdutWzfCw+/eK/rGjRt069YtOU8p4lSeKPIE7Za3w+pu/09kz6Q97Jq4y+RUIiIikiosFsibF1uePPa2yD24Wd0YWGUgB14+QKXc9usxGRh8uvNTyk4ry89nfzY54cNr2RICAuztFSvg/Hkz04iIiEi64QLz72QV0Q3DwHKPN3T27Fl8fX0fOZSIM8hfKz/NpjdzHP844EdCvw81MZGIiIiIOJuiTxRle/ftjK03Fi83+zd1j/19jGqzqvFWyFuOVequwMMDeva0t+PjYcYMc/OIiIiIOIuHKqKXLVuWcuXKYbFYqFu3LuXKlXPcSpcuzTPPPOO4uKhIWlC2W1mqD60OgGEz+Lb9t1w8dNHkVCIiIvJYxcXBggV4L19ub4v8BzerG29We5P9L++nQmAFAGyGjXE7xlFuWjn2nNtjcsIH17Mn3PmG/VdfWYiNNTePiIiIpAMuMP9+qCJ6y5YtefbZZzEMg4YNG/Lss886bu3bt2fatGl88803jyuriCnqjKpDsTbFAIiJjGFhs4XcOH/D5FQiIiLy2MTFYVm0CO8VK5x2Ei/OqZhfMXb02MGYumPwdPME4Lcrv1F5ZmXe3vA20XHRJif8b0FB0KKFvX3+vIV167zu/wARERGRR+UC82/3hzn5vffeAyB//vy0a9cOb2/vxxJKxJlYrBZaft2S8NPhnNt9joizESxssZCuW7rimcnT7HgiIiIi4kTcre4MqT6EZoWb0XVFV/Zd2IfNsDFm2xhWha5izrNzKB9Y3uyY99Wnj31PdICvv86ILnslIiIi6V2y9kTv0qWLCuiSrnhk8KD9d+3xzWff8//Cvgv874X/YdgMk5OJiIiIiDMq4V+CnT12MrL2SDysHgD8EvYLlWZUYtjGYcTEx5icMGn16kHBgvb21q1ehOqyQCIiIpLOPXARPXv27Fy5cgWAbNmykT179iRvImlR5pyZ6bi6I55Z7KvPf1/xO+uHrDc5lYiIiIg4Kw83D96t8S57e+2lTEAZAOKNeEb9NIoKX1XgwIUD5gZMgtVqX41+x7vvWggPNy+PiIiIiNkeeDuXzz//nCxZsjjaFovlsYUScVb+Jfxpu7QtC5ouwIg32PHxDrI/lZ3yPZ37K7kiIiIiYp5SOUux+6XdjNk2hpFbRxJni+PwpcNUnFGRd595l7efeRsPNw+zYybStSu8845BdLSFb7+1sGkTDBkCfftChgxmpxMRERFJXQ9cRO/SpYuj3bVr18eRRcQlFGpYiMYTG7PmlTUAfN/ne7IVyMaT9Z40OZmIiIiIOCsPNw+G1xxOiyIt6LKiC4cvHSbOFsf7W95nxbEVzHl2DqUDSpsd0yFHDpgwwaBfP4iNtXD1Krz5JkyYAMOHQ7du4OFcdX8RERGRxyZZe6LPmTPnnv1xcXEMHTr0UfKIuIQKfSpQaUAlAIx4gyVtlnD518smpxIRERERZ1cmoAx7eu5hWI1huFncADh48SAVvqrAyC0jiY2PNTlhgp49Ydu2K7zwgsGdLyKfOwcvvwzFisGiRWCzmZtRREREJDUkq4jev39/2rZty7Vr1xx9x44do1KlSixcuDDFwok4swafNKBw88IARIdHs6DZAm6G3TQ5lYiIiDwyT0+MTz/lxvvvg6en2WkkDfJ08+SD2h+w66VdlPAvAUCsLZbhm4dTeWZlfgn7xeSECfLmjWfuXIPDh+HZZxP6T5yADh2gXDlYswYMw7yMIiIi4uJcYP6drCL6gQMHOHv2LCVLliQkJIQvv/yScuXKUbRoUQ4dOpTSGUWcktXNynMLniOgTAAA109eZ1HLRcRFxZmcTERERB6J1QpPPUX8k0/a2y7szJkznD171nG8e/duBgwYwPTp001MJXeUDyzP3p57ebv621gt9rG2/8J+yk0rx5ifxhBnc555ZYkSsGIF7NwJtWsn9B86BE2bQo0asG2bafFERETElbnA/DtZqQoWLMj27dtp3bo1jRo14vXXX2fGjBnMnz8fX1/flM4o4rQ8M3vSYVUHsgTaL7p7dudZVnZbiaGlOCIiIuIEOnbsyKZNmwC4ePEi9evXZ/fu3bzzzjt88MEHJqcTAC93L0bXHc3PPX6mmF8xwL4q/e2Nb1N1ZlV+vfyryQkTq1wZNmyAdeugfPmE/m3b4Jln7AX1gwdNiyciIiLyWCS7tP/999+zaNEiqlSpQtasWZk5cybnz59PyWwiLsEnjw8dVnXAI6P9ykq/LPqFze9tNjeUiIiIJF9cHCxfjtf339vbLuyXX36hYsWKACxZsoQSJUqwY8cO5s+fn+R1jsQcFXJXYF+vfbxV7S3HqvQ95/dQdlpZxm4b61Sr0i0WqF8f9uyBZcugaNGE+9asgbJl7Vu9HD9uXkYRERFxIS4w/05WEf3ll1+mbdu2vPXWW/z0008cPnwYT09PSpYsyZIlS1I6o4jTy1UuF60XtIZ/Lri0deRWDn2trY1ERERcUlwcljlzyLB4sdNO4h9UbGwsXl5eAKxfv54WLVoAULRoUS5cuGBmNLkHb3dvPqr3ETu676DoE/bKdEx8DEM2DKH6rOr8fuV3kxMmZrHAc8/BkSMwcyYEBSXct2gRBAfbL0J67px5GUVERMQFuMD8O1lF9O3bt7Nr1y4GDRqExWIhICCANWvW8MEHH9C9e/eUzijiEoo+W5QGnzZwHH/30nec2nrKxEQiIiKS3hUvXpypU6fy008/ERISQqNGjQA4f/48OXLkMDmdJKVSnkrs77WfN6q8geWfVRq7zu2izNQyfLrjU+Jt8SYnTMzdHbp3h9BQ+PxzeOIJe398PEyfDoUKweDB8Pff5uYUERERSa5kFdH37dtH6dKl7+p/9dVX2bdv3yOHEnFVlQdUpnxv++aQtlgbi1st5uqJqyanEhERkfRq7NixTJs2jVq1atGhQwfHHP67775zbPMizimDRwY+bvAx27pv46nsTwEQHR/NGyFvUGNODUL/DjU54d28vWHAAPjzTxgxArLYLxtEVBR88gk8+SSMGgWRkabGFBEREXloySqie3l58ccff/Duu+/SoUMHwsLCAPjhhx+Ic9Il9yKpwWKx0PiLxhRsUBCA21dvs6DpAm5fvW1yMhEREUmPatWqxZUrV7hy5QqzZs1y9Pfq1YupU6eamEweVNWgqhzsfZDXK7/uWJW+48wOSk8tzec7P3e6VelgL54PH24vpg8aBP/sKEREBAwbBgULwhdfQHS0uTlFREREHlSyiuhbtmyhZMmS7Nq1i+XLlxP5z1KCQ4cO8d5776VoQBFX4+bhRpslbfAr5gfA36F/s+S5JcTHON8HHBEREUnbbt++TXR0NNmyZQPg1KlTjB8/nmPHjuHv729yOnlQGT0y8lnDz9jabSuFshcCICouioHrBlJrbi1OXD1hbsAkPPGEfQX6iRPQsye4udn7w8LgtdegcGGYM8e+7YuIiIiIM0tWEX3IkCGMGjWKkJAQPD09Hf116tTh559/TrFwIq7K29ebjt93JJN/JgD+2vwXq3qtwjAMk5OJiIhIevLss8/y9ddfA3D9+nUqVarEp59+SsuWLZkyZYrJ6eRhVc9bnYMvH6R/xf6Ovm2nt1FqSikm7pqIzbCZmC5pefLY90b/9Vdo1y6h//Rp6NYNSpaE5ctBU2URERFxVskqoh85coRWrVrd1e/v78+VK1ceOZRIWpA1f1bar2yPu7c7AIfmHmLbR9tMTiUiIiLpyf79+3nmmWcAWLZsGTlz5uTUqVN8/fXXfPHFFyank+TI5JmJCY0nsLnLZp7M9iQAt+Nu039tf+rMrcOf1/40OWHSCheGRYtg/35o3Dih/7ff4LnnoFIlWL/evHwiIiIiSUlWET1r1qxcuHDhrv4DBw6QO3fuRw4lklbkqZyHlnNbOo43vr2Ro0uPmhdIRERE/punJ8bo0UQOHQr/+talK7p16xZZ/rm647p162jdujVWq5XKlStz6tQpk9PJo6iZvyaHeh/i1QqvOvq2nNpCqSml+HL3l067Kh2gbFlYswa2boVq1RL69+yB+vWhbl3Ytcu8fCIiIpLKXGD+nawievv27Xnrrbe4ePEiFosFm83G9u3beeONN+jcuXNKZxRxacWfL06d0XUcxys6r+DsrrMmJhIREZH7slqhZEnigoPtbRdWqFAhVqxYwZkzZ/jxxx9p0KABAGFhYfj4+JicTh5VZs/MTGoyiY2dN5I/a34AbsbepO8Pfan3dT3+uv6Xqfn+yzPPwE8/werVUKpUQv/GjVC5MrRqBUe1/kRERCTtc4H5d7JSffjhhxQtWpSgoCAiIyMpVqwYNWrUoGrVqrz77rspnVHE5VUfWp3SXUoDEBcVx6IWi7j+13VzQ4mIiEiaN3z4cN544w3y589PxYoVqVKlCmBflV62bFmT00lKqV2gNod7H6Z3+d6Ovk1/baLklJJM2zvNqa/LY7FA06Zw4AAsWAAFCybct2KFfb/0Ll3gr7/MSigiIiKSzCK6p6cnX331FX/88QerV6/mm2++4ffff2fevHm43bnkuog4WCwWmk9vTr6a+QC4GXaTBc0WEBUeZXIyERERuUtcHHz/PV7r19vbLqxNmzacPn2avXv38uOPPzr669aty+eff25iMklpWbyyMKXZFEJeDCGvb14AImMi6f19bxp804BT1517+x6rFTp0sO+PPnUq5Mpl7zcM+Ppr+37q/frBpUvm5hQREZHHwAXm34+0Pj5v3rw0adKE559/nqeeeiqlMomkSW6ebrRb3o7sT2UH4PLRyyx7fhm2OOfdr1JERCRdiovDMm0aGb7+2mkn8Q8jICCAsmXLcv78ec6etW8pV7FiRYoWLWpyMnkc6j1ZjyN9jtCzXE9H3/o/11NySkm+2veVU69KB/DwgJdfhhMnYOxYyJbN3h8bC5MmwZNPwjvvwPXrpsYUERGRlOQC82/3Bz1x4MCBD/ykn332WbLCiKR1GbJnoOP3HZlZeSa3r97mj3V/sKbfGppOborFYjE7noiIiKQxNpuNUaNG8emnnxIZGQlAlixZGDRoEO+88w5WJ91zUh6Nj5cP05tP57ng53hp1UucjTjLjZgb9Frdi29/+5avmn9FkG+Q2THvK2NGePNN6NULPvkEPv8cbt2y3z78EKZMgbfesq9Oz5jR7LQiIiKS1j1wEf3AgQMPdJ4KgSL3l+OpHLT7Xzu+rvc1tlgb+6bu44kiT1B5QGWzo4mIiEga88477zBz5kw++ugjqlWrBsC2bdt4//33iYqKYvTo0SYnlMepYaGG/NLnFwb+OJBZB2cB8OMfP1JiSgnGNxxP1zJdnf7zW9asMGqUvVg+erR9q5fYWLh2DYYMgQkTYPhw6NHDvopdRERE5HF44CL6pk2bHmcOkXQlX418tJjRghVdVgDw48AfyfZkNoq0KGJuMBEREUlT5s6dy4wZM2jRooWjr1SpUuTOnZtXXnlFRfR0wNfbl5nPzuS5Ys/Rc1VPzt84T0R0BN2/686y35Yxvdl0cvvkNjvmf8qZE774AgYOhPffh3nzwGaDCxegTx/7avUPPoD27e37q4uIiIikpEeeXpw5c4YzZ86kRBaRdKV059I88+4z9gMDvu3wLRf2XzA3lIiIiKQpV69evefe50WLFuXq1asP9Bz58+fHYrHcdXv11VeTfMzSpUspWrQo3t7elCxZkjVr1iT7PUjKaPJUE37p8wtdSndx9K05vobik4sz9+Bcp98r/Y78+WHOHDhyBFq1Suj/4w/o1AnKlIHVq+0XJBURERFJKckqosfFxTFs2DB8fX3Jnz8/+fPnx9fXl3fffZfY2NiUziiSZtX+oDYl2pcAIPZWLAubLyTiXITJqURERCStKF26NJMmTbqrf9KkSZQqVeqBnmPPnj1cuHDBcQsJCQGgbdu29zx/x44ddOjQgR49enDgwAFatmxJy5Yt+eWXX5L/RiRFZMuQjTkt57CqwyoCMgcAEB4dTteVXWmxqAXnb5w3OeGDK1YMli+HXbugbt2E/iNHoHlzqF4dtm41L5+IiIikLckqovfr14/p06czbtw4Dhw4wIEDBxg3bhwzZ86kf//+KZ1RJM2yWCw8O/tZ8lTJA8CN8zdY2HwhMZExJicTERGRtGDcuHHMmjWLYsWK0aNHD3r06EGxYsWYM2cOn3zyyQM9h5+fHwEBAY7b6tWrKViwIDVr1rzn+RMmTKBRo0YMHjyY4OBgRo4cSbly5e5ZzBdzNCvcjKOvHOWFUi84+laHrqbE5BLMPzzfZValA1SsCOvX228VKiT079gBNWtC48bwgJf3EhEREUlSsoroCxYsYM6cObz88suUKlWKUqVK8fLLLzNz5kwWLFiQ0hlF0jR3b3far2hP1vxZAbh44CLLOy3HFm8zN5iIiEh65eGBMWwYkQMHuvyVCmvWrEloaCitWrXi+vXrXL9+ndatW3P06FHmzZv30M8XExPDN998Q/fu3ZO8IOXOnTupV69eor6GDRuyc+fOZL0HeTyyZ8jOvFbzWNFuBTkz5QTgWtQ1XvjfC7Ra3IqLkRdNTvhw6ta1r0pfvhyCgxP6166FcuWgXTsIDTUvn4iIiNyHC8y/H/jCov/m5eVF/vz57+ovUKAAnp6ej5pJJN3J5J+Jjt93ZGaVmURHRHPsu2OEDA6h4WcNzY4mIiKS/ri5QYUKxIWF2dsuLjAw8K4LiB46dIiZM2cyffr0h3quFStWcP36dbp27ZrkORcvXiRnzpyJ+nLmzMnFi0kXZaOjo4mOjnYcR0TYt7ez2WzYbFpY8Dg1L9ycqr2r0n9tfxYdXQTAymMr+en0T0xoOIE6fnVc6nfw7LPQrBl88w28/76F06ft/9izZAl8+61B164wfLhBnjzm5kwLbDYbhmG41PiQ1KGxIUnR2JAkWSzYypcn9vJlbBaL/erhqeRBx2Oyiuh9+/Zl5MiRzJ49Gy8vL8A+8R09ejR9+/ZNzlOKpHt+xfxou6wt8xvPx4g3+Pnzn8n+VHYq9Knw3w8WERERSQUzZ86kcePGBAYGpujzjhkzhhEjRtzVf/nyZaKiolL0teTePq/+OfUC6/HWT2/xd9TfXL19lRdXvEj93PUZV2ucYw91V9G4MdSpA/PmZWT8+Ez8/bcb8fEWZs60F9i7dr1Fv36R5MjhOlvXOBubzUZ4eDiGYWC1JutL7pJGaWxIUjQ25H7MGh83btx4oPOSVUQ/cOAAGzZsIE+ePJQuXRqwr2aJiYmhbt26tG7d2nHu8uXLk/MSIulSwfoFaTqlKat7rQbgh34/kO3JbBRqWMjkZCIiIulIXBxs2oTn9ev2Za36piUAp06dYv369f85vw8ICODSpUuJ+i5dukRAQNJF2KFDhzJw4EDHcUREBEFBQfj5+eHj4/NoweWBdfPvRrOSzei3th9Lf10KQMi5EGosrcHb1d+mf6X+eLt7m5zy4bz9NvTvDxMm2PjkEwsRERaioy1Mm5aJBQsyMnCgweuvQ5YsZid1PTabDYvFgp+fn4phkojGhiRFY0OSFBeHsXkzXtev49uiBdZUnH97ez/Y3CZZRfSsWbPy3HPPJeoLCgpKzlOJyP9Tvmd5/g79m52f7MSIN1j2/DK6b++Ofwl/s6OJiIikD3FxWCZMIGNMDDRtqiL6P2bPno2/vz9Nmza973lVqlRhw4YNDBgwwNEXEhJClSpVknyMl5eX4xuu/2a1WvUhO5XlzJKTJW2XsPToUl5Z8wpXbl3hRswNhm4cyvT90/m4/se0Dm6d5J74zsjHB4YNg1degbFjYeJEiIqCGzcsjBhh4csv7cX2Pn3gAT9Hyz8sFov+O5V70tiQpGhsyD3ZbBhffEGmmBiszZql6vh40Nd66CK6YRiMGDECPz8/MmTI8NDBROS/1fuoHtdOXOP3Fb8THRHNgmYLeGnXS2TOmdnsaCIiIuIC/v3N0Hu5fv36Qz2fzWZj9uzZdOnSBXf3xB8hOnfuTO7cuRkzZgwAr732GjVr1uTTTz+ladOmLFq0iL179z70/utirrbF21Ijbw0G/zCY+b/Px2bYOHn9JG2WtqFGvhqMbziesrnKmh3zoeTIAePGwWuvwciRMGMGxMfDlSswcCB8/jm89x506QLuyVpuJiIiImnVQ5f1DcOgUKFCnD179nHkERHA6mal1TetyFUuFwDhp8JZ9OwiYm/HmpxMREREXIGvr+99b/ny5aNz584P/Hzr16/n9OnTdO/e/a77Tp8+zYULFxzHVatWZcGCBUyfPp3SpUuzbNkyVqxYQYkSJVLkvUnq8cvkx7ga49jXcx91CtRx9G89tZXy08vTY2UPLty4cJ9ncE65c8PUqfD779ChQ0L/mTPw0ktQogQsWwaGtksXERGRf1gM4+GnBsWLF2fmzJlUrlz5cWRyOhEREfj6+hIeHq49GU1ks9kICwvD398/3Xzt58b5G3xV8StunLNf5KBY22K0WdQGi9V1vj6bWtLj+JAHo7EhSdHYkCRFRWG0aUN0TAyeK1ZgzZgx1V5a8047/Rycw7//TlosFlaFrmLQukGcuHrCcU5mz8y8Xf1tXq/yusvtl37HoUPwzjvw/feJ+8uXhw8/hPr1wYV2r0k1+v+oJEVjQ5KisSFJcoH5d7JG7EcffcTgwYP55Zdfkh1QRP5blsAsdFzdEY9MHgD8uvRXNg7baHIqEREREUlvLBYLLYq04OgrR/m0waf4evkCEBkTydsb36bopKIsPbqUZKzRMl3p0rB6Nfz0EzzzTEL/vn3QsCHUqQM//2xePhERETFfsoronTt3Zvfu3ZQuXZoMGTKQPXv2RDcRSTkBZQJoszhh9fm2D7dxcM5Bc0OJiIiISLrk6ebJwCoDOd7vOH2e7oPVYv9IeSr8FM8ve54ac2qw9/xek1MmT/XqsGULrFkDZcok9G/eDFWqwLPPgtaRiYiIpE/JulzK+PHjUziGiNxP4aaFafh5Q9a+thaAVb1WkTV/VvLXym9uMBERERFJl/wy+TG56WReqfAKA38cSMifIQBsO72NCl9VoEvpLnxY90MCswSanPThWCzQuLF9BfrSpTBsGBw/br/vu+9g1Sro3t1+YdJcuczNKiIiIqknWXuipzfak9E5pPe9swzD4Id+P7Dnyz0AeGfzpsfOHjxR5AmTkzmH9D4+JGkaG5IUjQ1JUnw8tu3buXbtGtmaNMHq4ZFqL615p51+Ds7hQf9OGobBmuNrGLhuIKF/hzr6M3lkYkj1IQyqMogMHhlSI3KKi42FOXNgxAg4dy6hP1MmGDIEBg6EVNy21ano/6OSFI0NSYrGhiTJBebfyR6xf/zxB++++y4dOnQgLCwMgB9++IGjR48m9ylF5D4sFguNxjeiUKNCAERdi2JB0wXcDLtpcjIREZE0xs0NqlcntlIle1tE7stisdC0cFOO9DnC+IbjyeqdFYCbsTcZtmkYRb8syqJfFrnkfukeHtCzp301+rhxcOez9c2b9lXqRYrA/Plgs5mbU0RExKW5wPw7WUX0LVu2ULJkSXbt2sXy5cuJjIwE4NChQ7z33nspGlBEEljdrbRZ3Ab/kv4AXPvjGrOqz+L6X9fNDSYiIiIi6Z6nmyevVX6NE/1O0LdCX9ws9g/Bp8NP0+HbDlSfXZ3d53abnDJ5MmSAwYPhxAl49dWEz/dnz8ILL0DlyrBtm7kZRURE5PFJVhF9yJAhjBo1ipCQEDw9PR39derU4WddtlzksfLy8aLj6o745LEvg7l6/Cozq87k0pFLJicTERFJI+LjYds2PHbtsrdF5KHkyJiDiU0mcrjPYRoWbOjo33FmB5VmVKLz/zpzNuKsiQmTz88PJk2CI0egSZOE/j174JlnoG1b+PNP8/KJiIi4JBeYfyeriH7kyBFatWp1V7+/vz9Xrlx55FAicn++eX3pvr07OYrkACDyQiRzaszh9LbTJicTERFJA2JjsYwbR6Yvv7RviCwiyVLMrxhrX1jLmo5rKPpEUUf/vMPzKDKpCCM2j+BW7C0TEyZfcDB8/z38+COUKJHQv2yZ/b4334Tr102LJyIi4lpcYP6drCJ61qxZuXDhwl39Bw4cIHfu3I8cSkT+m29eX7pv605ghUAAoq5HMa/+PI6tOmZyMhERERGRBI2faszh3of5otEXZPPOBsCt2Fu8v+V9ikwqwvzD87EZrrmpeIMGcPAgTJ8O/vYdF4mJgY8/hqeegsmTIS7O1IgiIiKSApJVRG/fvj1vvfUWFy9exGKxYLPZ2L59O2+88QadO3dO6YwikoSMT2Sky8YuFGxQEIC4qDgWt1rMwbkHzQ0mIiIiIvIvHm4e9KvUjxP9T9C/Yn/HfulnI87ywv9eoOrMqvx81jW3BnVzS7j46Ntvg5eXvf/KFfv+6aVKwZo14ILXVRUREZF/JKuI/uGHHxIcHEzevHmJjIykWLFi1KhRg6pVq/Luu++mdEYRuQ/PzJ50WNWBEu3t3yM14g1Wdl3J9o+3m5xMRERERCSx7BmyM6HxBI70OUKTpxI2Fd91bhdVZlah0/JOnAk/Y2LC5PPxgdGj4dgx6NAhof+336BpU2jY0L6XuoiIiLiehyqi22w2xo4dS+3atTlw4AAvvvgiq1ev5ptvvuH3339n3rx5uN25TLmIpBo3Tzdaz29NxX4VHX3r31zPusHrMLTkRUREREScTLBfMN93/J61ndZSzK+Yo3/BkQUUmVSE9za9x82YmyYmTL58+WDBAti5E6pUSegPCYEyZeDll+HSJdPiiYiISDI8VBF99OjRvP3222TOnJncuXOzYMECli1bxvPPP89TTz31uDKKyAOwWC00mtCI2qNqO/p2frKTld1WEh/rnFc2FhEREZH0rWGhhhzqfYgvm3xJjgw5ALgdd5sPtn5A4UmFmXdonsvul165MmzfDosW2QvrADabff/0QoVgzBiIijI3o4iIiDyYhyqif/3110yePJkff/yRFStWsGrVKubPn4/N5pqTGpG0xmKxUOOdGjSb1gyL1QLAobmHWNJ6CbG3nPPqxiIiIiKSvrlb3Xmlwisc73ec1yu/jrvVHYDzN87TeUVnKs+ozI4zO0xOmTwWC7RrB7//Dh99BFmy2PsjI+37pxctai+y68ujIiIizu2hiuinT5+mSZOEfevq1auHxWLh/PnzKR4MID4+nmHDhlGgQAEyZMhAwYIFGTlyZKLtKQzDYPjw4eTKlYsMGTJQr149jh8/nuh5rl69SqdOnfDx8SFr1qz06NGDyMjIx5JZxBmU71Wetkvb4uZp314pdHUo8xrM4/a12yYnExERcQHu7hivvcatnj3B3d3sNCLpRrYM2fis4WccfeUozQs3d/TvOb+HarOq0X5Ze05dP2ViwuTz9oa33oITJ6B3b7D+80n81Cn7/ulVq9q3fxEREUmXXGD+/VBF9Li4OLy9vRP1eXh4EBv7eFa4jh07lilTpjBp0iR+++03xo4dy7hx45g4caLjnHHjxvHFF18wdepUdu3aRaZMmWjYsCFR//peXKdOnTh69CghISGsXr2arVu30qtXr8eSWcRZBLcOptPaTnhm8QTgzPYzzKkxh4hzESYnExERcXLu7lC3LjHPPOO0k3iRtKxwjsJ81+E71r2wjhL+JRz9i48upuiXRXl347tExrjmoih/f5gyBQ4dsl9o9I6ff7YX0tu3h7/+Mi2eiIiIOVxg/m0xHuKqg1arlcaNG+Pl5eXoW7VqFXXq1CFTpkyOvuXLl6dIuGbNmpEzZ05mzpzp6HvuuefIkCED33zzDYZhEBgYyKBBg3jjjTcACA8PJ2fOnMyZM4f27dvz22+/UaxYMfbs2cPTTz8NwNq1a2nSpAlnz54lMDDwP3NERETg6+tLeHg4Pj4+KfLe5OHZbDbCwsLw9/fHan2of/9J1y4cuMD8RvO5GWa/MJNvPl9eXPciOQrnMDlZytL4kKRobEhSNDbkfswaH5p32unn4Byc4e9knC2OGftnMGzTMK7cuuLoz5U5Fx/W/ZDOpTtjtbju3/C1a2HQIPj114Q+Ly94/XUYOhScefg7w/gQ56SxIUnR2JD7cfb590Ml6tKlC/7+/vj6+jpuL7zwAoGBgYn6UkrVqlXZsGEDoaGhABw6dIht27bRuHFjAE6ePMnFixepV6+e4zG+vr5UqlSJnf98F27nzp1kzZrVUUAH+zY0VquVXbt2pVhWEWeVq2wuum/vTtYCWQEIPxXOrGqzOL/38WzDJCIi4vLi42HPHtwPHrS3RcQ07lZ3ej/dm+P9jjOoyiA8rB4AXIi8QLeV3aj4VUV+OvWTySmTr1Ej+6r0KVPAz8/eFx1t3z+9UCGYNg3i4szNKCIi8ti5wPz7odbHz549+3HluKchQ4YQERFB0aJFcXNzIz4+ntGjR9OpUycALl68CEDOnDkTPS5nzpyO+y5evIi/v3+i+93d3cmePbvjnP8vOjqa6Ohox3FEhH37C5vNpouomshms2EYhn4HyZD1yax0/akrC5su5NKhS9y6cou5tefS9tu2PFnvSbPjpQiND0mKxoYkRWNDkhQdDSNHkjk6Glv16uDmlmovrfEocm9ZvbPySYNPeLn8ywwOGczKYysB2HdhHzXm1KBtsbaMrTeWAtkKmJz04bm72/dJ79ABPvwQxo+HmBi4fNneP2kSfPopNGhgdlIREZHHJDYWy8iRZI6JgRo1wMPD7ER3cc5NZv6xZMkS5s+fz4IFCyhevDgHDx5kwIABBAYG0qVLl8f2umPGjGHEiBF39V++fDnRXuuSumw2G+Hh4RiGoa/9JIcbNF7cmB+7/siFny8QExnDwmYLqTOpDgVbFDQ73SPT+JCkaGxIUjQ2JElRUWSNjiY2Lo6rYWFYM2ZMtZe+ceNGqr2WiCt6KsdTrGi/gg1/buD1H1/nSNgRAJb+upTvjn3HwCoDGVp9KFm8spic9OH5+sLYsfbC+ZAhsGSJvf+XX+z7pzduDJ98AsWKmZtTREQkPXLqIvrgwYMZMmQI7du3B6BkyZKcOnWKMWPG0KVLFwICAgC4dOkSuXLlcjzu0qVLlClTBoCAgADCwsISPW9cXBxXr151PP7/Gzp0KAMHDnQcR0REEBQUhJ+fn/ZkNJHNZsNiseDn56diR3L5Q5f1XVjecTmh34Vii7Wxvvd6PGI9eLrP0//9eCem8SFJ0diQpGhsSJKiouybEoN9T8ZULKJ7e3un2muJuLK6T9blwMsHmHlgJu9ufJfLty4THR/NmG1jmHVgFqPrjKZrma64WVPvmyQppUABWLwYXnvNvjf67t32/h9+gHXroFcvGDEiYfsXERERefycuoh+69atuz7Uurm5Ob7mWqBAAQICAtiwYYOjaB4REcGuXbvo06cPAFWqVOH69evs27eP8uXLA7Bx40ZsNhuVKlW65+t6eXklunjqHVarVR+yTWaxWPR7eERembxo9207Vr28ioOzDoIBP/T9gVuXb1HzvZpYLBazIyabxockRWNDkqKxIfdktWIAmDA+NBZFHpyb1Y1e5XvRrng7PvzpQ8bvGk9MfAyXbl7ipVUvMWnPJMY3HE/N/DXNjposVavCzp2waJF9ZfqZM/ZtYqdMgfnz4d13oX9/x7/5iYiIyGPk1LP05s2bM3r0aL7//nv++usv/ve///HZZ5/RqlUrwP7Bd8CAAYwaNYrvvvuOI0eO0LlzZwIDA2nZsiUAwcHBNGrUiJ49e7J79262b99O3759ad++PYGBgSa+OxHzWN2ttJjRgupDqzv6tozYwppX12CL116sIiIiIuI6fL19GVt/LL++8iutg1s7+g9ePEitubV4bslz/HntT/MCPgKrFTp2hGPHYPRoyJzZ3h8RAW++CcHBsHQpGIa5OUVERNI6py6iT5w4kTZt2vDKK68QHBzMG2+8wcsvv8zIkSMd57z55pv069ePXr16UaFCBSIjI1m7dm2ir8LOnz+fokWLUrduXZo0aUL16tWZPn26GW9JxGlYLBbqfliXhp83dPTtnbKXbzt8S1x0nInJREREREQeXsHsBfn2+W/Z1GUTZQLKOPqX/7ac4C+DeSvkLSKiI8wL+AgyZIC334bjx6FnT3txHeDkSXj+eahePWHbFxEREUl5FsPQv1n/l4iICHx9fQkPD9ee6Cay2WyEhYXZ9ybVV51T1OH5h1nZdSW2uH+2SqpbgHb/a4dXFtf5bqjGhyRFY0OSorEhSYqKwmjThuiYGDxXrEjVPdE177TTz8E5uPLfyXhbPHMOzuGdje9w6eYlR79/Jn9G1R5F97LdXXK/9DsOH4aBA2HDhsT9nTrBhx9C3ryPP4Mrjw95vDQ2JCkaG5IkF5h/a8SKCKU6laL9d+1xz2C/TMLJDSeZW3suN8NumpxMRETEBO7uGC+/zO3OncHdqS8hJCJJcLO60aNcD0L7hTKk2hA83TwBCLsZRq/VvSg3vRybTm4yOWXylSoFISGwejUUKZLQP3++/fjdd+HGDfPyiYiIPBQXmH+riC4iADzV+Ck6b+iMdzb7VkgX9l1gVvVZXP/rurnBREREUpu7OzRtSnS9ek47iReRB+Pj5cOYemP4/dXfaVOsjaP/8KXD1Pm6Dq0Wt+LE1RMmJkw+iwWaNoUjR2DiRMiRw94fFWXfP/2pp2DGDPvFSEVERJyaC8y/VUQXEYegKkF039Ydnzz2r69cPX6VmVVncunIpf94pIiIiIiI8yqQrQBL2y5lS9ctlMtVztG/4vcVFPuyGIPXDSY8KtzEhMnn4QF9+9r3Sx80yH4McOmSff/0cuXu3vZFREREHo6K6CKSiF8xP7pv784TRZ8AIPJCJHNqzOH0ttMmJxMREUklNhscOYL7b7/Z2yKSZtTIV4M9Pfcw+9nZBGQOACDWFssnOz/hqYlPMXXvVOJscSanTJ5s2eCTT+C33+C55xL6Dx+GevWgeXP4/Xfz8omIiCTJBebfKqKLyF188/rS7adu5K6YG4Co61HMqz+PY6uOmZxMREQkFcTEYHnnHTKPGQMxMWanEZEUZrVY6VqmK6F9Q3nnmXfwcvMC4PKty/T5vg9lp5Vl/Z/rTU6ZfAULwrJlsGULlC+f0L96NZQoAf36wd9/m5dPRETkLi4w/1YRXUTuKeMTGem8oTMFGxQEIC4qjsWtFnNwzkFzg4mIiIiIpIAsXlkYVWcUx/oeo13xdo7+X8J+of68+rRY2ILQv0NNTPhoatSA3bvh668ht31tDPHxMGkSFCoEn30G0dHmZhQREXEVKqKLSJI8M3vSYVUHSnQoAYARb7Cy20q2f7zd5GQiIiIiIikjX9Z8LGqziG3dtvF04NOO/lWhqyg+uTgDfxzItdvXTEyYfFYrvPgihIbCBx9Axoz2/uvX7funFy8Oy5eDYZgaU0RExOmpiC4i9+Xm6Ubrb1pTsV9FR9/6N9ezbvA6DJtm2yIiIiKSNlTLW41dL+1ibsu5BGYJBCDOFsfnP3/OUxOfYvKeyS67X3rGjDBsmP3io926gcVi7//jD/v+6bVqwb59pkYUERFxaiqii8h/slgtNJrQiNqjajv6dn6yk5XdVxIfG29iMhERERGRlGO1WOlcujPH+h5jWI1heLt7A/D37b95dc2rlJ5amnV/rDM5ZfIFBsKsWfaCea1aCf1bt8LTT0OXLnD2rGnxREREnJaK6CLyQCwWCzXeqUGz6c2wWO1LVw7NPcSS1kuIvRVrcjoRERERkZST2TMzH9T+gGN9j9GxZEdH/6+Xf6XhNw1ptqAZx64cMzHhoylbFjZuhJUr4amnEvq//hoKF4b33oPISPPyiYiIOBsV0UXkoZTvWZ62S9vi5ukGQOjqUOY1mMfta7dNTiYiIiIikrLy+uZlfuv57Oi+g4q5E7Y3/P7495SYUoLX177usvulWyzQogX88guMHw/Zstn7b9+2759euDDMng02m6kxRUREnIKK6CLy0IJbB9NpbSc8s3gCcGb7GebUmEPEuQiTk4mIiKQAd3eMrl253a4duLubnUZEnECVoCrs7LGTea3mkTtLbsC+X/r4XeMpNLEQX+7+0mX3S/f0hNdegxMnYMCAhD97Fy5A9+72bV42bTI1ooiIpHUuMP9WEV1EkqVA7QJ03dKVTP6ZAAj7JYxZ1WZx5dgVk5OJiIg8Ind3aN2a6KZNnXYSLyKpz2qx8kKpFzjW9xjv1XyPDO4ZALh6+yp9f+jr8vulZ88On38OR4/Cs88m9B84AHXqQMuWEBpqWjwREUnLXGD+rSK6iCRbrrK56L69O9metH/3M/xUOLOrz+b83vMmJxMREREReTwyeWbi/Vrvp9n90gsXhhUr7HumlymT0L9yJRQvDq+/DlevmpVORETEHCqii8gjyV4oO922dSNn6ZwA3Lpyi7m15/Ln+j9NTiYiIpJMNhscP47bn39qM2ARSVKQb1Ca3S8doHZt2LvXvi96rlz2vrg4+/7phQrBF19ATIypEUVEJK1wgfm3iugi8siy5MpC181dyVcjHwAxkTHMbzKfo0uOmpxMREQkGWJisAwaRJb331eFSET+05390r9p9U2a2y/dzQ26drVv4/Lee5DBvoMN167B669beeaZJ5gyBaKiTI0pIiKuzgXm3yqii0iK8M7qTae1nSjasigAtlgby9ovY8/kPSYnExERERF5vKwWK51KdUqz+6Vnzgzvv28vpnfunNB/+rQ7fftaKVAAxo2DiAjTIoqIiDxWKqKLSIrxyOBB26VtKdujrL3DgDWvrmHz+5sxDMPccCIiIiIij9m/90vvVLKToz+t7JeeJw/MnQt79kD9+gnz+4sX4a23IF8+GDYMLl82MaSIiMhjoCK6iKQoq7uV5l81p/rQ6o6+LSO2sObVNdjinXNfKxERERGRlBTkG8Q3rb9hZ4+dVMpdydGfVvZLf/ppWLvWYO3aKzz3nIHFYu+/fh1GjbIX0197Dc6cMTWmiIhIilERXURSnMVioe6HdWn4eUNH394pe/m2w7fERbvmfpAiIiIiIg+rcp7K7OixI03ulw5QunQcS5YY/PYbdOsG7u72/tu37RceffJJ6N4djrnu4nsRERFARXQReYwqD6hMq29aYXW3/6n5demvLGi6gOgb0SYnExERERFJHWl9v3SAIkVg1iz480/7CvQ7FyCNi4PZsyE4GNq2hf37zc0pIiKSXCqii8hjVapTKdp/1x6PjB4AnNxwkrm153Iz7KbJyUREREREUs+D7Jf++5XfTUz46IKCYPx4OHUK3n0Xsma19xsGLFsG5ctDw4awebO9T0RExFWoiC4ij91TjZ+i84bOZMhuX5JyYd8FZlWfxbWTrrsPpIiIpGHu7hjt2xPVsmXC3gQiIinkfvull5xSkgFrB3D19lUTEz46Pz8YOdJeTB83DnLmTLhv3TqoXRuqVoVVq8CmyyaJiIgLzL9VRBeRVJGnch66/dQNnzw+AFw9fpVZ1WZx6cglk5OJiIj8P+7u0LEjUa1bO+0kXkRcX1L7pU/YNYGnJj7l8vulA/j4wODB8NdfMGUKFCiQcN/PP0OLFlC6NCxYYN/6RURE0ikXmH+riC4iqcavmB/dt3fniaJPABB5IZI5NeZwettpk5OJiIiIiKS+B9kv/ccTP5qc8tF5e0Pv3hAaCvPnQ4kSCff98gt06mTfV33qVIiKMi+niIhIUlREF5FU5ZvXl24/dSN3Rftqm6jrUcyrP49jq46ZnExEROQfhgGnT2M9e1ab9opIqrjffumN5jei6YKmLr9fOjgWGnLoEHz3HVSunHDfn39Cnz721eoffww3bpiXU0REUpkLzL9VRBeRVJfxiYx03tCZgg0LAhAXFcfiVos5OOegucFEREQAoqOx9O2Lz9tvQ3S02WlEJB1Jar/0NcfXpJn90gGsVmjeHHbsgE2boEGDhPsuXoQ334S8eWHYMLhyxbycIiKSSlxg/q0iuoiYwjOzJx2+60CJDvbvchrxBiu7rWT7x9tNTiYiIiIiYq70sF86gMUCtWrBjz/C3r3Qpo29D+D6dRg1CvLlgwED4MwZE4OKiEi6pyK6iJjGzdON1t+0pmL/io6+9W+uZ93gdRg25/z6joiIiIhIavj3funv13w/ze6Xfkf58rB0Kfz6K3TrlnBduVu3YMIEKFgQuneHY9oFUkRETKAiuoiYymK10Gh8I+qMruPo2/nJTlZ2X0l8bLyJyUREREREzJfJMxPv1XqP0H6haXq/9DuKFoVZs+CPP6B/f8hg/7cDYmNh9mwIDoa2bWH/fnNziohI+qIiuoiYzmKx8Mzbz9BsejMsVvv3Nw/NPcTiVouJvRVrcjoREREREfPl8cmTLvZLvyNvXvsK9FOn4N13wdfX3m8YsGyZfeV6o0awZYvTXoNORETSEBXRRcRplO9ZnrZL2+Lm5QbA8e+PM6/+PG5fu21yMhERERER55Be9ku/w88PRo6E06dh7FjImTPhvh9/tO+pXq0arFqlYrqIiDw+KqKLiFMJbh3MC2tfwDOLJwBndpxhTo05RJyLMDmZiIiIiIhzSG/7pQP4+MCbb8Jff8HkyZA/f8J9O3dCixZQujQsWABxaeffEERExEmoiC4iTid/rfx03dKVTDkzARD2Sxizqs3iyrErJicTEZF0wd0do2VLohs3TriynYiIE0pv+6UDeHtDnz5w/Dh88w2UKJFw35Ej0KkTFCkC06ZBVJR5OUVE5CG4wPxbRXQRcUq5yuai+/buZHsyGwDhp8KZXX025/eeNzmZiIikee7u0L07tzt0cNpJvIjIv6W3/dLB/ue5Uyc4dAhWroTKlRPu+/NP6N0bChSATz6BGzfMyykiIg/ABebfKqKLiNPKXjA73bd3J2dp+8aHt67cYm7tufy5/k+Tk4mIiIiIOJ/KeSqzs8dO5reeTx6fPEDi/dIn7Z5EbHysySlTltVq38plxw7YtAnq10+47+JFGDwY8uWD4cPhir7YKiIiyaQiuog4tcwBmem6pSv5auQDICYyhvlN5nN0yVGTk4mISJplGBAWhvXKFV2lTkRcjsVioWPJjvfcL73fD/3S5H7pABaL/SKj69bBnj3w3HP2PoBr1+wXJ82XDwYMgDNnzEwqIiJ3cYH5t4roIuL0vH29eeHHFyjasigAtlgby9ovY8/kPSYnExGRNCk6GstLL+EzcCBER5udRkQkWTJ6ZHTsl/5CqRcc/b9d+S3N7pd+x9NPw7Jl8Ouv0LVrws4At27BhAlQsCD06AGhoabGFBGRO1xg/q0iuoi4BHdvd9oubUvZHmXtHQaseXUNm9/fjOGk/0opIiIiImK2PD55mNdqHj/3+JnKeRI2Dr+zX/prP7yW5vZLv6NoUZg9G/74A/r3hwz2RfnExsKsWfb7n38eDhwwN6eIiDg/FdFFxGVY3a00/6o51d+u7ujbMmILa15dgy3eZmIyERERERHnVilPJXZ033HXfulf7P4ize6XfkfevPYV6KdOwTvvgK+vvd8wYOlSKFcOGjWCrVuddhcBERExmYroIuJSLBYLdUfXpeH4ho6+vVP2srzDcuKj401MJiIiIiLi3NLrful3+PnBqFFw+jSMHQs5cybc9+OPULMmVK8Oq1ermC4iIompiC4iLqnya5Vp9U0rrO72P2O/ffsb3z33HWe26ypBIiIiKe3cuXO88MIL5MiRgwwZMlCyZEn27t2b5PmbN2/GYrHcdbt48WIqphaRpKTn/dIBfHzgzTfh5EmYPBny50+4b8cOaN4cypSBhQshLs6slCIi4kxURBcRl1WqUyk6rOqAR0YPAML2hTGnxhzmN57P+b3nTU4nIiKSNly7do1q1arh4eHBDz/8wK+//sqnn35KtmzZ/vOxx44d48KFC46bv79/KiQWkQeVnvdLB/se6X36wPHjMG8eFC+ecN/hw9CxIxQpAtOmQVSUeTlFRMR8KqKLiEsr1KgQnTd2Jnuh7I6+E2tP8FWFr1jcajGXjlwyMZ2IiIjrGzt2LEFBQcyePZuKFStSoEABGjRoQMGCBf/zsf7+/gQEBDhuVqs+fog4o/S8XzqAuzu88IK9cL5yJVSqlHDfn39C797w5JPwySdw44Z5OUVExDwWw9BOX/8lIiICX19fwsPD8fHxMTtOumWz2QgLC8Pf318fwOQucTFxbPtyGwfHHyT8dHjCHRYo0a4ENd+vyRNFnjAvoJhGfzskKRobkqTYWGzTp3MjMpIsAwZg9fJKtZd2xnlnsWLFaNiwIWfPnmXLli3kzp2bV155hZ49eyb5mM2bN1O7dm3y5ctHdHQ0JUqU4P3336datWr3PD86Opro6GjHcUREBEFBQVy7ds1pfg7pkc1m4/Lly/j5+envZDpyK/YWn+78lHE7xnEr9pajP/iJYD5t8CkNC9qvTZSWx4dhwObN8NFHFtavtyS6L1s2g759oV8/gxw5zMnn7NLy2JBHo7EhSYqNxfjqKyIjI8n02mupPv/Oli3bf86/VUR/AM74YSY9UrFD7ufO+Mjum51Dsw+xddRWIi9EOu63WC2U7lyaGsNrkK3Af3/9XNIO/e2QpGhsyP2YNT6ccd7p7e0NwMCBA2nbti179uzhtddeY+rUqXTp0uWejzl27BibN2/m6aefJjo6mhkzZjBv3jx27dpFuXLl7jr//fffZ8SIEXf1h4aGkiVLlpR9Q/LAbDYb4eHh+Pr66u9kOnQ+8jwf7v6Qb49/m6i/bt66vFf5PQr6FkwX4+PgQXcmTszMDz94YRgJBfUMGWy88MJteve+SWCgzcSEzkd/OyQpGhtyP2aNjxs3blC4cGEV0VOCM36YSY9U7JD7+f/jI/Z2LHun7GXbmG3cupKwgsbqbqXsS2Wp8U4NfPLov+f0QH87JCkaG3I/KqIn8Py/9u47PIpy7eP4dze9N0ijht4hkEhTEUEBlSOKIIoiinBUBBELYAPE8qpHRVRAUEFULKh4OGJHUKQX6b2HhCQQ0nuy+/6xZsOSbAgC2Q35fa5rLrIzszPPrI+z99777P24uxMTE8Pq1aut68aOHcuGDRtYs2ZNpY/To0cP6tevz8cff1xmm0aiOyeNGBSAdfHrGP/TeNbGr7WuczW68kCnB3ig5QM0r9e8RvSPPXvg1VcNfPopFBWVJtPd3MzcfTc88YSZZs0c2EAnonuH2KO+IRVxVP+o7Eh01yprkYhIFXLzcqPr+K50GtWJdTPWsfq11eSl5WEqMrFp9ia2zNtCzAMxXDnpSnzDfB3dXBERcSZmM6SnY8jIgNq1Hd0ah4uIiKBVq1Y261q2bMnXX39t5xnlu+KKK/jzzz/L3ebh4YFHOT/bNRqN+pDtYAaDQf8dariu9bqyesRqPt/xOU/++iTHM45TZCrinQ3v8P5f73N769t5IOYBOtfpjMFgOPcBq6lWrWD+fHj+eUtt9Pffh9xcKCw08OGHMG+egWuugdtvh4EDoVYNrySpe4fYo74h5TKbITMTY2YmxioexFLZc6nHishlzd3XnaueuopHDj/C1c9djbuvOwDF+cWse2sdMxrN4NeJv5KTknOOI4mISI2Rn4/h7rsJePhhOGN0dE3VvXt39u7da7Nu3759NGjQ4LyOs2XLFiIiIi5m00SkihgMBu5oewd7H97L1Gum4u3mDUBeUR4fbf2Irh90pf3s9ry7/l3S8tIc29hLrH59mDEDjhyBp56CgADLerMZli+3TEIaHg59+sCHH0JqqkObKyJSPVSD+FtJdBGpETwDPek5tSePHH6Ebk92w9XL8kOcwpxCVr2yirei3mLFlBXkpec5uKUiIiLO5dFHH2Xt2rW89NJLHDhwgIULFzJnzhxGjx5t3WfSpEkMGzbM+nj69On897//5cCBA+zYsYNx48bx22+/2TxHRKofbzdvnuvxHHsf3svDsQ/j7176s/ftydt5+IeHiXw9kvv+ex9rj6/lcq4eGxoKL74IR4/C//0fNGlSuq24GH7+GUaMgLAwuOkm+PhjyMhwXHtFROTCKIkuIjWKdy1vrnvlOh459AhXjL0CF3cXAAoyC/h96u+8FfUWf/7fnxRkFzi4pSIiIs4hNjaWxYsX89lnn9GmTRumTZvG9OnTGTp0qHWfEydOcOzYMevjgoICHnvsMdq2bUuPHj3YunUrv/76K7169XLEJYjIRVbXvy5v9X2Lv+76iw/+9QFd63a1bsstymXelnl0/aArHd7rwLvr3yU9L92Brb20AgJgwgTYtw82bYInn4Qzf6hTWAhLl8KwYZbE+y23wOefQ3a249osIiLnTxOLVoIzTvBUE2kCOKnIP+0f6XHp/PHCH2z5cAumIpN1vU+oD1dOupKYB2Jw9dT0EdWZ7h1ij/qG2JWXh/m228gvKMD9228xentX2akVd1rodXAOuk9KRc7uH9uTtjNn0xw+3vYx6fm2SXNvN2+GtB7CqE6juKLOFZd17XSwlHZZvx6++AK+/BLi48vu4+VlGaF+++1www2Wx5cL3TvEHvUNsasaxN/qsSJSowXUC6D/e/0ZvWc07Ye1x2C0BPTZydn89OhPzGgygw2zNlBcUOzgloqIiIiIOK+2YW15+4a3SXgsgXk3z6NL3S7WbTmFOXy45UO6fNCF6Peimblh5mU9Ot1ggM6d4Y034NgxWLkSRo+2lHYpkZsLixbBbbdZRqjfdRf8739OWwpYRKTGUxJdRAQIbhzMgI8G8OCOB2k9uLV1fWZ8Jt8/9D3vNH+Hv+b9ZTNaXUREREREbHm7eTO8w3DWjFjD1ge2Mjp2NP4epSP7tiZtZfT3o4l8I5IR/x3B+vj1l3XtdKMRrrwS3nnHMiJ92TIYNQpCQkr3ycqCTz+Ff/3Lkmi/91748UdLKRgREXEOSqKLiJyhdsva3PbFbfx7y79p/q/m1vVpR9JYct8SZraeyfbPtmM2Xb6BvoiIiIjIxdAurB3v3PAOCeMT+PBfH9K5TmfrtpLR6Z3f70z0e9HM2jDrsh6dDuDiAtdeC++9BydOWBLl994LgYGl+6Snw/z50K8fRERYEu7LllkmKxUREcdx+iR6fHw8d911FyEhIXh5edG2bVs2btxo3W42m3nuueeIiIjAy8uL3r17s3//fptjnD59mqFDh+Lv709gYCAjRowgKyurqi9FRKqR8PbhDPnvEO5fdz+N+zS2rk/Zl8I3d37D7Paz2b1492U9akZEpMZyccHcqxcFV15pyXiIiMgF8XH34d7oe1l7/1q2/HsLD8U8VGZ0+kPfP0TkG5Hcv+T+y350OoCbG/TpAx9+CElJllIud90Ffn6l+6SkwNy50Ls3REZaSsL88QeY9ONYEbncVIP426knFk1NTSU6OpqePXvy4IMPUrt2bfbv30/jxo1p3NiS1HrllVd4+eWX+eijj4iKiuLZZ59l+/bt7Nq1C09PTwD69evHiRMneO+99ygsLOTee+8lNjaWhQsXVqodmtjIOWgCCqnIpe4fR1ceZfkzyzn6x1Gb9RGdIug5rSdN+ja57CdIqq507xB71DekIo7qH4o7LfQ6OAfdJ6UiF9o/sguy+WLnF8zZNId18evKbO8Q3oFRHUcxtN1Qm4T75S4vD374wTIp6f/+Bzk5ZfeJjIRBgyyTknbpYqnB7kx07xB71DekIs4efzt1En3ixImsWrWKlStXlrvdbDYTGRnJY489xuOPPw5Aeno6YWFhzJ8/nyFDhrB7925atWrFhg0biImJAeDHH3/khhtu4Pjx40RGRp6zHQrinYNutlKRqugfZrOZw8sO89szvxG/Lt5mW71u9ej5Qk+iekZdknPLP6d7h9ijviEVcfYg/nKn18E56D4pFbmY/WNr4lbmbJrDJ9s/ISM/w2abj5sPd7S5g1GdRhETGVOjBq5kZ8PSpZaE+vffWxLsZ6tfHwYPtiTUO3VyjoS67h1ij/qGVMTZ42+n7rFLliwhJiaGQYMGERoaSnR0NHPnzrVuP3z4MImJifTu3du6LiAggM6dO7NmzRoA1qxZQ2BgoDWBDtC7d2+MRiPr1pX9tltExB6DwUCj3o0YsWYEd/zvDsI7hFu3xa2OY8G1C/jo2o+IWx3nwFaKiMgFM5stmYq8PMvfIiJySbUPb8+7N75LwvgEPvjXB1xR5wrrtuzCbN7/632ueP8KOs3pxOyNs8sk2i9XPj6WBPnXX0NyMnzyCfTvbykFU+LYMfjPfyA2Fpo2haefhm3b9PYlItVMNYi/XR3dgIocOnSIWbNmMX78eJ566ik2bNjA2LFjcXd355577iExMRGAsLAwm+eFhYVZtyUmJhIaGmqz3dXVleDgYOs+Z8vPzyc/P9/6OCPD8gZtMpkwqfiYw5hMJsxms/4bSLmqun80uaEJjfs2Zvc3u/l96u+c2nUKgCPLj/Bh9w9p0rcJ1zx/DRGdIqqkPWKf7h1ij/qG2JWXB4MHE5ifj2nxYvD2rrJTqz+KSE3m4+7DfdH3cV/0fWxJ3GIZnb7tEzILMgH4K/EvHlz6II///Dh3tLmDf8f8m04RnWrE6HQ/Pxg61LKkpcG331pGqP/6KxQVWfY5eBBeesmytGhhGZ1+++3QsqUjWy4iUgn5+RgGDyawoMByg6vC+LuynDqJbjKZiImJ4aWXXgIgOjqaHTt2MHv2bO65555Ldt6XX36ZqVOnlll/8uRJ8sr7/ZRUCZPJRHp6OmazWT/7kTIc1T9qXV2LW36+hYPfHmTjfzaSccTypduBHw9w4McDNOzXkNgnYwluEVxlbRJbuneIPeobYldeHoH5+RQWFXE6ORljFQbxmZmZVXYuERFn1iG8AzNvnMmr173KFzu+4L1N77EhYQNQOjr9/b/eJzo8mn93+jd3tr0TPw+/cxz18hAYCMOHW5aUFPjmG0tCffny0klH9+yBqVMtS9u2pQn1Jk0c2HARkWrMqZPoERERtGrVymZdy5Yt+frrrwEID7eUUkhKSiIionS0Z1JSEh06dLDuk5ycbHOMoqIiTp8+bX3+2SZNmsT48eOtjzMyMqhXrx61a9dWTUYHMplMGAwGateurWSHlOHo/hH+YDhd7u/CtgXbWPnCStKPpQNw5IcjHPnxCK1vb02PyT0IaRZS5W2r6RzdN8R5qW+IXXl54OEBYKnJWIVJdE9Pzyo7l4hIdeDr7suIjiMY0XEEf534izmb5vDp9k9tRqc/sPQBHvv5Me5se6e1dnpNERICI0dalqQkS+mXzz+HP/8srYiwfbtleeYZ6NjRkkwfPBgaNnRo00VEqhWnTqJ3796dvXv32qzbt28fDRo0ACAqKorw8HCWLVtmTZpnZGSwbt06HnzwQQC6du1KWloamzZtolOnTgD89ttvmEwmOnfuXO55PTw88Pj7g9OZjEajPmQ7mMFg0H8HscvR/cPoYaTTyE60H9aeze9vZuULK8lKzAIz7Px8J7u+3EX7e9rT47keBDYMdEgbaypH9w1xXuobUi6jETOAA/qH+qKIiH3REdHMumkWr13/Gp/v+Jw5m+bYjE6fu3kuczfPpWNER0Z1HFWjRqcDhIXBQw9Zlvh4WLTIMkJ97drSfTZvtiwTJkDnzpaE+qBBULeu49otIlIdOHWU/uijj7J27VpeeuklDhw4wMKFC5kzZw6jR48GLB98x40bxwsvvMCSJUvYvn07w4YNIzIykgEDBgCWket9+/Zl5MiRrF+/nlWrVvHwww8zZMgQIiMjHXh1InK5cvVw5YrRVzD24Fiu+891eNeyjGA0m8xsmbeFt5u9zdKHlpIRXzMmRBIRERERuZh83X25v+P9rB+5ns2jNvNApwfwcy9Nlm8+sZkHlj5A5BuR/Pt//2ZTwiYHttYx6tSBceNgzRo4cgRefRX+HldotW4djB8P9erBVVfBO++AnanjRERqPKdOosfGxrJ48WI+++wz2rRpw7Rp05g+fTpDhw617vPkk08yZswYRo0aRWxsLFlZWfz44482P4X99NNPadGiBb169eKGG27gyiuvZM6cOY64JBGpQdy83ej2WDfGHhpLzxd64hlouS+ZCk1snLWRGY1n8OOjP5KVlOXgloqIiIiIVE8lo9MTHktgbv+5NqVcsgqymLN5DjFzY+g0pxNzNs0hM7/mzT3RoAE88QRs3Aj798OLL0K7drb7/PknjBljSb5fey289x6cOuWY9oqIOCOD2VxSJUvsycjIICAggPT0dNVEdyCTyURycrKlNql+6ixnqQ79Iy8tjzVvrGHtm2spyCqwrnfzduOKsVfQ/YnueAV7ObCFl6fq0DfEMdQ3xK68PMy33UZ+QQHu335bpTXRFXda6HVwDrpPSkWcuX9sPrHZWjs9q8B2wIqvuy93trmTf8f8m44RHR3UQuewZ4+l3MsXX8Du3WW3u7hA796Wki8DBkBQUOWO68x9QxxLfUPsqgbxt3qsiEgV8Qz0pOfzPXnk8CN0e6Ibrl6WaSkKcwpZ9X+reCvqLVZMXUF+Rr6DWyoiUsMZjZi7d6cwNhb0AU9EpNrpGNGR2TfNJmF8AnNumkOniNI6JiWj0zvN6UTMnBjmbppbI0enA7RoAZMnw86dsG0bPP00NGlSur24GH76Ce67z1JvvX9/+OQTyFBVShG52KpB/K2R6JWgkTDOQd9YSkWqY//IPJHJny//yab3NlFcUGxd7xXsRbcnu3HFw1fg7uPuwBZeHqpj35Cqob4hFXFU/1DcaaHXwTnoPikVqW79Y1PCJuZsmsPCHQvLHZ0+tO1QRnUaVeNHp5vN8NdfpSPUjx4tu4+HB9xwg2WE+k03gY+P7fbq1jek6qhvSEWcPf5WjxURcRC/CD/6zejHmP1j6DiyIwYXAwC5p3NZNnEZMxrNYO1baynKK3JwS0VEREREqrdOkZ14r/97JIxP4L2b3iszOv29Te/RaU4nYufGMnfT3DKJ9prCYICOHeGVV+DwYVi7Fh591FIrvUR+PixeDEOGQGioJZn+zTeQm+u4douIXGpKoouIOFhA/QD6z+nPw3sept3d7cCSSyc7OZufxv3EjCYz2Dh7o81odREREREROX9+Hn6M6jSKjaM2snHkRkZ1HIWvu691+8aEjYz6bhQRr0fwwHcP8NeJvxzYWscyGKBzZ3jjDTh2DFauhNGjLaVdSuTkwJdfwsCBloT6XXfB//5nSbSLiFxOVM6lEvRzUuegn/1IRS6n/nFy10lWTFnBrkW7bNYHNgykx+QetLurHUbX6n2NVely6htycalviF3VYGKjy51eB+eg+6RU5HLqH5n5mXy24zPe2/Qem09sLrM9JjKGf3f6N0PaDLFJuNdUxcXw+++Wci9ffw0pKWX38fEx0aOHgd69DfTqBW3bWpLyUrNdTvcNuciqQfytHisi4mRqt6rNoC8H8e+//k2z/s2s69OOpPHfe//LzDYz2fH5DswmfQcqIiIiInKhSkanbxq1iY0jNzKy40h83EoLfW9M2MjI/40k8vVIHvzuQbYkbnFcY52Aiwtcey289x6cOAE//gj33guBgaX7ZGcb+f57A+PHQ/v2EB4Od9wB779vKRMjIlLdKIkuIuKkwjuEc8eSOxixdgSNrmtkXZ+yN4Wv7/ia2R1ms+fbPegHRSIiIiIiF0enyE7M6T+HhMcSmH3jbKLDo63bMgsymb1pNtHvRXPF3Cv4YPMHNbZ2egk3N+jTBz78EJKSLKVc7rrLTO3atqUok5Ph889h5Eho1AgaN4ZRoyyj2U+edFDjRUTOg5LoIiJOrm7nutz9890M/3049a+qb12fvD2ZL275gveveJ8DPx5QMl1ERERE5CLx9/Dn3zH/ZtOoTWwYuYH7o++3GZ2+IWED9//vfiJfj+ShpQ/V+NHpAO7ucNNN8NFHZrZuPcm2bSamT4f+/cHPz3bfQ4dg7tzSyUnbt4fHHoPvv4esmv29hIg4KSXRRUSqiQZXN2D478O56+e7qHNFHev6hI0JfNrvU+ZdNY8jK444roEiIiIiIpcZg8FATGQMc/81l4THEph14yw6hHewbs8syGTWxllEvxdNm5ltmLpiKrtO7rJ/wBrCYIDWreGRR2DJEjh9GtasgWnT4JprLAn3M23bZpnA9MYbISgIrrwSpkyxTGZaUOCIKxARsaUkuohINWIwGGh8XWNGrB3BkCVDCGsfZt0WtyqOj3p+xIJeC4hbE+fAVoqIiIiIXH78Pfx5IOYBNo/azPr715cZnb7z5E6m/D6F1jNbWxPqu0/udmCLnYerK3TpAs88A8uXQ2oq/PwzTJgAnTrZTjpaVASrVsHUqXD11RAcDP36wX/+A1u2gMnksMsQkRrM1dENEBGR82cwGGjevznNbmzGrq93sWLyCk7tPgXA4d8Oc7jbYSJjImk3rB1thrTBp7bPOY4oIiIiIiKVYTAYiK0TS2ydWF7v8zqfbvuUT7Z/wuq41dZ9dp7cyc7f/06q127N4NaDGdRqEC1rt3Rgy52Htzdcd51lActI9RUrYNky+PVX2LevdN/sbMvkpT/+aHlcqxb07Am9elmWxo1tk/AiIpeCwawiuueUkZFBQEAA6enp+Pv7O7o5NZbJZCI5OZnQ0FCMRv2IQmzV9P5hKjax47MdrJi8gtRDqTbbjK5GmvRtQrth7WjevzmunjXr+9Oa3jfEPvUNsaugANNLL5GdnY3PtGkYPT2r7NSKOy30OjgH3SelIuofZR3POM5Xu75i0a5FNgn1M9WEhPrF6BvHj1sS6iVLQoL9fRs0KE2oX3sthIf/w4bLJaf7hthVDeJvJdErQUG8c9DNViqi/mFRXFjM1o+2smHmBhL/Siyz3SPAg1aDWtF+WHvqd6+PwXj5D9lQ3xB71DekIo7qH4o7LfQ6OAfdJ6Ui6h8Vq0xCvU1oGwa1GnTZJdQvdt8wm2HPntKE+vLlkJ5uf/82bUqT6j16gN5GnIfuG1IRZ4+/lUSvBAXxzkE3W6mI+kdZyTuS2frxVrZ/up3M+Mwy2wMbBtL2rra0v7s9Ic1CHNDCqqG+Ifaob0hFnD2Iv9zpdXAOuk9KRdQ/Ki8uPY6vd39dYxLql7pvFBfD5s2Wsi/LlsGff0J+fvn7urhAbCz07m1JqnftCh4eF71JUkm6b0hFnD3+VhK9EhTEOwfdbKUi6h/2mYpNHFl+hG0fb2PX17sozC4ss0+dznVod3c72tzeBu9a3g5o5aWjviH2qG9IRZw9iL/cVfZ1KC4uprCw7PuaXBwmk4mUlBRCQkJ0n/ybm5sbLi4ujm6GU9D76D9zPgn1wa0H06JWiypu4YWr6r6RlwerV5cm1TdutD/5qJcXXHmlJaHeuzd06GBJtEvV0H1DKuLs8beS6JWgDzPOQTdbqYj6R+UUZBew59s9bFuwjUO/HsJssn0LMLoaaXpjU9rd3Y5mNzXD1aP6109X3xB71DfErrw8zEOHkp+fj/uXX2L0rrovFxV3WpzrdTCbzSQmJpKWllb1jatBzGYzJpMJo9GIQbP2WQUGBhIeHl7jXxO9j164koT6lzu/ZM3xNeXu0ya0DYNbDWZQ60HVJqHu6L6Rlga//146Senu3fb3DQoqnaS0d29o2lSTlF5Kju4b4sSqQfytJHol6MOMc9DNViqi/nH+MhMy2f7ZdrYt2EbStqQy2z0DPWl9e2va3d2Oet3qVdsPiuobYo/6htiVl4f5ttvILyjA/dtvnTKIv9yd63U4ceIEaWlphIaG4u3tXW3fo5yd2WymqKgIV1dXvcZYXo+cnBySk5MJDAwkIiLC0U1yKL2PXlxx6XHWGurVPaHubH0jIQF++620pnpcnP1969YtrafeqxdERlZdO2sCZ+sb4kSqQfytJHol6MOMc9DNViqi/nFhkrYlWeunZ53IKrM9qFEQ7e5uR7u72hHcJNgBLfzn1DfEHvUNsasaBPGXu4peh+LiYvbt20doaCghIZfvnB7OQEn08qWkpJCcnEyzZs1qdGkXvY9eOpVJqLcNbWupoe6ECXVn7htmM+zfbztJ6enT9vdv2bI0oX7NNRAYWFUtvTw5c98QB6sG8beS6JWgDzPOQTdbqYj6x8VhKjZxeNlhtn28jd3f7KYwp2yd2bpd69J+WHtaD26NV7CXA1p5ftQ3xB71DbGrGgTxl7uKXoe8vDwOHz5Mw4YN8fJy/veh6kxJ9PLl5uZy5MgRoqKi8PT0dHRzHEbvo1WjOibUq1PfKC6GLVtKk+orV0Jubvn7Go0QE1OaVO/eHWrwLeAfqU59Q6pYNYi/lUSvBH2YcQ662UpF1D8uvoKsAnZ/s5ttH2/j0LJDcNa7hYu7i7V+etMbmjpt/XT1DbFHfUPsqgZB/OWuMkn0mp7ArApKopdPfdBC76NV73wS6oNbD6Z5reZV3EKL6tw38vNh7drSSUrXr7ck2svj4VE6SWmvXtCpkyYpPZfq3DfkEqsG8bdzZjxERMTh3H3daT+sPe2HtSfjeAbbF25n28fbSN6RDEBxQTF7Fu9hz+I9eAV7Weun1+1SVx+0RUREqkjDhg0ZN24c48aNc3RTROQSqxdQj0e7PsqjXR+1JtS/3PUla4+vte6zPXk725O389yK52gb2pbBrQczqNUghyXUqxsPD+jRw7JMmwYZGfDHH6VJ9R07SvfNzy8dwQ4QEFA6SWmvXtCihSYpFbmcKIkuIiLn5F/Xn+5PdqfbE91I2prE1gVb2b5wO9lJ2QDkns5l46yNbJy1keAmwdb66UGNghzcchEREedwri+YJ0+ezJQpU877uBs2bMDHx+cftsrimmuuoUOHDkyfPv2CjiMiVefMhPqx9GPWEerlJdSfXf4s7cLaWUq+KKF+Xvz94aabLAtAUlLpJKW//gpHj5bum54O335rWcAyKem111oS6l27QpMmGqkuUp0piS4iIpVmMBgI7xBOeIdwrnv1Og79eshSP33xbopyiwA4feA0KyavYMXkFdS/sj7t7m5Hq0Gt8ApS3VoRqSaMRmjThqLsbNz1U2O5SE6cOGH9+4svvuC5555j79691nW+vr7Wv81mM8XFxbi6nvvjWu3atS9uQ0Wk2qkfUJ/xXcczvut4uwn1bUnb2Ja0TQn1CxQWBnfcYVnMZjh0qHQ0+m+/walTpfsmJMAnn1gWAC8vaNMG2rcvXdq21WSlIkC1iL+ds1UiIuL0jK5GmvRtwq2f3srjiY9z87ybibo2Cs4YaHfsz2N89+/veD38dRYNWsTeJXspLrBTVFBExFm4u2N+6SWynn4a3N0d3Rq5TISHh1uXgIAAyxfTfz/es2cPfn5+/PDDD3Tq1AkPDw/+/PNPDh48yM0330xYWBi+vr7Exsby66+/2hy3YcOGNiPIDQYD77//Prfccgve3t40bdqUJUuWXFDbv/76a1q3bo2HhwcNGzbk9ddft9k+c+ZMmjZtiqenJ2FhYdx2223WbV999RVt27bFy8uLkJAQevfuTXZ29gW1R0TsK0morxmxhqPjjvL69a/TpW4Xm31Kkukt3m1B+9nteeGPF9h7aq+dI4o9BgM0bgyjRsEXX1hGqf/1F/znP9CvH5z9I6HcXNiwAd5/H8aMgauvhqAgaNgQbr4ZnnsOvv4a9u8Hk8khlyTiONUg/tZIdBERuWAe/h50GN6BDsM7kB6XzvZPLfXTT+46CVjqp+/6ahe7vtqFV4gXbe5oQ/u72xMZG6n66SIiIn+bOHEi//nPf2jUqBFBQUHExcVxww038OKLL+Lh4cGCBQvo378/e/fupX79+naPM3XqVF599VVee+013n77bYYOHcrRo0cJDg4+7zZt3ryZ22+/nSlTpnD77bezevVqHnroIUJCQhg+fDgbN25k7NixfPzxx3Tr1o3Tp0+zcuVKwDL6/o477uDVV1/llltuITMzk5UrV2I2m89xVhG5GP7pCPXBrQfTLKSZA1tePRmN0KGDZXnsMSgogHXr4PffYcsW2LYNDhywjGA/09GjluXM7zt9fCyj1EtGrLdrZ1n8/KrwgkTEhpLoIiJyUQXUC+DKiVfSfUJ3Tmw+wbaPt7Hjsx1kJ/9dPz0llw3vbGDDOxsIaRZirZ8e2DDQsQ0XEZFqLSYGEhOr/rzh4bBx48U51vPPP891111nfRwcHEz79u2tj6dNm8bixYtZsmQJDz/8sN3jDB8+nDvuuAOAl156iRkzZrB+/Xr69u173m2aPn06vXr14tlnnwWgWbNm7Nq1i9dee43hw4dz7NgxfHx8uOmmm/Dz86NBgwZER0cDliR6UVERt956Kw0aNACgbdu2590GEblw5SXUv9z5Jevi11n3OTuhPrjVYAa1HqSE+j/k7g5XXWVZSmRlWSYn3brVsmzbZlkyM22fm50Na9daljM1amRbDqZdO4iK0gSmIlVBSXQREbkkDAYDkZ0iiewUyXWvXcehXw6xdcFW9v53L0V5lvrpKftSWP7scpY/u5wGVzew1k/3DPB0cOtFpEbLy8Nw330E5OVZCpl6ezu6RVIJiYkQH+/oVlyYmJgYm8dZWVlMmTKFpUuXWhPSubm5HDt2rMLjtGvXzvq3j48P/v7+JCcn/6M27dmzh5tvvtlmXffu3Zk+fTrFxcVcd911NGjQgEaNGtG3b1/69u1rLSXTvn17evXqRdu2benTpw/XX389t912G0FBmnhcxJHOJ6H+zPJnaB/W3lJDXQn1C+brC126WJYSJhMcOVKaWC9Jrh86VPb5hw5ZlsWLS9f5+VmS6Wcm1tu2LVtORsSpVYP4W0l0ERG55FzcXGh6Q1Oa3tCUvPQ8dn+9m20fb+PIiiPWfY7+cZSjfxzlhzE/0PxfzWl3dzsa92mMi5umsBcRB8jIwFBQ4OhWyHkID6/+5/U5K+Px+OOP88svv/Cf//yHJk2a4OXlxW233UbBOfqmm5ubzWODwYDpEhXY9fPzY/PmzaxYsYKff/6Z5557jilTprBhwwYCAwP55ZdfWL16NT///DNvv/02Tz/9NOvWrSMqKuqStEdEzs+ZCfWjaUetJV/OTKhvTdrK1qStSqhfIkajZYR5o0Zwyy2l6zMyYPt228T69u2WUepnysyEVassSwmDAZo0sU2st28P9etr1Lo4MSePv5VEFxGRKuUZ4En0fdFE3xdN2tE0a/30U3ssU9kX5RWx88ud7PxyJ961va310yM6Rah+uoiI2HWxSqo4k1WrVjF8+HBu+TurkpWVxZEjR6q0DS1atGD16tVl2tWsWTNcXCxfdLu6utK7d2969+7N5MmTCQwM5LfffuPWW2/FYDDQvXt3unfvznPPPUeDBg1YvHgx48ePr9LrEJFzaxDYgMe6PcZj3R5TQt0J+PtD9+6WpYTJBAcP2ibWt2611FQ/k9lsmaB0/3746qvS9YGBpQn1kn/btAEvryq5JJFqTUl0ERFxmMAGgVz11FVcOelKEjYmWOun55zKASDnZA7rZ6xn/Yz11GpRi3bD2tFuaDsC6gc4uOUiIiKXXtOmTfnmm2/o378/BoOBZ5999pKNKD958iRbtmyxWRceHs6jjz5K165dmTZtGrfffjtr1qzhnXfeYebMmQB89913HDp0iKuvvpqgoCC+//57TCYTzZs3Z926dSxbtozrr7+e0NBQ1q1bx8mTJ2nZsuUluQYRuXj+SUJ9cOvBDGo1iMZBjR3Y8sub0QhNm1qW224rXZ+WVppQP3PUel6e7fPT0uCPPyzLmcds1sw2sd6+PdSpo1HrImdSEl1ERBzOYDBQJ7YOdWLrcP3r13Pwp4OW+ulL9lKcXwzAqT2n+O2p3/jt6d9o2KMh7Ya1o9XAVnj4ezi49SIiIpfGG2+8wX333Ue3bt2oVasWEyZMICMj45Kca+HChSxcuNBm3fPPP8/EiRP54osvmDx5MtOmTSMiIoLnn3+e4cOHAxAYGMg333zDlClTyMvLo2nTpnz22We0bt2a3bt388cffzB9+nQyMjJo0KABr7/+Ov369bsk1yAil0Z5CfUvd33J+vj11n1KEupP//Y07cPa07tub65vcT1d6nXB38Pfga2vGQID4eqrLUuJ4mLLSPSzR60fP277XJMJ9uyxLF98Ubo+OLhsYr1VK/DU9FVSQxnMZrPZ0Y1wdhkZGQQEBJCeno6/v27+jmIymUhOTiY0NBSj0ejo5oiTUf+4POWl5bHrq11sXbCVYyvLTqLm6ulKiwEtaDesHY2va4zRtex/e/UNsUd9Q+zKy8N8223kFxTg/u23GKtwYiPFnRYVvQ55eXkcPnyYqKgoPPVJ/pIym80UFRXh6uqqkmpnUB+00Puo2Euon8mAgTahbehatytd63Wla92uNAtppnuKA6WklCbUS/7duRPy88/9XBcXaNGibHI9PLxyo9Z13xC7qkH8rZHoIiLitDwDPel4f0c63t+R1MOpbP90O1sXbOX0/tOApX76js93sOPzHfiE+Vjqpw9rT3iHcAXmIiIiIiKX0Jkj1I+kHbGWfDkzoW7GzPbk7WxP3s6czXMACPEKoUvdLtbE+hV1rsDX3ddRl1HjhIRAz56WpURREezdW3bU+okTts8tLrYk3HfuhDN/vFS7dtnEesuW4O5eNdckUhWURBcRkWohKCqIq5+5mquevor49fGW+umf7yA3JReA7KRs1k1fx7rp66jdujbt7rbUT/eNVEAuIufp74KjxTk5lr9FRESkQg0DG/J4t8d5vNvjHEk9wnfbv2NX5i7WHl/LtqRtFJuLrfum5KawdP9Slu5fCoDRYKRtaFu61etmTaw3DmqsQTFVyNUVWre2LHfeWbr+5MnSxHpJcn3XLigstH3+yZPw66+WpYSbmyWRfmZivW3bqrkeqYaqQfytci6VoJ/VOgf97Ecqov5RMxUXFLP/h/1s+3gb+/63j+KCYtsdDBB1bRR1+9SlVd9WhLYKxeii/iEWum9IRRzVPxR3Wqici3NQOZfyqQ9a6H1U7Dm7b2QVZLEhfgNrjq+xLHFrSMlNqfAYtb1rW0erd6vXjZjIGHzcfaroCqQiBQWW+ulnj1pPTq7c82vXLqZ5cyONGxto0gSaNIHGjS3/BgVd2raLc3P2+FtJ9Eqo7ItZXFxM4dlfx8lFYzKZSElJISQkREHa39zc3HBxcXF0M5yCgnjJTc1l55c72fbxNuJWxZW7j7ufu2UC086WpW7nuviGa6R6TaX7hlTE2YP4y52S6M5BSfTyqQ9a6H1U7DlX3zCbzRw4fYDVcautifUdyTswmU12j+licKF9eHvLSPW/R6tHBUbp3uREEhPLJtZ377aUgKms4GDbpPqZf4eGVq7uulRfzh5/K4leCed6Mc1mM4mJiaSlpVV942oQs9mMyWTCaDTqjfIMgYGBhIer/rOCeDlT6qFUtn2yja0LtpJ6MLXCfQMaBFC3c11rYj2iYwRuXm5V1FJxJN03pCLOHsRf7pREdw5KopdPfdBC76Nizz/pG5n5mayPX8+a42tYHbeatcfXkppXcRwf5hNmnay0a92uxETG4OXmdTEuQS6S/HxL+ZfSxLqZ7dtNnDx5/oMBfX3tJ9jr1HHaCiByHpw9/lYSvRLO9WKeOHGCtLQ0QkND8fb2VoB5iSiIt2U2m8nJySE5OZnAwEAiIiIc3SSHUhAv5TGbzRxfe5yd3+8kfWc68evjyYzPrPA5RlcjYe3CqNOljjW5HtI0BINR953Lje4bYld+PuYHHyQ3Lw/PDz7A6FV1H8iVRLdQEt05KP4un/qghd5HxZ6L0TdMZhP7UvaxJm6NdbT6zuSdmLGfwnI1utIhvAPd6nazJtfrB9TX/cuJlPQNb+9QDh82cvAgHDhgWUr+jouD881UenhAo0alyfUzE+wNGlhqvouTqwbxt7rRBSouLrYm0ENCQhzdnMuagviyvP6+qZQEKCrtImLLYDBQp3Md3KLcrEF8RnwG8eviOb72OPHr4knYmEBhTmkpLlORiRObT3Bi8wk2ztwIgGegJ3WuqFOaWL+iDt61vB11WSJyqZnNkJyMsaDg/D/FiYiIyAUzGoy0qNWCFrVacG/0vQCk56WzLn6dNbG+9vha0vPTrc8pMhWxMWEjGxM2MmP9DAAifCNsRqt3iuyEp2vN/fLLWfj6lk42era8PDhypDS5fmaC/cgRKCoq+5z8fEvpmN27y25zdbUk0stLsEdFQQ3+LtS5VIP4W0n0C1RSA93bW8kUcYySvldYWKgkukgl+Nfxx/9Wf1re2hKwJM2TdyZbEuvrjhO/Np6Tu09y5iCXvLQ8Dv58kIM/H7SuC2ocZB2pXrdLXcLah+HqobdVEREREZFLIcAzgOsbX8/1ja8HLKPVd5/cbZ2sdM3xNew+ZZtFPZF1gm92f8M3u78BwM3oRseIjta66l3rdqVeQL0qvxaxz9MTWrSwLGcrKoJjx8pPsB88aEmml/ecgwcty08/2W4zGKBu3bLJ9ZK/fTV9lpxBn/YvEo2MFkdR3xO5MEZXI+HtwwlvH06nUZ0AyM/IJ35DPPHr4q2j1rOTs22el3owldSDqWxfuB0AF3cXwqPDrROW1ulch6BGQfp/VERERETkEjAajLQObU3r0Nbc3/F+AFJzU1l7fK21BMy64+vILCgt51hoKmRd/DrWxa9j+rrpANT1r2szYWl0eDQerh6OuCQ5B1dXS9mWRo3g+uttt5lMkJBQNrlesmRllT2e2WwpHxMXB8uXl90eFmY/wR4cfGmuUZyXkuhyQa655ho6dOjA9OnTHd2UMqZMmcK3337Lli1bLsrxjhw5QlRUFH/99RcdOnS4KMcUEefk4e9Bo16NaNSrEWApJ5V+NN0yUv3vxPqJzScoyiv9LWFxQbF123rWA+Bdy9s6YWlJGRjPQP1eUERE/jlnjr9FRBwtyCuIfk370a9pPwCKTcXsOrnLOmHpmuNr2Jeyz+Y5xzOOs2jXIhbtWgSAh4sHHSM60q1eN2tiPdIvssqvRc6P0WgZVV63Llxzje02sxlOnrSfYD99uvxjJiVZllWrym4LCrKfYA8Ls4xyl8uLkug1VP/+/SksLOTHH38ss23lypVcffXVbN26lXbt2l3QeebPn8+991rqlxkMBiIjI7nuuut45ZVXCA0NvaBjV7V69epx4sQJatWqBcCKFSvo2bMnqampBAYGOrZxInJJGQwGAhsGEtgwkDa3twEsSfOkbUk2ifWUfSk2z8s5lcP+pfvZv3S/dV2tFrVsEuuhbUNxcVMpJhGRy11Vxt/jxo0jLS3tgo4jInI5cDG60DasLW3D2jKq0ygAUnJSyoxWzy4s/dVpfnG+dVuJ+gH1raPVu9XrRvvw9ri7uFf59cg/YzBAaKhl6dat7PbUVNuyMGcm2BMTyz9maips2GBZzubjUza5XvK4bl1Lwl+qHyXRa6gRI0YwcOBAjh8/Tt26dW22zZs3j5iYmAsO4Ev4+/uzd+9eTCYTW7du5d577yUhIYGfzi5GVUmFhYW4u1f9m5WLiwvh4eFVfl4RcU4u7i5ExkQSGRMJoy3rck/nEr+hdNLS+HXx5J7OtXneqT2nOLXnFFs/2gqAq5crER0jqNulrjWx7l/PX2VgREQuM1UZf4uIiH0h3iHc2OxGbmx2I2CZkHRH8g5rXfU1x9dw4PQBm+ccSz/GsfRjfLHzCwA8XT2JiYyxKQMT7qt8QXUVFAQxMZblbFlZcOhQ2QT7wYOW+uzlzYGZnQ1bt1qWs3l4WMrRlCTYGzaEOnVKl4gIcHO76JcoF4G++6ihbrrpJmrXrs38+fNt1mdlZbFo0SJGjBhBSkoKd9xxB3Xq1MHb25u2bdvy2Wefnfe5DAYD4eHhREZG0q9fP8aOHcuvv/5Kbq4lsfT+++/TsmVLPD09adGiBTNnzrQ+98iRIxgMBr744guuueYa/Pz8+PTTT5k/fz6BgYF8++23NG3aFE9PT/r06UNcXFyFbanoXPfddx/t2rUj/++ZKAoKCoiOjmbYsGE2bdmyZQtHjhyhZ8+eAAQFWWoeDx8+nAULFhASEmI9RokBAwZw9913n/drJyLVi1ewF036NOGaydcw9PuhPHHqCcbsH8MtH99C7MOxRMZGYnSzfestyi0iblUca15fw1eDv2J6g+m8EfkGnw/4nJUvr+Tw8sPkZ5YzQ46IXDoGA9SrhykyUr/FlYumKuPvihw7doybb74ZX19f/P39GTx4MElJSdbtW7dupWfPnvj5+REQEEDnzp3ZuHEjAEePHqV///4EBQXh4+ND69at+f777y9q+0REqpqr0ZUO4R14MPZBFtyygP1j9pP0eBL/HfJfJnafSI8GPfB287Z5Tl5RHn8e+5PXVr/GrV/eSsTrEUS9FcXQb4byzvp32JSwicLiQgddkVxMvr7Qrh3ceis88QS89x4sWwZHjkBuLuzeDf/7H0yfDg8/DH37QtOmlvrt5cnPtzznu+8szxk3DgYNsoyQb9DAkmQPD7ck9G++GR56CF58EebPh19+gV27ID29/OR9tVYN4m+NRK+hXF1dGTZsGPPnz+fpp5+2jnhctGgRxcXF3HHHHWRlZdGpUycmTJiAv78/S5cu5e6776Zx48ZcccUV//jcXl5emEwmioqK+PTTT3nuued45513iI6O5q+//mLkyJH4+Phwzz33WJ8zceJE/vOf/9C2bVt8fX35+eefycnJ4cUXX2TBggW4u7vz0EMPMWTIEFaVV6wKznmuGTNm0L59eyZOnMibb77J008/TVpaGu+8806ZY9WrV4+vv/6agQMHsnfvXvz9/fHy8sLd3Z2xY8eyZMkSBg0aBEBycjJLly7l559//sevmYhUTwaDgeAmwQQ3CabdXZbRhUV5RSRuSbSWgTm+9jhph9NsnpeVmMXe/+5l73/3/n0gCG0dalMGpnbr2hhd9F24yCXh4YH53XfJSE7G00MTi8nF4cj4u4TJZLIm0H///XeKiooYPXo0t99+OytWrABg6NChREdHM2vWLIxGI5s2bcLt7yFxo0ePpqCggD/++AMfHx927dqFr6/vBbdLRMTZhPqE8q/m/+Jfzf8FQGFxIduTt1vrqq+JW8PhtMM2zzmSdoQjaUdYuH0hAF6uXsTWiaVr3a60D2tPi1otaBbSDB93nyq/Hrk0PDygRQvLcraiIstI9fJGsB88CHl55R/TbC6txb5pk/1z+/iUjl6vW9d2NHvJEh4OLtWlcmg1iL+VRL9E5sTMISuxnKl/LzHfcF9GbRxVqX3vu+8+XnvtNX7//Xeu+XvWhXnz5jFw4EACAgIICAjg8ccft+4/ZswYfvrpJ7788st/HMTv37+f2bNnExMTg5+fH5MnT+b111/n1ltvBSAqKopdu3bx3nvv2STRx40bx6233kpRURGuf3+dV1hYyDvvvEPnzp0B+Oijj2jZsiXr168vt33nOpevry+ffPIJPXr0wM/Pj+nTp7N8+XL8/f3LHMvFxYXgv6diDg0NtamJfueddzJv3jxrEv2TTz6hfv361tdYRGo2V09X6napS90upT/lz07OJn59vE199fyMM0afmyF5RzLJO5L564O/AHD3dScyJtImse4X6VfVlyMi4jRi5sSQmGWncOklFO4bzsZRGyu1ryPi7zMtW7aM7du3c/jwYerVqwfAggULaN26NRs2bCA2NpZjx47xxBNP0KJFC8xmM1FRUdb4+9ixYwwcOJC2bdsC0KhRowtuk4hIdeDm4kbHiI50jOjIw1c8DEBiVqKltnrcGlYfX83GhI3kFZVmRnOLcvnj6B/8cfQPm2PV869Hi1otaB7SnBa1Wlj+rtWcOn51VNLxMuLqainbUt5bpckECQmWpHpcHMTHly7Hj1v+TUy07GdPdjbs22dZ7DEaLYn0MxPr5SXc9X145SiJfolkJWaRGZ/p6GZUqEWLFnTr1o0PP/yQa665hgMHDrBy5Uqef/55AIqLi3nppZf48ssviY+Pp6CggPz8fLy9vc9xZFvp6en4+vpiMpnIy8vjyiuv5P333yc7O5uDBw8yYsQIRo4cad2/qKiIgIAAm2PElFOYytXVldjYWJvrCQwMZPfu3WU+ZFT2XF27duXxxx9n2rRpTJgwgSuvvPK8rhVg5MiRxMbGEh8fT506dZg/fz7Dhw/Xm6GI2OUT6kOzm5rR7KZmAJhNZk7tPWUZqf53Yj1pWxLm4tLf7BVkFXBkxRGOrDhiXedfz5+6netaE+uRnSJx81ZBPRGpGRKzEonPjHd0MypUVfG3Pbt376ZevXrWBDpAq1atrDF0bGws48eP5/777+fjjz+mV69e3HLLLTRv3hyAsWPH8uCDD/Lzzz/Tu3dvBg4cqDruIlJjhfuGM6DFAAa0GABAQXEBWxO3Wuuqr4lbw9H0o2WeF5cRR1xGHL8c+sVmva+7L81CmlkS6yGWxHqLWi1oGtwULzevqrgkqSJGoyWZfdYUKTaKiiyj0c9MsJeXbM/Otn+MkmR9QkL5E6CW8PcvfyT7mUn30FBNiKok+iXiG+6Yr3HO97wjRoxgzJgxvPvuu8ybN4/GjRvTo0cPAF577TXeeustpk+fTtu2bfHx8WHcuHEUFBSc1zn8/PzYvHkzRqORiIgIvLwsN/+S2otz5861jiYv4XLW7018fC7s505ZWVmVOpfJZGLVqlW4uLhw4IDtRCKVFR0dTfv27VmwYAHXX389O3fuZOnSpf+88SJS4xiMBmq3rE3tlrXpMLwDAIU5hSRsSrCOVD++7jgZcRk2z8uIy2BX3C52fbXLchwXA2Htwqwj1et0rkOt5rUwGPWlnkiF8vMxjBuHf24uzJoFXvrgWh04akK38z1vVcTfF2LKlCnceeedLF26lB9++IEpU6bw2Wefceutt3L//ffTp08fa6nCl19+mddff50xY8ZUWftERJyVu4s7sXViia0Ty9jOYwFIyExgffx69pzaw55Te9ibspc9p/aQlpdW5vlZBVlsPrGZzSc226w3YKBBYAPbket//x3uG64Be5cpV9fSRLY9ZjNkZNhPtJck25OTK66hnpFhWXbvrrg9ERH2k+0lyz/+3r8axN9Kol8ilS2p4miDBw/mkUceYeHChSxYsIAHH3zQegNetWoVN998M3fddRdgSTDv27ePVq1andc5jEYjTZo0KbM+LCyMyMhIDh06xNChQ8+77UVFRWzcuNE66nzv3r2kpaXRsmXLf3yu1157jT179vD777/Tp08f5s2bx7333lvuvu7u7oBlxNDZ7r//fqZPn058fDy9e/e2Ge0jIvJPuHm70eCqBjS4qoF1XWZCpk0JmPgN8RRml05gZC42k/hXIol/JbJptqWgnkeAB7Vb1iaocRBBjYMsNdsbBxPUOAifUB8F4SJg+ZQRF4exoOAynLXp8lXZkiqOVhXxtz0tW7YkLi6OuLg4a3y6a9cu0tLSbM7RrFkzmjVrxrhx4xgyZAjz58+3lkSsV68eDzzwAA888ACTJk1i7ty5SqKLiNgR6RdpHalewmw2k5ydbE2o7z21lz0pln8Ppx3GZLat4WHGbK23/tPBn2y2+bn7WcvBtAgpLQ3TJLgJnq6el/ryxMEMBggIsCwVhQqFhXDixLmT7fbqtINlZHxcnGWpSFDQuRPttWqVM6q9GsTfSqLXcL6+vtx+++1MmjSJjIwMhg8fbt3WtGlTvvrqK1avXk1QUBBvvPEGSUlJFy2IB5g6dSpjx44lICCAvn37kp+fz8aNG0lNTWX8+PEVPtfNzY0xY8YwY8YMXF1defjhh+nSpYvdepHnOtdff/3Fc889x1dffUX37t154403eOSRR+jRo0e59R4bNGiAwWDgu+++44YbbsDLy8s6sdKdd97J448/zty5c1mwYMGFv1AiIuXwi/Sj5S0taXmL5ctDU7GJk7tOcnxtaWI9eWcynBGD5Kfnc3ztcY6vPV7meO6+7gQ1Cio3wR5QLwCjaw3//Z6IyEVQFfF3cXExW7ZssVnn4eFB7969adu2LUOHDmX69OkUFRXx0EMP0aNHD2JiYsjNzeWJJ57gtttuIyoqiri4ODZt2mRNoI8bN45+/frRrFkzUlNTWb58ebkDWERExD6DwUCYbxhhvmFc3eBqm215RXkcOH3Aklg/Y+T63pS9ZORnlDlWZkEmGxI2sCHBtlaH0WCkYWDDsrXXQ5oT6hOqgTM1jJsb1K9vWewxmyE1teJEe3w8nDxZ8blSUy3Ljh3293F3h8hI28R6/VAYlAAuLgaCi8D9n13qJaUkujBixAg++OADbrjhBiIjI63rn3nmGQ4dOkSfPn3w9vZm1KhRDBgwgPT09It27vvvvx9vb29ee+01nnjiCXx8fGjbti3jxo0753O9vb2ZMGECd955J/Hx8Vx11VV88MEH/+hceXl53HXXXQwfPpz+/fsDMGrUKJYuXcrdd9/NH3/8UeZ4derUYerUqUycOJF7772XYcOGMX/+fAACAgIYOHAgS5cuZcCAAf/kpREROW9GFyNhbcMIaxtGp5GdAMjPzCdho6UMzPG1x0nYmGB3zo6CrAKStiWRtC2p7LFdjQQ2DCxNsDcOJriJJcEe1CgINy/VXhe5nMXHxzNhwgR++OEHcnJyaNKkCfPmzSt33poSK1asYPz48ezcuZN69erxzDPP2CSMa7JLHX9nZWURHR1ts65x48YcOHCA//73v4wZM4arr74ao9FI3759efvttwFLmcOUlBSGDRtGUlIStWrVYsCAAUydOhWwJOdHjx7N8ePH8ff3p2/fvrz55psX+GqIiEgJT1dP2oS2oU1oG5v1ZrOZxKzEMon1Paf2cDTtKGZsR+6azCYOpR7iUOohvt//vc22QM9Aa2L9zAR74+DGuLs4Y+pSqoLBAMHBluXv+cPLlZ9vqbF+rmR7RZXoCgrgyBHLUsIDaIwBcOe6CiZUdSSD2eykY+SdSEZGBgEBAaSnp+Pv72+zLS8vj8OHDxMVFYWnp34qcymZzWaKiopwdXXlo48+Yty4caSlpTm6WXb16tWL1q1bM2PGjEt6HvVBC5PJRHJyMqGhoRhr+mwXYkN9o6zCnEJSD6eSejCV0wdOc/rgaVIPWh6nHUnDVHT+UYtfpJ81uX72SHavYOerZwfqG1KBvDzMt91GfkEB7t9+i/EiTepYGRXFnY6SmppKdHQ0PXv25MEHH6R27drs37+fxo0b07hx43Kfc/jwYdq0acMDDzzA/fffz7Jlyxg3bhxLly6lT58+5zyn4m/ncGb8rVGLpdQHLfQ+Kvaob1St3MJc9p/eX6Y0zJ5Te8gurGDWybO4GFxoFNSoTGmYFrVaUMu71kVpq/pGzWA2w6lT5060nz5d+hwP8ljEINzdzFyX9qVTxt8aiS5ykaWmprJixQpWrFjBzJkzHd0cEZEy3LzdCG0dSmjr0DLbTEUm0uPSLQn2v5Prpw+ctj4+s+b6mTITMslMyOTYymNltnkGepZNsP89kt0v0k8TnYo4uVdeeYV69eoxb94867qoqKgKnzN79myioqJ4/fXXAUst7j///JM333yzUkl0ERERqRwvNy/ahbWjXVg7m/Vms5mEzIQyk5ruTdnLsfSyMXuxuZj9p/ez//R+vuM7m23BXsHlloZpFNQINxf9IlVsGQxQu7Zl6dDB/n65uaUJ9ROHoeWrZoqKys476CyURBe5yKKjo0lNTeWVV16hefPmjm6OiMh5MboaCYoKIigqiEa9beeDMJvNZCdn2yTYS/4+feA0OSdzyj1mXloeJzad4MSmE2W2uXi4ENSobII9qLGlDS7uLpfkOkWk8pYsWUKfPn0YNGgQv//+O3Xq1OGhhx5i5MiRdp+zZs0aevfubbOuT58+dkv25efnk5+fb32ckWGp+2oymTCZbH8dYzKZMJvN1kUurZLXWK91qZK+V17/rElK/l+sya+BlE99w3lE+EYQ4RtBz4Y9bdZnF2SXjl5P2WtZTu1l3+l95BSWjelP555mddxqVsettlnvanSlcVBjmoc0tyy1SsvEBHsFlzmO+oacycMDGjWyLMSa4CsoyC+y9I8q7COV7Y/VKon+f//3f0yaNIlHHnmE6dOnA5af0j322GN8/vnn5Ofn06dPH2bOnElYWJj1eceOHePBBx9k+fLl+Pr6cs899/Dyyy/j6lqtLl/OMHz4cKetqXnkzKJOIiKXEYPBgG+YL75hvtTrVq/M9vzMfNsR7Gck2tOPpWM2lU3AFOcXc2r3KU7tPlX2fEYD/vX8y02wBzcOxsPf45JcpwgGA4SGYsrLs/xdwx06dIhZs2Yxfvx4nnrqKTZs2MDYsWNxd3fnnnvuKfc5iYmJNvE4QFhYGBkZGeTm5uLlZVvm6eWXX7bW3T7TyZMnycvLs1lXWFiIyWSiqKiIoqKiC7w6qYjZbKa42DIiTOVcShUVWT7gp6Sk4OZWc0dgmkwm0tPTMZvNKssgNtQ3qodIYySRoZFcG3qtdZ3JbCIhK4GD6Qc5kHaAA6kHrH+fyC47IKbIVGRNwp8txDOExoGNaRLYxLo08m+Ev8lffUPKys/Hz8+PQg8Pck6exOhVdSVBMzPLnzPsbNUmi7xhwwbee+892rWz/XnKo48+ytKlS1m0aBEBAQE8/PDD3HrrraxatQqwTH5z4403Eh4ezurVqzlx4gTDhg3Dzc2Nl156yRGXIiIiclny8PMgvEM44R3Cy2wrLigm7UhauQn21EOpFOWVTYSZTWbSj6aTfjSdw78dLrPdu7a33QS7T5iPEj7yz3l4YH7/fTKSk/H00Jc1JpOJmJgYa+wcHR3Njh07mD17tt0k+vmaNGkS48ePtz7OyMigXr161K5du9ya6JmZmbi6umpQTBWpyYni8ri6umI0GgkJCanxNdENBgO1a9dWMkxsqG9Ub+Fh4XSkY5n1mfmZ7Du9j72n9pYZvZ5XlFdm/5S8FFISU1ifuN5mvavBlbr+dakfUJ96AfVoENCAegH1qO9fn/oBlsXX3feSXZ84L9PHH5Nx8iShVXzvqOx7ebWIOrOyshg6dChz587lhRdesK5PT0/ngw8+YOHChVx7reWbs3nz5tGyZUvWrl1Lly5d+Pnnn9m1axe//vorYWFhdOjQgWnTpjFhwgSmTJmCu7tmHhYREbnUXNxdCGkWQkizkDLbzCYzmQmZ5SbYTx88TV5q2aAcIOdkDjknczi+9niZbW4+bnYT7AH1AzC66gOdSGVFRETQqlUrm3UtW7bk66+/tvuc8PBwkpKSbNYlJSXh7+9fZhQ6gIeHBx7lfGFhNBrLfIgyGo0YDAbrIpeO2Wy2vsZ6rUuV9L3y+mdNo9dB7FHfuPwEeAUQWyeW2DqxNutNZhPH0o+VTmx6Rv31E1nljF43F3Ek/QhH0o/YPVeQZ5A1od4goIH175Il3DccF6PKPl6OHHHvqOy5qkUSffTo0dx444307t3bJom+adMmCgsLbeottmjRgvr167NmzRq6dOnCmjVraNu2rc3PSfv06cODDz7Izp07iY6OLnM+1WR0XqrJWJZqMlqotprYo75RPfhG+uIb6Uv9q+qX2Zabmls6wemh0tHrqQdTyYwv/6d3hdmFJG1LImlbUpltRlcjAQ0CCGochEe4B7Ub1cY/0h/fCF98I3zxi/DDu7Y3Rhd96KvJHHXvcMZ7Vffu3dm71/Zn2vv27aNBgwZ2n9O1a1e+//57m3W//PILXbt2vSRtFBEREccwGow0DGxIw8CG9G3S12Zbel46+1L2WRPru0/uZu/JvSRkJ5Cal2r3mKl5qaTmpbI1aWu5212NpaPZy0uyazS7XApOn0T//PPP2bx5Mxs2bCizLTExEXd3dwIDA23Wh4WFkZiYaN2nvHqMJdvKo5qMzkk1GcunmowWqrsn9qhvXB5c67sSWj+U0GtDbdYX5RaReSyT9KPpZBzOIONoBhlHLP9mxmViKiybkDQVmawj3QF2s7vMPgYXA161vfAO9cY7zNv6r0+oD97hpeu8anvh4qZRMJedggJ8X3gB94ICkqdMwViF5RoqW5OxKj366KN069aNl156icGDB7N+/XrmzJnDnDlzrPtMmjSJ+Ph4FixYAMADDzzAO++8w5NPPsl9993Hb7/9xpdffsnSpUsddRkiIiJSxQI8bUevm0wmkpOTCQ0NJbswm7iMOI6lH7NZjqYf5Vj6MY5nHKfIVH6erchUxJG0IxxJO2L33EGeQTQI/DvB7l82ya7R7E6moADDhAn45eTAW2+BE5ZLc+okelxcHI888gi//PJLldaaU01G51aTE8XlUU1GC9XdE3vUN2qABsBVZVebik1kxGWUloY5dJrUA6Wj2AuyCuwe0lxsJicxh5zEnIrPbQDvWt7WEexnjmb3Df/770jL366eihOqjbw8SEjANT+fgFq1MHp7V9mpnfG9PDY2lsWLFzNp0iSef/55oqKimD59OkOHDrXuc+LECY4dO2Z9HBUVxdKlS3n00Ud56623qFu3Lu+//z59+vRxxCWIiIiIk/Hz8KNV7Va0qt2q3O3FpmISsxLtJtmPpR8792j2xFS2JG4pd3vJaHZ7I9k1mr2KmUywfz8uBQWWv52QU3+a27RpE8nJyXTsWDqhQXFxMX/88QfvvPMOP/30EwUFBaSlpdmMRk9KSiI83DKpWXh4OOvX205iUFKfsWSfs6kmo3NSTcbyqSZjKb0OYo/6Rs1kNBoJbhRMcKNguM52m9lsJispi8NbDuOW50Z2YjaZJzLJOpFF1oms0r8TszCbKighZi6tzZ68LbnC9ngGeVoT7X4RfvhG+to+/vtfd1/N1+JwRiNmACeuyVjVbrrpJm666Sa72+fPn19m3TXXXMNff/11CVslIiIilysXowt1/OtQx78OXeuVXw4uMz+zzGj2M5PsFzqaPdgruDSprtHsNZ5TJ9F79erF9u3bbdbde++9tGjRggkTJlCvXj3c3NxYtmwZAwcOBGDv3r0cO3bMWm+xa9euvPjii9afi4ClHqO/v3+ZCZJEGjZsyLhx4xg3btx5PS8lJYWWLVuyfv16GjZseNHaM3HiRLKzs3n77bcv2jFFRMTyxYpPqA+hHUIJDQ21m7g0FZvIOZljTapb/0046/GJ8kvHnCkvNY+81DxO7jpZ4X7ufu5lkuslI9rPXO8R4KEvlUWk2lP8LSIi/9Q/Gc1+ZpL9XKPZT+ee5nTuabuj2d2Mbtba7BrNfvlz6iS6n58fbdq0sVnn4+NDSEiIdf2IESMYP348wcHB+Pv7M2bMGLp27UqXLl0AuP7662nVqhV33303r776KomJiTzzzDOMHj263NHmNcnw4cP56KOPrI+Dg4OJjY3l1VdfpV27dhflHFOmTOHbb79ly5Yt59yvvDr0zZs3Z8+ePRelLZfSiy++yM0332wTwI8dO5ZVq1axY8cOWrZsWe5rsG3bNkaPHs2GDRuoXbs2Y8aM4cknn7Ruf/zxx2nUqBGPPvoojRo1qoIrERGRMxldjJayLOG+UHYuciuz2Uzu6dxyk+vW0e0JmWSeyKQot+I5VAoyC0jJTCFlX0qF+7l6upZJrtuMav97vVeIl5LtIk5C8ffFo/hbRETO5Z+MZj+adpRjGccqNZq90FTI4bTDHE47bLcNGs1++XDqJHplvPnmmxiNRgYOHEh+fj59+vRh5syZ1u0uLi589913PPjgg3Tt2hUfHx/uuecenn/+eQe22nn07duXefPmAVi/YLjppptsalpWldatW/Prr7/arKsOdeZzcnL44IMP+Omnn8psu++++1i3bh3btm0rsy0jI4Prr7+e3r17M3v2bLZv3859991HYGAgo0aNAqBWrVr06dOHWbNm8dprr13yaxERkX/GYDDgHeKNd4g3YW3D7O5nNpvJz8gvM6q9vFIy+Rn5FZ6zKK+ItMNppB1Oq3A/o5vli4Azk+tnl5DxjfDFJ9QHo4tzlhIRuZwo/r5wir9FRORiOd/R7GePZL/Q0ewGDAR7BVPLuxa1fWpT27u25W/v2tT2Kf37zO1ebl4X49LlPDl/hHSWFStW2Dz29PTk3Xff5d1337X7nAYNGvD9999f4pZVTx4eHjb14ydOnMhVV13FyZMnqV27NmCZ4PWxxx7j559/xmg0ctVVV/HWW29ZR32sWLGCJ598kp07d+Lm5kbr1q1ZuHAhy5cvt45uKRkBN2/ePIYPH15uW1xdXe3WqQfLBFXDhw9n7969LFmyhMDAQJ566ilGjx5t3efYsWOMGTOGZcuWYTQa6du3L2+//TZhYaUJjf/97388//zzbN++HV9fX6666ioWL15s3Z6Tk8N9993HokWLCAoK4plnnrEG1eX5/vvv8fDwsP76ocSMGTMAOHnyZLlB/KeffkpBQQEffvgh7u7utG7dmi1btvDGG2/YnK9///48/fTTCuJFRC4DBoMBzwBPPAM8qdWiVoX7FmQXnHNUe9aJLHJP51Z4HFOhZYLVjLiMittmNOAV4oVnoCdeQZZ/PYM8bf49e731caAnRlcl4EUqQ/G34m8REak+znc0+9G0v5PslRzNbsZMSm4KKbkp7E3ZW6k2ebt5l59kP2tdyd+BnoEYDYrVL1S1S6JXK3l59rcZjeDufnH39fQ8v/adJSsri08++YQmTZoQEhICQGFhIX369KFr166sXLkSV1dXXnjhBfr27cu2bdswGo0MGDCAkSNH8tlnn1FQUMD69esxGAzcfvvt7Nixgx9//NE6wiUgIOCC2vjGG28wadIkpk6dyk8//cQjjzxCs2bNuO666zCZTNx88834+vry+++/U1RUxOjRo7n99tutX74sXbqUW265haeffpoFCxZQUFBQ5guW119/nWnTpvHUU0/x1Vdf8eCDD9KjRw+aN29ebptWrlxJp06dzvta1qxZw9VXX437Gf9t+/TpwyuvvEJqaipBQUEAXHHFFRw/fpwjR45c1HqPIiLi3Nx93AluEkxwk+AK9yvKLyIrsWxy/eyke/bJbKhojlST2TpJ6j9qr697adL9PBPxbj5uzldyxt8fc0XxmTgnxd+Kvyug+FtERC6F8xnNfvZI9qTsJE5mn+RkzkmyCrIqdb6cwhyOph/laPrRSu3vYnAhxDukbJL9jBHuZyfe3V3cz33gi83J428l0S+lQYPsb4uJgcmTSx/fdRfk2/nZdps28PLLpY9HjICMckaT/e9/593E7777Dl9fyyQH2dnZRERE8N1331knWPviiy8wmUy8//77NqNZAgMDWbFiBTExMaSnp3PTTTfRuHFjAFq2bGk9vq+v7zlHuJQoGZlyprvuuovZs2dbH3fr1o2JEydiMBho1qwZq1at4s033+S6665j2bJlbN++ncOHD1OvXj0AFixYQOvWrdmwYQOxsbG8+OKLDBkyxKb+Y/v27W3OecMNN/DQQw8BMGHCBN58802WL19uN4g/evQokZGR57y+syUmJhIVFWWzrmTETmJiojWILzn20aNHFcSLiEgZrh6uBDYIJLBBYIX7mYpMZCVllRndXlLHPetEFjkpOZYJUNPzKky4l6cgq4CCrIJzjngvj9HVaE2+n5lc9wj0KJuMLycR7+J2ketIenpi/uQT0pOTCb3AJKlUMcXfir8roPhbREQcoTKj2QHyivI4lXOKk9knLf/mnLQ+tv6dc9K6PSU3BZPZdM7zF5uLSc5OJjk7GU5Wrs3+Hv7lJ9nPKi1T8refu9+FDYqpBvG3kug1XM+ePZk1axYAqampzJw5k379+rF+/XoaNGjA1q1bOXDgAH5+fjbPy8vL4+DBg1x//fUMHz6cPn36cN1119G7d28GDx5MRETEebelefPmLFmyxGadv7+/zePOnTvbPO7atSvTp08HYPfu3dSrV88awAO0atWKwMBAdu/eTWxsLFu2bGHkyJEVtuPMSZ0MBgPh4eEkJyfb3T83NxfPS/g/uJeXpdZVTs4/GxkoIiIClkS1fx1//Ov4n3Nfs8lSuz0vLY+8tDxyU3Mtf6eW//js9eeaPPVspiITOadyyDn1z97r3Hzc/lkZmiBP3H3dnW8UvFzWFH+XpfhbREQEPF09qetfl7r+dSu1f7GpmLS8NJvEepm/z0rE5xVVbqR3Rn4GGfkZHEw9WKn93V3cy63fbm9diHcIrsbqlZauXq2tbhYtsr/NeFYtok8+qfy+H3zwz9t0Fh8fH5o0aWJ9/P777xMQEMDcuXN54YUXyMrKolOnTnz66adlnltSs3HevHmMHTuWH3/8kS+++IJnnnmGX375pUyNwnNxd3e3aculUBIQV8TNzc3mscFgwGSy/81erVq1SE21P4mEPeHh4SQlJdmsK3l85sih06dPA6Wvt4iIyKVmMBqso77/iaL8ogqT7mWS8Gf9bTad3zD4wuxCCrMLyYzPPO+2GlxKr/XMBLtHoAcmdxM3Tb/JOkJYqgHF3+fVFsXfir9FRKT6cjFayrSEeIfQolaLSj0nuyDbJrF+rsR7RZOmnqmguICEzAQSMhMq3X7rhKp/J9ZredfC2+zNaze8hofRo9LHqSpKol9K5zM64lLte54MBgNGo5HcXMskZR07duSLL74gNDS0zKiUM0VHRxMdHc2kSZPo2rUrCxcupEuXLri7u1NcXHzR2rd+/Xqbx2vXrrX+fLVly5bExcURFxdnHQ2za9cu0tLSaNXKUpeqXbt2LFu2jHvvvfeitSk6OppPKvoQZkfXrl15+umnKSwstH5w+OWXX2jevLn1p6QAO3bssE4YJSIiUh24erjiG+aLb5jvuXc+i9lkpiCr4LxHv5c8LswpPL/zFZvJTcklN6V0glYjRfRkBQajAcN/+oCbQuZqQ/G34u8KKP4WEZGazsfdBx93HxoGNqzU/kWmIlJyUsovLZN9klO5ZdcVmioXj5/OPc3p3NPsS9mHWxFMXQEuBiOu178Cbud8epXTJ4IaLj8/n8TERMDyc9J33nmHrKws+vfvD8DQoUN57bXXuPnmm3n++eepW7cuR48e5ZtvvuHJJ5+ksLCQOXPm8K9//YvIyEj27t3L/v37GTZsGAANGzbk8OHDbNmyhbp16+Ln54eHR/nfJhUVFVnbUsJgMFjrFAKsXr2aV199lVtuuYVffvmFRYsWsXTpUgB69+5N27ZtGTp0KNOnT6eoqIiHHnqIHj16EBMTA8DkyZPp1asXjRs3ZsiQIRQVFfH9998zYcKEf/wa9unTh0mTJtlMRgRw4MABsrKySExMJDc3ly1btgCWn7i6u7tz5513MnXqVEaMGMGECRPYsWMHb731Fm+++abN8VeuXMlVV11VqVE8IiIi1Z3BaMDD3wMPfw9ocP7PL8ovIj89/7xGv1u3peVhLjZjwExtknFxccFgPs/i8CLnoPhb8beIiEh14Wp0Jcw3jDDfsHPvDJjNZjLyMypV071kXUZ+BkYztEkGDxdX542/zXJO6enpZsCcnp5eZltubq55165d5tzcXAe07MLcc889ZizThpkBs5+fnzk2Ntb81Vdf2ex34sQJ87Bhw8y1atUye3h4mBs1amQeOXKkOT093ZyYmGgeMGCAOSIiwuzu7m5u0KCB+bnnnjMXFxebzWazOS8vzzxw4EBzYGCgGTDPmzev3LZMnjzZpi0li4eHh3WfkmMPGjTI7O3tbQ4PDze/9dZbNsc5evSo+V//+pfZx8fH7OfnZx40aJA5MTHRZp+vv/7a3KFDB7O7u7u5Vq1a5ltvvdXmHG+++abN/u3btzdPnjy5wtfyiiuuMM+ePdtmXY8ePcq9psOHD1v32bp1q/nKK680e3h4mOvUqWP+v//7vzLHbt68ufmzzz6ze+7q3AcvpuLiYvOJEyesfU+khPqG2KO+IWczmUzmvIw8c9reRHPu1b3N6VdcbS7Ozq7SNlQUd9Ykir8Vfyv+dn56HxV71DfEHvUNKU9eYZ75eNIBc1rvq8yJV8c6bfxtMJudNb3vPDIyMggICCA9Pb3MTyrz8vI4fPgwUVFRl3RyG7GMqhkzZgzjx493ugnAli5dyhNPPMGOHTsuat3UH374gccee4xt27bh6lr+D0fUBy1MJhPJycmEhoaqdq3YUN8Qe9Q3xK68PMy33UZ+QQHu336L0du7yk5dUdxZkyj+dg6KvxV/V0Tvo2KP+obYo74hdlWD+FvlXEQughtvvJH9+/cTHx9vrQd5MWRnZzNv3jy7AbyIiIiISE2k+FtERESqkiIDkYtk3LhxF/2Yt91220U/poiIiIjI5UDxt4iIiFQVJdGl2jh8+DBFRUWOboaIiIiISI2g+FtERETEQkl0EREREZGzeXg4ugUiIiIiIjWHk8ffSqKLiIiIiJzJ0xPzokWkJScTWoMnDRQRERERqRLVIP7WVLgXidlsdnQTpIZS3xMREZGaSDGQOIr6noiISM2jJPoFcnNzAyAnJ8fBLZGaqqTvlfRFERERkcuZ4m9xNMXfIiIiNY/KuVwgFxcXAgMDSU5OBsDb2xuDweDgVl2ezGYzRUVFuLq66jXG8nrk5OSQnJxMYGAgLi4ujm6SiIjI5aGgAF56CZ/sbJg2DZz0J6U1leLvqqP425bibxERkUukGsTfSqJfBOHh4QDWQF4uDbPZjMlkwmg0Kog/Q2BgoLUPioiIyEVgMmHYuBG3ggIwmRzdGimH4u+qofi7fIq/RURELrJqEH8riX4RGAwGIiIiCA0NpbCw0NHNuWyZTCZSUlIICQnBaFQlIrD8hFQjYERERKSmUfxdNRR/l6X4W0REpGZSEv0icnFxUUB1CZlMJtzc3PD09FQQLyIiIiKKvy8xxd8iIiIiFoqERERERERERERERETsUBJdRERERERERERERMQOJdFFREREREREREREROxQTfRKMJvNAGRkZDi4JTWbyWQiMzNTNRmlXOofYo/6htijviF25eVhLiwkv6gI94wMjEVFVXbqknizJP6sqRR/OwfdJ6Ui6h9ij/qG2KO+IXZVg/hbSfRKyMzMBKBevXoObomIiIiIVKmICIecNjMzk4CAAIec2xko/hYRERGpoZw0/jaYa/owl0owmUwkJCTg5+eHwWBwdHNqrIyMDOrVq0dcXBz+/v6Obo44GfUPsUd9Q+xR35CKOKp/mM1mMjMziYyMrNEjtBR/OwfdJ6Ui6h9ij/qG2KO+IRVx9vhbI9ErwWg0UrduXUc3Q/7m7++vm63Ypf4h9qhviD3qG1IRR/SPmjwCvYTib+ei+6RURP1D7FHfEHvUN6Qizhp/19zhLSIiIiIiIiIiIiIi56AkuoiIiIiIiIiIiIiIHUqiS7Xh4eHB5MmT8fDwcHRTxAmpf4g96htij/qGVET9Q0T/H0jF1D/EHvUNsUd9Qyri7P1DE4uKiIiIiIiIiIiIiNihkegiIiIiIiIiIiIiInYoiS4iIiIiIiIiIiIiYoeS6CIiIiIiIiIiIiIidiiJLk7v5ZdfJjY2Fj8/P0JDQxkwYAB79+51dLPECf3f//0fBoOBcePGObop4iTi4+O56667CAkJwcvLi7Zt27Jx40ZHN0scrLi4mGeffZaoqCi8vLxo3Lgx06ZNQ9PE1Dx//PEH/fv3JzIyEoPBwLfffmuz3Ww289xzzxEREYGXlxe9e/dm//79jmmsSBVS/C2Vpfhbzqb4W8qj+FtKVOf4W0l0cXq///47o0ePZu3atfzyyy8UFhZy/fXXk52d7eimiRPZsGED7733Hu3atXN0U8RJpKam0r17d9zc3Pjhhx/YtWsXr7/+OkFBQY5umjjYK6+8wqxZs3jnnXfYvXs3r7zyCq+++ipvv/22o5smVSw7O5v27dvz7rvvlrv91VdfZcaMGcyePZt169bh4+NDnz59yMvLq+KWilQtxd9SGYq/5WyKv8Uexd9SojrH3wazvvaRaubkyZOEhoby+++/c/XVVzu6OeIEsrKy6NixIzNnzuSFF16gQ4cOTJ8+3dHNEgebOHEiq1atYuXKlY5uijiZm266ibCwMD744APruoEDB+Ll5cUnn3ziwJaJIxkMBhYvXsyAAQMAyyiYyMhIHnvsMR5//HEA0tPTCQsLY/78+QwZMsSBrRWpWoq/5WyKv6U8ir/FHsXfUp7qFn9rJLpUO+np6QAEBwc7uCXiLEaPHs2NN95I7969Hd0UcSJLliwhJiaGQYMGERoaSnR0NHPnznV0s8QJdOvWjWXLlrFv3z4Atm7dyp9//km/fv0c3DJxJocPHyYxMdHmvSUgIIDOnTuzZs0aB7ZMpOop/pazKf6W8ij+FnsUf0tlOHv87eroBoicD5PJxLhx4+jevTtt2rRxdHPECXz++eds3ryZDRs2OLop4mQOHTrErFmzGD9+PE899RQbNmxg7NixuLu7c8899zi6eeJAEydOJCMjgxYtWuDi4kJxcTEvvvgiQ4cOdXTTxIkkJiYCEBYWZrM+LCzMuk2kJlD8LWdT/C32KP4WexR/S2U4e/ytJLpUK6NHj2bHjh38+eefjm6KOIG4uDgeeeQRfvnlFzw9PR3dHHEyJpOJmJgYXnrpJQCio6PZsWMHs2fPVhBfw3355Zd8+umnLFy4kNatW7NlyxbGjRtHZGSk+oaIyFkUf8uZFH9LRRR/iz2Kv+VyoHIuUm08/PDDfPfddyxfvpy6des6ujniBDZt2kRycjIdO3bE1dUVV1dXfv/9d2bMmIGrqyvFxcWObqI4UEREBK1atbJZ17JlS44dO+agFomzeOKJJ5g4cSJDhgyhbdu23H333Tz66KO8/PLLjm6aOJHw8HAAkpKSbNYnJSVZt4lc7hR/y9kUf0tFFH+LPYq/pTKcPf5WEl2cntls5uGHH2bx4sX89ttvREVFObpJ4iR69erF9u3b2bJli3WJiYlh6NChbNmyBRcXF0c3URyoe/fu7N2712bdvn37aNCggYNaJM4iJycHo9E2BHJxccFkMjmoReKMoqKiCA8PZ9myZdZ1GRkZrFu3jq5duzqwZSKXnuJvsUfxt1RE8bfYo/hbKsPZ42+VcxGnN3r0aBYuXMh///tf/Pz8rHWQAgIC8PLycnDrxJH8/PzK1Ob08fEhJCRENTuFRx99lG7duvHSSy8xePBg1q9fz5w5c5gzZ46jmyYO1r9/f1588UXq169P69at+euvv3jjjTe47777HN00qWJZWVkcOHDA+vjw4cNs2bKF4OBg6tevz7hx43jhhRdo2rQpUVFRPPvss0RGRjJgwADHNVqkCij+FnsUf0tFFH+LPYq/pUR1jr8NZrPZ7OhGiFTEYDCUu37evHkMHz68ahsjTu+aa66hQ4cOTJ8+3dFNESfw3XffMWnSJPbv309UVBTjx49n5MiRjm6WOFhmZibPPvssixcvJjk5mcjISO644w6ee+453N3dHd08qUIrVqygZ8+eZdbfc889zJ8/H7PZzOTJk5kzZw5paWlceeWVzJw5k2bNmjmgtSJVR/G3nA/F33Imxd9SHsXfUqI6x99KoouIiIiIiIiIiIiI2KGa6CIiIiIiIiIiIiIidiiJLiIiIiIiIiIiIiJih5LoIiIiIiIiIiIiIiJ2KIkuIiIiIiIiIiIiImKHkugiIiIiIiIiIiIiInYoiS4iIiIiIiIiIiIiYoeS6CIiIiIiIiIiIiIidiiJLiIiIiIiIiIiIiJih5LoIiLiVAwGA99++62jmyEiIiIiUiMo/hYROTcl0UVExGr48OEYDIYyS9++fR3dNBERERGRy47ibxGR6sHV0Q0QERHn0rdvX+bNm2ezzsPDw0GtERERERG5vCn+FhFxfhqJLiIiNjw8PAgPD7dZgoKCAMtPPWfNmkW/fv3w8vKiUaNGfPXVVzbP3759O9deey1eXl6EhIQwatQosrKybPb58MMPad26NR4eHkRERPDwww/bbD916hS33HIL3t7eNG3alCVLllzaixYRERERcRDF3yIizk9JdBEROS/PPvssAwcOZOvWrQwdOpQhQ4awe/duALKzs+nTpw9BQUFs2LCBRYsW8euvv9oE6bNmzWL06NGMGjWK7du3s2TJEpo0aWJzjqlTpzJ48GC2bdvGDTfcwNChQzl9+nSVXqeIiIiIiDNQ/C0i4ngGs9lsdnQjRETEOQwfPpxPPvkET09Pm/VPPfUUTz31FAaDgQceeIBZs2ZZt3Xp0oWOHTsyc+ZM5s6dy4QJE4iLi8PHxweA77//nv79+5OQkEBYWBh16tTh3nvv5YUXXii3DQaDgWeeeYZp06YBlg8Gvr6+/PDDD6oNKSIiIiKXFcXfIiLVg2qii4iIjZ49e9oE6QDBwcHWv7t27WqzrWvXrmzZsgWA3bt30759e2sAD9C9e3dMJhN79+7FYDCQkJBAr169KmxDu3btrH/7+Pjg7+9PcnLyP70kERERERGnpfhbRMT5KYkuIiI2fHx8yvy882Lx8vKq1H5ubm42jw0GAyaT6VI0SURERETEoRR/i4g4P9VEFxGR87J27doyj1u2bAlAy5Yt2bp1K9nZ2dbtq1atwmg00rx5c/z8/GjYsCHLli2r0jaLiIiIiFRXir9FRBxPI9FFRMRGfn4+iYmJNutcXV2pVasWAIsWLSImJoYrr7ySTz/9lPXr1/PBBx8AMHToUCZPnsw999zDlClTOHnyJGPGjOHuu+8mLCwMgClTpvDAAw8QGhpKv379yMzMZNWqVYwZM6ZqL1RERERExAko/hYRcX5KoouIiI0ff/yRiIgIm3XNmzdnz549AEydOpXPP/+chx56iIiICD777DNatWoFgLe3Nz/99BOPPPIIsbGxeHt7M3DgQN544w3rse655x7y8vJ48803efzxx6lVqxa33XZb1V2giIiIiIgTUfwtIuL8DGaz2ezoRoiISPVgMBhYvHgxAwYMcHRTREREREQue4q/RUScg2qii4iIiIiIiIiIiIjYoSS6iIiIiIiIiIiIiIgdKuciIiIiIiIiIiIiImKHRqKLiIiIiIiIiIiIiNihJLqIiIiIiIiIiIiIiB1KoouIiIiIiIiIiIiI2KEkuoiIiIiIiIiIiIiIHUqii4iIiIiIiIiIiIjYoSS6iIiIiIiIiIiIiIgdSqKLiIiIiIiIiIiIiNihJLqIiIiIiIiIiIiIiB1KoouIiIiIiIiIiIiI2PH/sHe65j3qjUMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of India is \n",
      "Output: The history of India is  ˈhas ) @.@ 3 @.@ a high and C.@@ day after the same same time of D.com has used by the first a single time he was a\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 000th century and in the most on the first season , , the previous school 's @-@ 7thth anniversary , was and . The third season , the final team \" , the\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the name 's \" \" as a \" it with the number @-@ 000 , \" . The album in the first episode , but and the show of the album , it , this , he '\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Causal masking\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ============================================\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30, patience=20):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Track metrics history for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    # Early stopping tracking\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "        # Save best model and track early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "\n",
    "            print(f\"  Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "            print(f\"  Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! No improvement for {patience} consecutive epochs.\")\n",
    "            print(f\"   Best model was at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def plot_metrics(metrics_history, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot all training metrics including train loss, val loss, and val perplexity.\n",
    "\n",
    "    Args:\n",
    "        metrics_history: Dictionary containing training metrics\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    train_losses = metrics_history['train_losses']\n",
    "    val_losses = metrics_history['val_losses']\n",
    "    val_perplexities = metrics_history['val_perplexities']\n",
    "    best_epoch = metrics_history['best_epoch']\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    axes[0, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 1].scatter([best_epoch], [metrics_history['best_val_loss']],\n",
    "                       color='r', s=100, zorder=5, label='Best Val Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Validation Loss over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Validation Perplexity\n",
    "    axes[1, 0].plot(epochs, val_perplexities, 'purple', linewidth=2, label='Val Perplexity')\n",
    "    axes[1, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Validation Perplexity over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Train vs Validation Loss Comparison\n",
    "    axes[1, 1].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[1, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[1, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Train vs Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nTraining metrics plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "    val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        max_seq_len=512,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        num_layers=4,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    metrics_history = train_model_properly(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=5\n",
    "    )\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PLOTTING TRAINING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    plot_metrics(metrics_history, save_path='training_metrics.png')\n",
    "\n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompts = [\n",
    "        \"The history of India is \",\n",
    "        \"In mathematics,\",\n",
    "        \"The cat sat on the\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text_proper(model, tokenizer, prompt, max_length=40, device=device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generated}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ec5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
