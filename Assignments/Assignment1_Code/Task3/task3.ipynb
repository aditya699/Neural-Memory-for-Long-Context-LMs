{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.4903,  1.9947,  0.6297], requires_grad=True)\n",
      "Output: tensor([-0.4903,  3.9895,  1.8890], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([2, 10])\n",
      "Output embeddings shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "#Create\n",
    "vocab_size = 1000  # Size of the vocabulary\n",
    "d_model = 512    # Dimension of the embeddings\n",
    "token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "#Test\n",
    "token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "output= token_embed (token_ids)\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output position embeddings shape: torch.Size([10, 512])\n",
      "Positions Used: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "#Create\n",
    "pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "output = pos_embed(10)  # Example input sequence length\n",
    "print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98afba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# Test\n",
    "embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "output = embeddings(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: -0.0006\n",
      "  Std: 0.9969\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: 0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create FFN\n",
    "ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 512])\n",
      "Multi-head attention works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "    # Create input\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "    print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "    print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "✅ Transformer Block works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# Forward pass\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Number of blocks: 6\n"
     ]
    }
   ],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create model with 6 stacked blocks\n",
    "model = Transformer(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,  # 6 transformer blocks!\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "output = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# Create complete language model\n",
    "model = LanguageModel(\n",
    "    vocab_size=1000,\n",
    "    max_seq_len=2048,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test\n",
    "token_ids = torch.randint(0, 1000, (2, 10))\n",
    "logits = model(token_ids)\n",
    "\n",
    "print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa64319cafa84fb99bbecf3bf767ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff8acc2c0eb4c32992cb280d8487b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52366507ce7a4cf699a2b48d83c79fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ba141559ad45309bdd992fca9d798f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05274b10823b4c1eaf150752aba86b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467c2c0e9574478da5e51b632d03e643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412eceee6a7b468db30ef82990be4ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n",
      "\n",
      "First training example:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677d46739fee46d398d4440834982449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa524c7ff4e414cbd2483ed0a5a57af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349a3061e18e401eaca61b478ee41efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf80c56c7754a6e91db6b4364fd74d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067cfc176e74d16a36c68b607d9fca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "\n",
      "Total sequences: 4584\n",
      "\n",
      "Sample 0:\n",
      "Input shape: torch.Size([512])\n",
      "Target shape: torch.Size([512])\n",
      "First 10 input tokens: tensor([   28,   569, 18354,  7496, 17740,  6711,   796, 10445,    73, 13090])\n",
      "First 10 target tokens: tensor([  569, 18354,  7496, 17740,  6711,   796, 10445,    73, 13090,   645])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8c895",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "```\n",
    "Loss(So loss is computed for each token)(Define the Loss)\n",
    "↓\n",
    "Optimizer(Optimizer updates the model's weights to minimize loss.)\n",
    "↓\n",
    "Training Loop(Forward-Loss-clear Gradients-Backward-Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 5])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "\n",
      "After reshaping:\n",
      "Logits flat: torch.Size([6, 5])\n",
      "Targets flat: torch.Size([6])\n",
      "\n",
      "Loss: 2.0549\n"
     ]
    }
   ],
   "source": [
    "#Define the Loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake model output (logits)\n",
    "# [batch=2, seq=3, vocab_size=5]\n",
    "logits = torch.randn(2, 3, 5)\n",
    "\n",
    "# Target token IDs\n",
    "# [batch=2, seq=3]\n",
    "targets = torch.tensor([\n",
    "    [2, 4, 1],  # Sequence 1: correct tokens are 2, 4, 1\n",
    "    [0, 3, 2]   # Sequence 2: correct tokens are 0, 3, 2\n",
    "])\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)    # [2, 3, 5]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# BUT! CrossEntropyLoss expects:\n",
    "# logits: [N, vocab_size] where N = batch * seq\n",
    "# targets: [N]\n",
    "\n",
    "# So we need to reshape!\n",
    "logits_flat = logits.view(-1, 5)      # [6, 5]  (2*3=6 positions)\n",
    "targets_flat = targets.view(-1)        # [6]\n",
    "\n",
    "print(\"\\nAfter reshaping:\")\n",
    "print(\"Logits flat:\", logits_flat.shape)   # [6, 5]\n",
    "print(\"Targets flat:\", targets_flat.shape) # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Logits shape: torch.Size([32, 512, 50257])\n",
      "\n",
      "Flattened logits: torch.Size([16384, 50257])\n",
      "Flattened targets: torch.Size([16384])\n",
      "\n",
      "Loss: 11.0243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate model training\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Get a batch from dataloader\n",
    "input_ids, target_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")    # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Logits shape: {logits.shape}\")      # [32, 512, 50257]\n",
    "\n",
    "# Reshape for loss calculation\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "logits_flat = logits.view(batch_size * seq_len, vocab_size)  # [16384, 50257]\n",
    "targets_flat = target_ids.view(batch_size * seq_len)         # [16384]\n",
    "\n",
    "print(f\"\\nFlattened logits: {logits_flat.shape}\")\n",
    "print(f\"Flattened targets: {targets_flat.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created!\n",
      "Learning rate: 3e-4 = 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),  # All model weights to optimize\n",
    "    lr=3e-4,            # Learning rate (how big each update is)\n",
    "    weight_decay=0.01   # Regularization (prevents overfitting)\n",
    ")\n",
    "\n",
    "print(\"Optimizer created!\")\n",
    "print(f\"Learning rate: 3e-4 = {3e-4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fca023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Training ===\n",
      "Weight sample: -0.2838\n",
      "Loss: 1.8065\n",
      "Gradient sample: -0.151754\n",
      "=== After Training ===\n",
      "Weight sample: -0.2828\n",
      "Weight changed! ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(2, 10)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()           # Clear old gradients\n",
    "output = model(x)               # Forward\n",
    "loss = criterion(output, targets)  # Loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()                 # Calculate gradients\n",
    "print(f\"Gradient sample: {model.weight.grad[0, 0].item():.6f}\")\n",
    "\n",
    "optimizer.step()                # Update weights\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "print(\"Weight changed! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2794887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the language model\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            # Move data to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)  # [batch, seq, vocab]\n",
    "            \n",
    "            # Reshape for loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bf95b",
   "metadata": {},
   "source": [
    "# Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a23b0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after clearing: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after cleanup: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete ALL variables\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "del train_loader\n",
    "del train_dataset\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22332564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Check GPU is clean\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  # Should be 0!\n",
    "\n",
    "# 3. Load data (only what we need)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Continue to next step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde39b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial GPU Memory: 0.00 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Dataset ready. GPU Memory: 0.00 GB\n",
      "Model created. GPU Memory: 0.05 GB\n",
      "Parameters: 13,329,233\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 9168/9168 [02:13<00:00, 68.68it/s, loss=3.5917] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 3.5917\n",
      "TRAINING COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# ============================================\n",
    "# 1. COMPONENTS (copy your classes here)\n",
    "# ============================================\n",
    "\n",
    "# MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# FeedForward class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "# LanguageModel class\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# WikiTextDataset class\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SETUP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE MODEL (TINY!)\n",
    "# ============================================\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=128,\n",
    "    d_model=128,\n",
    "    num_heads=2,\n",
    "    d_ff=512,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. OPTIMIZER & LOSS\n",
    "# ============================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================\n",
    "# 5. TRAIN!\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
    "\n",
    "print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f50a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.23 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "MHA Model parameters: 70,559,825\n",
      "GPU Memory after model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear GPU first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=256)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Create PROPER MHA model (bigger than test)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=256,\n",
    "    d_model=512,     # Assignment size\n",
    "    num_heads=8,     # Assignment size  \n",
    "    d_ff=2048,       # Assignment size\n",
    "    num_layers=6     # Assignment size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"MHA Model parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory after model: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afed9b",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and calculate perplexity\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape and calculate loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()  # Back to training mode\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aa804",
   "metadata": {},
   "source": [
    "# Training MHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING MHA MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1146/1146 [01:31<00:00, 12.48it/s, loss=0.4095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.4095\n",
      "  Val Loss: 0.2125\n",
      "  Val Perplexity: 1.24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1146/1146 [01:32<00:00, 12.35it/s, loss=0.0845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.0845\n",
      "  Val Loss: 0.1206\n",
      "  Val Perplexity: 1.13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1146/1146 [01:33<00:00, 12.31it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 0.0419\n",
      "  Val Loss: 0.0996\n",
      "  Val Perplexity: 1.10\n",
      "\n",
      "============================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MHA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model = mha_model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "        targets_flat = target_ids.view(batch_size * seq_len)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_perplexity = validate_model(mha_model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d841",
   "metadata": {},
   "source": [
    "# Save the MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e78c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA model saved as 'mha_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': 0.0419,\n",
    "    'val_loss': 0.0996,\n",
    "    'val_perplexity': 1.10,\n",
    "}, 'mha_model.pt')\n",
    "\n",
    "print(\"✅ MHA model saved as 'mha_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e73e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The history of\n",
      "Generating...\n",
      "\n",
      "Output: The history of education of revered of of Interstate fighters of of of intelligence education of of intelligence of ofjohn of of church of of of of of of parish of of\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: In mathematics,\n",
      "Generating...\n",
      "\n",
      "Output: In mathematics,InIn plInInInInInInInInInInInInInInInInInInInInInInInInInInIn\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: The cat sat on the\n",
      "Generating...\n",
      "\n",
      "Output: The cat sat on the cat added added catalsuruuru the added on added on added added added added added added added added added added added added added on on added added added\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_fixed(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with top-k sampling (more stable)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generating...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # TOP-K SAMPLING (key difference!)\n",
    "            # Only consider top 50 most likely tokens\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test with better sampling\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_fixed(mha_model, tokenizer, prompt, max_length=30, top_k=50, device=device)\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0217fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 1.56 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "======================================================================\n",
      "TRAINING MHA MODEL (PROPERLY)\n",
      "======================================================================\n",
      "Parameters: 70,690,897\n",
      "GPU Memory: 1.28 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 573/573 [01:31<00:00,  6.26it/s, loss=6.8247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 6.8247\n",
      "  Val Loss: 6.3977\n",
      "  Val Perplexity: 600.47\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 573/573 [01:32<00:00,  6.18it/s, loss=5.9716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 5.9716\n",
      "  Val Loss: 6.1046\n",
      "  Val Perplexity: 447.93\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 573/573 [01:32<00:00,  6.17it/s, loss=5.4670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 5.4670\n",
      "  Val Loss: 5.6924\n",
      "  Val Perplexity: 296.61\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 573/573 [01:33<00:00,  6.16it/s, loss=3.0203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 3.0203\n",
      "  Val Loss: 1.1246\n",
      "  Val Perplexity: 3.08\n",
      "  ✅ Best model so far!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 573/573 [01:33<00:00,  6.15it/s, loss=0.6486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 0.6486\n",
      "  Val Loss: 0.3403\n",
      "  Val Perplexity: 1.41\n",
      "  ✅ Best model so far!\n",
      "\n",
      "======================================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "Final Val Perplexity: 1.41\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Better Trainning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CLEAR GPU AND START FRESH\n",
    "# ============================================\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# BETTER HYPERPARAMETERS (Less Overfitting)\n",
    "# ============================================\n",
    "\n",
    "# Create datasets with LONGER sequences\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "# ============================================\n",
    "# BETTER TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"  ✅ Best model so far!\")\n",
    "        print()\n",
    "    \n",
    "    return val_loss, val_perplexity\n",
    "\n",
    "# ============================================\n",
    "# TRAIN MHA MODEL PROPERLY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MHA MODEL (PROPERLY)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    dropout=0.2  # Higher dropout to reduce overfitting\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.1)  # Higher weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\\n\")\n",
    "\n",
    "# Train for 5 epochs\n",
    "mha_val_loss, mha_perplexity = train_model_properly(\n",
    "    mha_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'val_loss': mha_val_loss,\n",
    "    'val_perplexity': mha_perplexity,\n",
    "}, 'mha_model_proper.pt')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA TRAINING COMPLETE!\")\n",
    "print(f\"Final Val Perplexity: {mha_perplexity:.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04cfc5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING MHA MODEL GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of\n",
      "Output: The history of history largest historyThe history largest 36 largest history hurricanesThe Forces largest 36rd history First history hurricanes charge history history 200 feet foot kmrdrd largest history 36 kilometers kilometers km Run Run chart First history history\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics, By By By By By By By By By By By By By By By — By By By By By By By By By By By By By By By By By By By By By By By By\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the the the on on Allied on the on the on Allied on on the set on on the on on Allied on the on on on the on on on set on on the the on on on right on\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING MHA MODEL GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_proper(mha_model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301e23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
