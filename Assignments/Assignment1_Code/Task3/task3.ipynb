{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e8141",
   "metadata": {},
   "source": [
    "# NOTE: We will revise pytorch and implement a language model from scratch hopefully this will clear all doubts for future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47786215",
   "metadata": {},
   "source": [
    "# Pytorch Basic Session :1 (Do not run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef10f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : Understanding nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #By calling super().__init__(), you're saying: \"Hey parent class, set up all your infrastructure before I add my custom stuff.\"(parent class's constructor.))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7af540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Output: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#Setp 1:1 Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "# Create model\n",
    "model = SimpleModule()\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)  # This calls forward() automatically\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6197a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "Weight: Parameter containing:\n",
      "tensor([-0.8741,  1.4167,  1.5167], requires_grad=True)\n",
      "Output: tensor([-0.8741,  2.8334,  4.5501], grad_fn=<MulBackward0>)\n",
      "\n",
      "Is weight learnable? True\n"
     ]
    }
   ],
   "source": [
    "#Step 2 :Understanding nn.Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModuleWithWeight(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This creates a LEARNABLE parameter\n",
    "        self.weight = nn.Parameter(torch.randn(size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "# Create model\n",
    "model = ModuleWithWeight(3)\n",
    "\n",
    "# Test\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weight: {model.weight}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"\\nIs weight learnable? {model.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83ec1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([256, 512])\n",
      "Bias shape: torch.Size([256])\n",
      "Weight requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Understanding nn.Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "input_dim = 512\n",
    "output_dim = 256\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# What does it contain?\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 10, 512])\n",
      "Output shape: torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create linear layer\n",
    "linear = nn.Linear(512, 256)\n",
    "# This creates a weight matrix of 256 and 512\n",
    "\n",
    "# Create input\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # [32, 10, 512]\n",
    "print(f\"Output shape: {output.shape}\")  # [32, 10, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f85e53",
   "metadata": {},
   "source": [
    "# Buliding Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffa16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Start by inheriting from nn.Module\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__() #Call the parent class's constructor so that all methods of nn.Module are properly initialized.\n",
    "        #nn.embedding creates a lookup table that maps from integer indices (representing tokens) to dense vectors of fixed size (the embeddings).\n",
    "        #nn.embedding ,creates a lookup table and intializes it with random values. and makes the values learnable parameters of the model. (also registers it as a parameter of the module)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "# #Create\n",
    "# vocab_size = 1000  # Size of the vocabulary\n",
    "# d_model = 512    # Dimension of the embeddings\n",
    "# token_embed= TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# #Test\n",
    "# token_ids=torch.randint(0, vocab_size, (2,10))\n",
    "# output= token_embed (token_ids)\n",
    "# print(f\"Input token IDs shape: {token_ids.shape}\")  # [2, 10]\n",
    "# print(f\"Output embeddings shape: {output.shape}\")    # [2, 10,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6049e",
   "metadata": {},
   "source": [
    "# Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69259fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module): #Inherit from nn.Module\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__() #Call the parent class's constructor to make sure all methods of nn.Module are properly initialized. and available.\n",
    "        #NOTE:Transformers (Original paper) used sinusoidal position embeddings, but nn.Embedding is a common choice for learnable position embeddings.(we will make sure gradients also flow through these embeddings during training.)\n",
    "        #This creates a learnable position embedding table of size (max_seq_len, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        #Generate position indices from 0 to seq_len - 1\n",
    "        positions=torch.arange(seq_len)#This creaates a tensor with seqential numbersye\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "# #Create\n",
    "# pos_embed= PositionEmbedding(max_seq_len=2048, d_model=512)\n",
    "        \n",
    "# output = pos_embed(10)  # Example input sequence length\n",
    "# print(f\"Output position embeddings shape: {output.shape}\")  # [10, 512]\n",
    "# print(f\"Positions Used: {torch.arange(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ce3",
   "metadata": {},
   "source": [
    "# Combine Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98afba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):#This is normal inheritance from nn.Module\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__() #Initialize the parent class nn.Module so that all its methods and attributes are available. and usable.\n",
    "        \"\"\"\n",
    "        1.token embedding layer to convert token IDs into dense vectors.\n",
    "        2.position embedding layer to add positional information to the token embeddings.\n",
    "\n",
    "        NOTE:Both embeddings are learnable parameters of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        tokens = self.token_embed(token_ids)\n",
    "        #torch.arrange helps to convert tokens into id's which can be used to get position embeddings from nn.Embedding lookup table.\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "        \n",
    "        return tokens + pos\n",
    "\n",
    "# # Test\n",
    "# embeddings = Embeddings(vocab_size=1000, max_seq_len=2048, d_model=512)\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [2, 10]\n",
    "# output = embeddings(token_ids)\n",
    "\n",
    "# print(f\"Input shape: {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c78b",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "\n",
      "Before LayerNorm:\n",
      "  Mean: 0.0131\n",
      "  Std: 0.9961\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: -0.0000\n",
      "  Std: 1.0000\n",
      "Gamma (scale): torch.Size([512])\n",
      "Beta (shift): torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#Normalizes features to have zero mean and unit variance across the feature dimension\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Simple implementation of LayerNorm\n",
    "d_model = 512\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#Test\n",
    "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBefore LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "#NOTE:Layer norm has two learnable parameters per feature dimension: weight and bias. These parameters allow the model to scale and shift the normalized output, providing flexibility in how the normalized values are represented.\n",
    "# It has 2 learnable parameters:\n",
    "print(f\"Gamma (scale): {layer_norm.weight.shape}\")  # [512]\n",
    "print(f\"Beta (shift): {layer_norm.bias.shape}\")     # [512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8f75",
   "metadata": {},
   "source": [
    "# FFN (Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf997eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (512)\n",
    "            d_ff: Hidden dimension (2048, typically 4x d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two linear layers\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # Expand: 512 → 2048\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Activate\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Contract: 2048 → 512\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Dropout again\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create FFN\n",
    "# ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# # Test\n",
    "# x = torch.randn(2, 10, 512)  # [batch, seq, d_model]\n",
    "# output = ffn(x)\n",
    "\n",
    "# print(f\"Input shape: {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd38e7b",
   "metadata": {},
   "source": [
    "# Residual Connection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea266302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 10, 512])\n",
      "Transformed: torch.Size([2, 10, 512])\n",
      "With residual: torch.Size([2, 10, 512])\n",
      "\n",
      "It's just addition!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# Some operation (pretend it's attention)\n",
    "transformed = torch.randn(2, 10, 512)\n",
    "\n",
    "# WITHOUT residual\n",
    "output_no_residual = transformed\n",
    "\n",
    "# WITH residual (just add!)\n",
    "output_with_residual = x + transformed\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Transformed: {transformed.shape}\")\n",
    "print(f\"With residual: {output_with_residual.shape}\")\n",
    "print(\"\\nIt's just addition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc5873",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the standard multi-head attention class.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):  # This runs when you create an object of this class\n",
    "        super().__init__()  # This is used to call nn.module's init method which initializes the methods and attributes of the nn.module class\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # We are storing all these so that they can be anywhere in the code\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "       \n",
    "        # nn.Linear is PyTorch's fully connected (dense) layer that performs a linear transformation on the input.\n",
    "        # It takes the input and multiplies it by a weight matrix and adds a bias term.\n",
    "        # So it does a y=xw^T+b\n",
    "        \n",
    "        # So we need to create projections for Q, K, V (the parameters are input_dim, output_dim), so self.q_proj will create a weight matrix of size d_model x d_model,the weight initlization follows Xavier/Kaiming Initilication\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Post combination of all heads we need a final projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout helps us to randomly drop out some neurons to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # This is the method which runs when you call the model\n",
    "    def forward(self, x):\n",
    "        # This is tuple unpacking\n",
    "        batch_size, seq_len, _ = x.size()  # Fixed: using _ instead of d_model to avoid shadowing\n",
    "\n",
    "        # Now we need to project the input matrix into a different matrix\n",
    "        # So we need to create projections for Q, K, V\n",
    "        # Q: What am i looking for?\n",
    "        # K: What do i contain?\n",
    "        # V: What information do i have?\n",
    "\n",
    "        Q = self.q_proj(x)  # Query = x@W_q^T + b_q  #This actually calls the forward method\n",
    "        K = self.k_proj(x)  # Key = x@W_k^T + b_k\n",
    "        V = self.v_proj(x)  # Value = x@W_v^T + b_v\n",
    "        \n",
    "        # Now we wish to split the query, key and value matrices into multiple attention heads so that we can perform parallel computations\n",
    "        # Now we are reshaping the matrix to (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to transpose the matrix to put heads first\n",
    "        # We are doing this since we want to compute attention for each head separately\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Scaling prevents softmax from saturating\n",
    "        # scores[i,j]: how much token i should attend to token j high score means more attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "\n",
    "        # Apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # We need to multiply with V\n",
    "        # (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Here we are taking combination of information from all the heads weighted by attention\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # We need to concatenate heads back\n",
    "        # This is done to transpose the output and make it contiguous in memory (since a simple transpose is not contiguous)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # This is concatenation of heads\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # Fixed: batch -> batch_size, d_model -> self.d_model\n",
    "\n",
    "        # Final Projection\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# # Test the implementation\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create model\n",
    "#     model = MultiHeadAttention(d_model=512, num_heads=8, dropout=0.1)\n",
    "    \n",
    "#     # Create input\n",
    "#     batch_size = 32\n",
    "#     seq_len = 10\n",
    "#     x = torch.randn(batch_size, seq_len, 512)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(x)\n",
    "    \n",
    "#     print(f\"Input shape:  {x.shape}\")       # [32, 10, 512]\n",
    "#     print(f\"Output shape: {output.shape}\")   # [32, 10, 512]\n",
    "#     print(\"Multi-head attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979720",
   "metadata": {},
   "source": [
    "# Creating a Transformers Block\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Multi-Head Attention \n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual (add input back)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505f7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 1: Multi-Head Attention + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 1: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 2: Layer Norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Step 3: Attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 5: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        # ============================================\n",
    "        # BLOCK 2: Feed-Forward Network + Residual\n",
    "        # ============================================\n",
    "        \n",
    "        # Step 6: Save input for residual\n",
    "        residual = x\n",
    "        \n",
    "        # Step 7: Layer Norm\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Step 8: FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # Step 9: Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 10: Residual connection\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# # Input\n",
    "# x = torch.randn(2, 10, 512)  # [batch=2, seq=10, d_model=512]\n",
    "\n",
    "# # Forward pass\n",
    "# output = block(x)\n",
    "\n",
    "# print(f\"Input shape:  {x.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(\"✅ Transformer Block works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c0105",
   "metadata": {},
   "source": [
    "# Now we need to stack multiple transformers Blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03979ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ModuleList is a pytorch container that creates a list of modules that Pytorch can track\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (e.g., 50000)\n",
    "            max_seq_len: Maximum sequence length (e.g., 2048)\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "            d_ff: FFN hidden dimension (e.g., 2048)\n",
    "            num_layers: Number of transformer blocks to stack (e.g., 6)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# # Create model with 6 stacked blocks\n",
    "# model = Transformer(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,  # 6 transformer blocks!\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq=10]\n",
    "# output = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Number of blocks: {len(model.blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd357",
   "metadata": {},
   "source": [
    "# Final Output Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c312c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output Head: Project to vocabulary\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Tie weights between token embeddings and lm_head (Weight Sharing) this is important for reducing the number of parameters and improving generalization. Morevover see token embeddings and lm_head are performing inverse operations. example token embeddings map token IDs to vectors, while lm_head maps vectors back to token logits.\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [batch, seq]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embeddings(token_ids)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        #NOTE:lm_head is a linear layer that maps the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "        # Logits are the raw, unnormalized scores outputted by the model before applying softmax to get probabilities.\n",
    "        logits = self.lm_head(x)  # [batch, seq, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# # Create complete language model\n",
    "# model = LanguageModel(\n",
    "#     vocab_size=1000,\n",
    "#     max_seq_len=2048,\n",
    "#     d_model=512,\n",
    "#     num_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     num_layers=6,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Test\n",
    "# token_ids = torch.randint(0, 1000, (2, 10))\n",
    "# logits = model(token_ids)\n",
    "\n",
    "# print(f\"Input shape:  {token_ids.shape}\")      # [2, 10]\n",
    "# print(f\"Output shape: {logits.shape}\")         # [2, 10, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf56e4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924cbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63110246f574d3b9619f91106e284e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb9d20f814d4e2babd2e6f3d54289ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d914ea77d1245efba483e244e0e13f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2205567f7a6e4569a98bc6a72a3d1ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725d7499e2d49a688659c3fb1831a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045fdf868dc14f05982f875a6b09e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ee7af13c564dfa90f7fc6410cff48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 36718\n",
      "Validation samples: 3760\n",
      "Test samples: 4358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96497ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for non-empty examples:\n",
      "\n",
      "Example 1:\n",
      "= Valkyria Chronicles III =\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty example\n",
    "print(\"\\nLooking for non-empty examples:\")\n",
    "for i in range(10):\n",
    "    text = dataset['train'][i]['text'].strip()\n",
    "    if len(text) > 0:\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(text[:300])  # First 300 chars\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693c44",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c6ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75181c8aab614c9abad954eca1c2317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406fef5760b41559009fc5c12ab09c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082a49dfc5a44d5389cf08218021987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ddf0c3c500446b82278509620e8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee748b27664c308f84070ffd8eb5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Example tokens:\n",
      "\n",
      "Text: The cat sat on the mat\n",
      "Token IDs: [464, 3797, 3332, 319, 262, 2603]\n",
      "Decoded back: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 tokenizer needs a pad token (it doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Example tokens:\")\n",
    "\n",
    "# Test it\n",
    "text = \"The cat sat on the mat\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1acb3",
   "metadata": {},
   "source": [
    "# Create a pytroch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f8da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "Sample 0: 10\n",
      "Sample 2: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.So mostly when we create a pytorch dataset\n",
    "we need three things:\n",
    "- __init__ : to initialize the dataset object, load data, and set up any necessary variables..\n",
    "- __len__ : to return the total number of samples in the dataset.\n",
    "- __getitem__ : to retrieve a single sample from the dataset given an index.\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize: Load/prepare data\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return: How many samples?\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return: Give me sample number 'idx'\n",
    "        pass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store some numbers\n",
    "        self.data = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many samples?\n",
    "        return len(self.data)  # 5\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sample number idx\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\")  # 5\n",
    "print(f\"Sample 0: {dataset[0]}\")  # 10\n",
    "print(f\"Sample 2: {dataset[2]}\")  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ed63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 samples\n",
      "\n",
      "Sample 0: input=10, target=20\n",
      "Sample 1: input=20, target=30\n",
      "Sample 2: input=30, target=40\n",
      "Sample 3: input=40, target=50\n",
      "Sample 4: input=50, target=60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InputTargetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Store a sequence\n",
    "        self.data = [10, 20, 30, 40, 50, 60]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # We can make 5 pairs (last one has no target, so -1)\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: current number\n",
    "        # Target: next number\n",
    "        input_val = self.data[idx]\n",
    "        target_val = self.data[idx + 1]\n",
    "        \n",
    "        return input_val, target_val\n",
    "\n",
    "# Create dataset\n",
    "dataset = InputTargetDataset()\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} samples\\n\")\n",
    "\n",
    "# Get samples\n",
    "for i in range(len(dataset)):\n",
    "    input_val, target_val = dataset[i]\n",
    "    print(f\"Sample {i}: input={input_val}, target={target_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: HuggingFace dataset (dataset['train'])\n",
    "            tokenizer: GPT2Tokenizer\n",
    "            max_length: Sequence length (512)\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Step 1: Tokenize ALL text into one long list\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # How many sequences of length 512?\n",
    "        return len(self.tokens) // self.max_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk starting at position (idx * 512)\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "        \n",
    "        # Input: tokens[start:end]\n",
    "        # Target: tokens[start+1:end+1] (shifted!)\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05541b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create our dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_dataset = \u001b[43mWikiTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get first sample\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mWikiTextDataset.__init__\u001b[39m\u001b[34m(self, data, tokenizer, max_length)\u001b[39m\n\u001b[32m     19\u001b[39m     text = example[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Skip empty lines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         all_tokens.extend(tokens)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Step 2: Convert to PyTorch tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2732\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2694\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2695\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2715\u001b[39m     **kwargs,\n\u001b[32m   2716\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2717\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2718\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2719\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2730\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     encoded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2735\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3123\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3096\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3111\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3114\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3115\u001b[39m     padding=padding,\n\u001b[32m   3116\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m     **kwargs,\n\u001b[32m   3121\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3142\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2.py:281\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load data and tokenizer\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create our dataset\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train'],\n",
    "    tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, target_ids = train_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "print(f\"First 10 input tokens: {input_ids[:10]}\")\n",
    "print(f\"First 10 target tokens: {target_ids[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d81b5",
   "metadata": {},
   "source": [
    "# Now we need to batch our data(Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 144\n",
      "Batch size: 32\n",
      "\n",
      "First batch:\n",
      "Input shape:  torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,      # 32 sequences per batch\n",
    "    shuffle=True,       # Shuffle data each epoch\n",
    "    num_workers=0       # Data loading processes (0 = main process)\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "\n",
    "# Get first batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, target_ids = batch\n",
    "\n",
    "print(f\"\\nFirst batch:\")\n",
    "print(f\"Input shape:  {input_ids.shape}\")   # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8c895",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "```\n",
    "Loss(So loss is computed for each token)(Define the Loss)\n",
    "↓\n",
    "Optimizer(Optimizer updates the model's weights to minimize loss.)\n",
    "↓\n",
    "Training Loop(Forward-Loss-clear Gradients-Backward-Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2aa6213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 5])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "\n",
      "After reshaping:\n",
      "Logits flat: torch.Size([6, 5])\n",
      "Targets flat: torch.Size([6])\n",
      "\n",
      "Loss: 2.0549\n"
     ]
    }
   ],
   "source": [
    "#Define the Loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake model output (logits)\n",
    "# [batch=2, seq=3, vocab_size=5]\n",
    "logits = torch.randn(2, 3, 5)\n",
    "\n",
    "# Target token IDs\n",
    "# [batch=2, seq=3]\n",
    "targets = torch.tensor([\n",
    "    [2, 4, 1],  # Sequence 1: correct tokens are 2, 4, 1\n",
    "    [0, 3, 2]   # Sequence 2: correct tokens are 0, 3, 2\n",
    "])\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)    # [2, 3, 5]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# BUT! CrossEntropyLoss expects:\n",
    "# logits: [N, vocab_size] where N = batch * seq\n",
    "# targets: [N]\n",
    "\n",
    "# So we need to reshape!\n",
    "logits_flat = logits.view(-1, 5)      # [6, 5]  (2*3=6 positions)\n",
    "targets_flat = targets.view(-1)        # [6]\n",
    "\n",
    "print(\"\\nAfter reshaping:\")\n",
    "print(\"Logits flat:\", logits_flat.shape)   # [6, 5]\n",
    "print(\"Targets flat:\", targets_flat.shape) # [6]\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 512])\n",
      "Target shape: torch.Size([32, 512])\n",
      "Logits shape: torch.Size([32, 512, 50257])\n",
      "\n",
      "Flattened logits: torch.Size([16384, 50257])\n",
      "Flattened targets: torch.Size([16384])\n",
      "\n",
      "Loss: 11.0243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate model training\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    max_seq_len=512,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Get a batch from dataloader\n",
    "input_ids, target_ids = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")    # [32, 512]\n",
    "print(f\"Target shape: {target_ids.shape}\")  # [32, 512]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids)\n",
    "print(f\"Logits shape: {logits.shape}\")      # [32, 512, 50257]\n",
    "\n",
    "# Reshape for loss calculation\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "logits_flat = logits.view(batch_size * seq_len, vocab_size)  # [16384, 50257]\n",
    "targets_flat = target_ids.view(batch_size * seq_len)         # [16384]\n",
    "\n",
    "print(f\"\\nFlattened logits: {logits_flat.shape}\")\n",
    "print(f\"Flattened targets: {targets_flat.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_flat, targets_flat)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created!\n",
      "Learning rate: 3e-4 = 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),  # All model weights to optimize\n",
    "    lr=3e-4,            # Learning rate (how big each update is)\n",
    "    weight_decay=0.01   # Regularization (prevents overfitting)\n",
    ")\n",
    "\n",
    "print(\"Optimizer created!\")\n",
    "print(f\"Learning rate: 3e-4 = {3e-4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82fca023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Training ===\n",
      "Weight sample: -0.2838\n",
      "Loss: 1.8065\n",
      "Gradient sample: -0.151754\n",
      "=== After Training ===\n",
      "Weight sample: -0.2828\n",
      "Weight changed! ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(2, 10)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "print(\"=== Before Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()           # Clear old gradients\n",
    "output = model(x)               # Forward\n",
    "loss = criterion(output, targets)  # Loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "loss.backward()                 # Calculate gradients\n",
    "print(f\"Gradient sample: {model.weight.grad[0, 0].item():.6f}\")\n",
    "\n",
    "optimizer.step()                # Update weights\n",
    "\n",
    "print(\"=== After Training ===\")\n",
    "print(f\"Weight sample: {model.weight[0, 0].item():.4f}\")\n",
    "print(\"Weight changed! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2794887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the language model\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.train()     # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            # Move data to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)  # [batch, seq, vocab]\n",
    "            \n",
    "            # Reshape for loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bf95b",
   "metadata": {},
   "source": [
    "# Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a23b0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after clearing: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after clearing: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after cleanup: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete ALL variables\n",
    "del model\n",
    "del optimizer\n",
    "del criterion\n",
    "del train_loader\n",
    "del train_dataset\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22332564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c80b82048e470b9012b2fe197a985d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057796956ac43cfa9c6440b034b37e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42992a4da2054fd4926e7eebd10a7f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc021f20604e3ebe39ca40180275e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d53f74a0349d7bbc222f81c40d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6d4ba893344c00a4d8dd3db6c882e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31973e826bd49358453ab05aa904754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5640029c2af348c2a3f9742ead92beec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5440a1482c14aa0b745ae720856390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc681d0ee0834c28bb41000258030d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beb87617ddb4006aa7c1afcf89bed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d975c24fa52a41c5baf31f003f6a5d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Check GPU is clean\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  # Should be 0!\n",
    "\n",
    "# 3. Load data (only what we need)\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde39b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial GPU Memory: 0.00 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Dataset ready. GPU Memory: 0.00 GB\n",
      "Model created. GPU Memory: 0.00 GB\n",
      "Parameters: 3,399,569\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# ============================================\n",
    "# 1. COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "# MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        #This initializes the linear layers for query, key, value and output projections\n",
    "        #Linear layers are just weight matrices that will be learned during training\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Dropout layer for regularization this is used to prevent overfitting by randomly setting some of the attention weights to zero during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Here we r unpack the input tensor x to get batch size and sequence length\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        #Here we r passing the input x through the linear layers to get the query, key, and value matrices\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        # Now we reshape Q, K, V to separate the heads for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ===== ADD CAUSAL MASKING HERE (NEW CODE) =====\n",
    "        #NOTE: Causal masking is essential in autoregressive models to ensure that each token can only attend to previous tokens and itself, preventing any \"future\" information leakage.\n",
    "        # Step 1: Get the sequence length from scores tensor\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        \n",
    "        # Step 2: Create causal mask (lower triangular matrix)\n",
    "        # torch.tril creates a matrix where positions above diagonal are 0, below/on diagonal are 1\n",
    "        # This allows each position to attend only to itself and previous positions\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        \n",
    "        # Step 3: Create boolean mask for positions to BLOCK\n",
    "        # Where causal_mask == 0 (upper triangle), we want to block attention\n",
    "        # These are the \"future\" positions that current token should not see\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        \n",
    "        # Step 4: Add batch and head dimensions for broadcasting\n",
    "        # Original mask shape: (seq_len, seq_len)\n",
    "        # After unsqueeze: (1, 1, seq_len, seq_len)\n",
    "        # This allows broadcasting across all batches and heads\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Step 5: Apply masked_fill - set future positions to -inf\n",
    "        # When softmax is applied, exp(-inf) = 0, effectively blocking attention to future tokens\n",
    "        # This preserves the autoregressive property: token at position i cannot see tokens at positions > i\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        # ===== END NEW CODE =====\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Handle any NaN values that might arise from softmax(-inf)\n",
    "        # If an entire row is -inf (shouldn't happen in practice), softmax creates NaN\n",
    "        # Replace any NaN with 0.0 for numerical stability\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# FeedForward class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# TransformerBlock class\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "# LanguageModel class\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# WikiTextDataset class\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "# train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. SETUP\n",
    "# ============================================\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# 3. CREATE MODEL (TINY!)\n",
    "# ============================================\n",
    "model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=16,        # SMALLER!\n",
    "    num_heads=2,\n",
    "    d_ff=64,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"Model created. GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. OPTIMIZER & LOSS\n",
    "# ============================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ============================================\n",
    "# 5. TRAIN!\n",
    "# ============================================\n",
    "\n",
    "# print(\"\\nStarting training...\\n\")\n",
    "# train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
    "\n",
    "# print(\"TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f50a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.23 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "MHA Model parameters: 70,559,825\n",
      "GPU Memory after model: 0.41 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear GPU first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=256)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=256)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Create PROPER MHA model (bigger than test)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=256,\n",
    "    d_model=512,     # Assignment size\n",
    "    num_heads=8,     # Assignment size  \n",
    "    d_ff=2048,       # Assignment size\n",
    "    num_layers=6     # Assignment size\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"MHA Model parameters: {sum(p.numel() for p in mha_model.parameters()):,}\")\n",
    "print(f\"GPU Memory after model: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afed9b",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and calculate perplexity\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape and calculate loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()  # Back to training mode\n",
    "    return avg_loss, perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aa804",
   "metadata": {},
   "source": [
    "# Training MHA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d272dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING MHA MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1146/1146 [01:36<00:00, 11.82it/s, loss=6.6002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 6.6002\n",
      "  Val Loss: 6.2297\n",
      "  Val Perplexity: 507.59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.76it/s, loss=5.6089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 5.6089\n",
      "  Val Loss: 5.8502\n",
      "  Val Perplexity: 347.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1146/1146 [01:37<00:00, 11.74it/s, loss=5.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 5.0045\n",
      "  Val Loss: 5.6814\n",
      "  Val Perplexity: 293.35\n",
      "\n",
      "============================================================\n",
      "MHA TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MHA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model = mha_model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "        targets_flat = target_ids.view(batch_size * seq_len)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_perplexity = validate_model(mha_model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {total_loss/num_batches:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Perplexity: {val_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807d841",
   "metadata": {},
   "source": [
    "# Save the MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e78c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA model saved as 'mha_model.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': mha_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': 0.0419,\n",
    "    'val_loss': 0.0996,\n",
    "    'val_perplexity': 1.10,\n",
    "}, 'mha_model.pt')\n",
    "\n",
    "print(\"✅ MHA model saved as 'mha_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e73e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The history of\n",
      "Generating...\n",
      "\n",
      "Output: The history of New Hampshire County , it provides a tropical wave that turned into the southeast of June and its southern Africa . The depression was upgraded through two days before the\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: In mathematics,\n",
      "Generating...\n",
      "\n",
      "Output: In mathematics,@ 000 men in the race , allowing several occasions , and a third half of five of 100 miles ( 36 km / h ) . After five hours\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prompt: The cat sat on the\n",
      "Generating...\n",
      "\n",
      "Output: The cat sat on the west of the tower and west bank before entering the intersection with the north that attracts their north is about 12 @.@ 7 miles ( 0 @.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_fixed(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate text with top-k sampling (more stable)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generating...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # TOP-K SAMPLING (key difference!)\n",
    "            # Only consider top 50 most likely tokens\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test with better sampling\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_fixed(mha_model, tokenizer, prompt, max_length=30, top_k=50, device=device)\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0217fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.11 GB\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "======================================================================\n",
      "TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\n",
      "======================================================================\n",
      "Parameters: 3,399,569\n",
      "Model Size: 3.40M (vs 70.7M before)\n",
      "Size Reduction: 20.8x smaller\n",
      "GPU Memory: 0.10 GB\n",
      "\n",
      "Dataset tokens: 2,347,038\n",
      "Chinchilla optimal params: 117,352\n",
      "Our model params: 3,399,569\n",
      "Ratio (ours/optimal): 29.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=17.2818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 17.2818\n",
      "  Val Loss: 9.4304\n",
      "  Val Perplexity: 12461.92\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 573/573 [00:21<00:00, 26.35it/s, loss=9.0392] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 9.0392\n",
      "  Val Loss: 7.7933\n",
      "  Val Perplexity: 2424.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=7.9895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 7.9895\n",
      "  Val Loss: 7.4918\n",
      "  Val Perplexity: 1793.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=7.6620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 7.6620\n",
      "  Val Loss: 7.3710\n",
      "  Val Perplexity: 1589.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=7.4919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 7.4919\n",
      "  Val Loss: 7.3031\n",
      "  Val Perplexity: 1484.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=7.3856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 7.3856\n",
      "  Val Loss: 7.2611\n",
      "  Val Perplexity: 1423.77\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=7.3077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 7.3077\n",
      "  Val Loss: 7.2225\n",
      "  Val Perplexity: 1369.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=7.2472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 7.2472\n",
      "  Val Loss: 7.1941\n",
      "  Val Perplexity: 1331.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.1967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 7.1967\n",
      "  Val Loss: 7.1640\n",
      "  Val Perplexity: 1292.12\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=7.1518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 7.1518\n",
      "  Val Loss: 7.1366\n",
      "  Val Perplexity: 1257.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=7.1103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11:\n",
      "  Train Loss: 7.1103\n",
      "  Val Loss: 7.1141\n",
      "  Val Perplexity: 1229.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:\n",
      "  Train Loss: 7.0726\n",
      "  Val Loss: 7.0844\n",
      "  Val Perplexity: 1193.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=7.0371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:\n",
      "  Train Loss: 7.0371\n",
      "  Val Loss: 7.0547\n",
      "  Val Perplexity: 1158.31\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=7.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:\n",
      "  Train Loss: 7.0039\n",
      "  Val Loss: 7.0269\n",
      "  Val Perplexity: 1126.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.9719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "  Train Loss: 6.9719\n",
      "  Val Loss: 6.9973\n",
      "  Val Perplexity: 1093.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=6.9402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:\n",
      "  Train Loss: 6.9402\n",
      "  Val Loss: 6.9682\n",
      "  Val Perplexity: 1062.30\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.9086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:\n",
      "  Train Loss: 6.9086\n",
      "  Val Loss: 6.9382\n",
      "  Val Perplexity: 1030.94\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.8775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:\n",
      "  Train Loss: 6.8775\n",
      "  Val Loss: 6.9086\n",
      "  Val Perplexity: 1000.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.8468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:\n",
      "  Train Loss: 6.8468\n",
      "  Val Loss: 6.8807\n",
      "  Val Perplexity: 973.33\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/300: 100%|██████████| 573/573 [00:22<00:00, 25.78it/s, loss=6.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Train Loss: 6.8150\n",
      "  Val Loss: 6.8526\n",
      "  Val Perplexity: 946.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=6.7826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:\n",
      "  Train Loss: 6.7826\n",
      "  Val Loss: 6.8194\n",
      "  Val Perplexity: 915.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=6.7494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:\n",
      "  Train Loss: 6.7494\n",
      "  Val Loss: 6.7898\n",
      "  Val Perplexity: 888.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.7157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23:\n",
      "  Train Loss: 6.7157\n",
      "  Val Loss: 6.7597\n",
      "  Val Perplexity: 862.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/300: 100%|██████████| 573/573 [00:22<00:00, 25.97it/s, loss=6.6823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:\n",
      "  Train Loss: 6.6823\n",
      "  Val Loss: 6.7281\n",
      "  Val Perplexity: 835.59\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=6.6478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:\n",
      "  Train Loss: 6.6478\n",
      "  Val Loss: 6.6979\n",
      "  Val Perplexity: 810.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=6.6135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:\n",
      "  Train Loss: 6.6135\n",
      "  Val Loss: 6.6672\n",
      "  Val Perplexity: 786.16\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=6.5799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:\n",
      "  Train Loss: 6.5799\n",
      "  Val Loss: 6.6371\n",
      "  Val Perplexity: 762.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=6.5470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:\n",
      "  Train Loss: 6.5470\n",
      "  Val Loss: 6.6070\n",
      "  Val Perplexity: 740.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.5139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:\n",
      "  Train Loss: 6.5139\n",
      "  Val Loss: 6.5786\n",
      "  Val Perplexity: 719.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.4835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:\n",
      "  Train Loss: 6.4835\n",
      "  Val Loss: 6.5555\n",
      "  Val Perplexity: 703.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=6.4533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:\n",
      "  Train Loss: 6.4533\n",
      "  Val Loss: 6.5314\n",
      "  Val Perplexity: 686.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=6.4249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:\n",
      "  Train Loss: 6.4249\n",
      "  Val Loss: 6.5110\n",
      "  Val Perplexity: 672.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=6.3976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:\n",
      "  Train Loss: 6.3976\n",
      "  Val Loss: 6.4873\n",
      "  Val Perplexity: 656.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=6.3709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:\n",
      "  Train Loss: 6.3709\n",
      "  Val Loss: 6.4699\n",
      "  Val Perplexity: 645.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=6.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:\n",
      "  Train Loss: 6.3447\n",
      "  Val Loss: 6.4494\n",
      "  Val Perplexity: 632.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.3202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:\n",
      "  Train Loss: 6.3202\n",
      "  Val Loss: 6.4301\n",
      "  Val Perplexity: 620.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.2953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:\n",
      "  Train Loss: 6.2953\n",
      "  Val Loss: 6.4136\n",
      "  Val Perplexity: 610.11\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=6.2716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:\n",
      "  Train Loss: 6.2716\n",
      "  Val Loss: 6.3966\n",
      "  Val Perplexity: 599.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=6.2481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:\n",
      "  Train Loss: 6.2481\n",
      "  Val Loss: 6.3787\n",
      "  Val Perplexity: 589.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=6.2248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:\n",
      "  Train Loss: 6.2248\n",
      "  Val Loss: 6.3651\n",
      "  Val Perplexity: 581.18\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=6.2026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:\n",
      "  Train Loss: 6.2026\n",
      "  Val Loss: 6.3507\n",
      "  Val Perplexity: 572.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=6.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:\n",
      "  Train Loss: 6.1795\n",
      "  Val Loss: 6.3363\n",
      "  Val Perplexity: 564.69\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=6.1574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:\n",
      "  Train Loss: 6.1574\n",
      "  Val Loss: 6.3197\n",
      "  Val Perplexity: 555.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=6.1353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:\n",
      "  Train Loss: 6.1353\n",
      "  Val Loss: 6.3071\n",
      "  Val Perplexity: 548.47\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=6.1128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:\n",
      "  Train Loss: 6.1128\n",
      "  Val Loss: 6.2887\n",
      "  Val Perplexity: 538.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/300: 100%|██████████| 573/573 [00:22<00:00, 25.87it/s, loss=6.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:\n",
      "  Train Loss: 6.0909\n",
      "  Val Loss: 6.2757\n",
      "  Val Perplexity: 531.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=6.0695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:\n",
      "  Train Loss: 6.0695\n",
      "  Val Loss: 6.2647\n",
      "  Val Perplexity: 525.70\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=6.0470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:\n",
      "  Train Loss: 6.0470\n",
      "  Val Loss: 6.2507\n",
      "  Val Perplexity: 518.40\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=6.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49:\n",
      "  Train Loss: 6.0255\n",
      "  Val Loss: 6.2357\n",
      "  Val Perplexity: 510.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=6.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50:\n",
      "  Train Loss: 6.0039\n",
      "  Val Loss: 6.2240\n",
      "  Val Perplexity: 504.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.9823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51:\n",
      "  Train Loss: 5.9823\n",
      "  Val Loss: 6.2094\n",
      "  Val Perplexity: 497.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.9610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52:\n",
      "  Train Loss: 5.9610\n",
      "  Val Loss: 6.1995\n",
      "  Val Perplexity: 492.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=5.9388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53:\n",
      "  Train Loss: 5.9388\n",
      "  Val Loss: 6.1850\n",
      "  Val Perplexity: 485.41\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.9185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54:\n",
      "  Train Loss: 5.9185\n",
      "  Val Loss: 6.1737\n",
      "  Val Perplexity: 479.98\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.8972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55:\n",
      "  Train Loss: 5.8972\n",
      "  Val Loss: 6.1587\n",
      "  Val Perplexity: 472.83\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.8764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56:\n",
      "  Train Loss: 5.8764\n",
      "  Val Loss: 6.1517\n",
      "  Val Perplexity: 469.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.8560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57:\n",
      "  Train Loss: 5.8560\n",
      "  Val Loss: 6.1389\n",
      "  Val Perplexity: 463.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.8352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58:\n",
      "  Train Loss: 5.8352\n",
      "  Val Loss: 6.1257\n",
      "  Val Perplexity: 457.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.8150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59:\n",
      "  Train Loss: 5.8150\n",
      "  Val Loss: 6.1153\n",
      "  Val Perplexity: 452.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.7939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60:\n",
      "  Train Loss: 5.7939\n",
      "  Val Loss: 6.1075\n",
      "  Val Perplexity: 449.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=5.7743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61:\n",
      "  Train Loss: 5.7743\n",
      "  Val Loss: 6.0926\n",
      "  Val Perplexity: 442.57\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=5.7544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62:\n",
      "  Train Loss: 5.7544\n",
      "  Val Loss: 6.0851\n",
      "  Val Perplexity: 439.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.7345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63:\n",
      "  Train Loss: 5.7345\n",
      "  Val Loss: 6.0757\n",
      "  Val Perplexity: 435.15\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.7148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64:\n",
      "  Train Loss: 5.7148\n",
      "  Val Loss: 6.0636\n",
      "  Val Perplexity: 429.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65:\n",
      "  Train Loss: 5.6955\n",
      "  Val Loss: 6.0552\n",
      "  Val Perplexity: 426.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.6763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66:\n",
      "  Train Loss: 5.6763\n",
      "  Val Loss: 6.0447\n",
      "  Val Perplexity: 421.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.6568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67:\n",
      "  Train Loss: 5.6568\n",
      "  Val Loss: 6.0344\n",
      "  Val Perplexity: 417.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/300: 100%|██████████| 573/573 [00:22<00:00, 25.95it/s, loss=5.6381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68:\n",
      "  Train Loss: 5.6381\n",
      "  Val Loss: 6.0232\n",
      "  Val Perplexity: 412.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=5.6185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69:\n",
      "  Train Loss: 5.6185\n",
      "  Val Loss: 6.0126\n",
      "  Val Perplexity: 408.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=5.5998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70:\n",
      "  Train Loss: 5.5998\n",
      "  Val Loss: 6.0051\n",
      "  Val Perplexity: 405.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.5808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71:\n",
      "  Train Loss: 5.5808\n",
      "  Val Loss: 5.9956\n",
      "  Val Perplexity: 401.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72:\n",
      "  Train Loss: 5.5614\n",
      "  Val Loss: 5.9816\n",
      "  Val Perplexity: 396.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.5438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73:\n",
      "  Train Loss: 5.5438\n",
      "  Val Loss: 5.9745\n",
      "  Val Perplexity: 393.26\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.5256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74:\n",
      "  Train Loss: 5.5256\n",
      "  Val Loss: 5.9710\n",
      "  Val Perplexity: 391.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=5.5069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75:\n",
      "  Train Loss: 5.5069\n",
      "  Val Loss: 5.9571\n",
      "  Val Perplexity: 386.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/300: 100%|██████████| 573/573 [00:22<00:00, 25.84it/s, loss=5.4891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76:\n",
      "  Train Loss: 5.4891\n",
      "  Val Loss: 5.9491\n",
      "  Val Perplexity: 383.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/300: 100%|██████████| 573/573 [00:22<00:00, 25.88it/s, loss=5.4718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77:\n",
      "  Train Loss: 5.4718\n",
      "  Val Loss: 5.9448\n",
      "  Val Perplexity: 381.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/300: 100%|██████████| 573/573 [00:22<00:00, 25.82it/s, loss=5.4534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78:\n",
      "  Train Loss: 5.4534\n",
      "  Val Loss: 5.9369\n",
      "  Val Perplexity: 378.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=5.4358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79:\n",
      "  Train Loss: 5.4358\n",
      "  Val Loss: 5.9293\n",
      "  Val Perplexity: 375.89\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=5.4189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80:\n",
      "  Train Loss: 5.4189\n",
      "  Val Loss: 5.9201\n",
      "  Val Perplexity: 372.46\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=5.4020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81:\n",
      "  Train Loss: 5.4020\n",
      "  Val Loss: 5.9130\n",
      "  Val Perplexity: 369.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/300: 100%|██████████| 573/573 [00:21<00:00, 26.29it/s, loss=5.3851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82:\n",
      "  Train Loss: 5.3851\n",
      "  Val Loss: 5.9069\n",
      "  Val Perplexity: 367.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/300: 100%|██████████| 573/573 [00:21<00:00, 26.24it/s, loss=5.3686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83:\n",
      "  Train Loss: 5.3686\n",
      "  Val Loss: 5.8972\n",
      "  Val Perplexity: 364.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.3516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84:\n",
      "  Train Loss: 5.3516\n",
      "  Val Loss: 5.8922\n",
      "  Val Perplexity: 362.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=5.3350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85:\n",
      "  Train Loss: 5.3350\n",
      "  Val Loss: 5.8852\n",
      "  Val Perplexity: 359.67\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/300: 100%|██████████| 573/573 [00:21<00:00, 26.28it/s, loss=5.3195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86:\n",
      "  Train Loss: 5.3195\n",
      "  Val Loss: 5.8777\n",
      "  Val Perplexity: 357.00\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.3030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87:\n",
      "  Train Loss: 5.3030\n",
      "  Val Loss: 5.8724\n",
      "  Val Perplexity: 355.09\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/300: 100%|██████████| 573/573 [00:22<00:00, 25.90it/s, loss=5.2877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88:\n",
      "  Train Loss: 5.2877\n",
      "  Val Loss: 5.8661\n",
      "  Val Perplexity: 352.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.2721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89:\n",
      "  Train Loss: 5.2721\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 351.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90:\n",
      "  Train Loss: 5.2560\n",
      "  Val Loss: 5.8551\n",
      "  Val Perplexity: 349.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=5.2417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91:\n",
      "  Train Loss: 5.2417\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.27\n",
      "  📊 Best model still at Epoch 90 (Val Loss: 5.8551)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.2266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92:\n",
      "  Train Loss: 5.2266\n",
      "  Val Loss: 5.8484\n",
      "  Val Perplexity: 346.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.2121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93:\n",
      "  Train Loss: 5.2121\n",
      "  Val Loss: 5.8430\n",
      "  Val Perplexity: 344.82\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=5.1973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94:\n",
      "  Train Loss: 5.1973\n",
      "  Val Loss: 5.8386\n",
      "  Val Perplexity: 343.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/300: 100%|██████████| 573/573 [00:21<00:00, 26.26it/s, loss=5.1825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95:\n",
      "  Train Loss: 5.1825\n",
      "  Val Loss: 5.8325\n",
      "  Val Perplexity: 341.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96:\n",
      "  Train Loss: 5.1687\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.74\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=5.1546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97:\n",
      "  Train Loss: 5.1546\n",
      "  Val Loss: 5.8275\n",
      "  Val Perplexity: 339.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=5.1403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98:\n",
      "  Train Loss: 5.1403\n",
      "  Val Loss: 5.8282\n",
      "  Val Perplexity: 339.76\n",
      "  📊 Best model still at Epoch 97 (Val Loss: 5.8275)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=5.1272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99:\n",
      "  Train Loss: 5.1272\n",
      "  Val Loss: 5.8204\n",
      "  Val Perplexity: 337.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=5.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100:\n",
      "  Train Loss: 5.1135\n",
      "  Val Loss: 5.8191\n",
      "  Val Perplexity: 336.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.1004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101:\n",
      "  Train Loss: 5.1004\n",
      "  Val Loss: 5.8183\n",
      "  Val Perplexity: 336.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102:\n",
      "  Train Loss: 5.0872\n",
      "  Val Loss: 5.8113\n",
      "  Val Perplexity: 334.06\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=5.0753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103:\n",
      "  Train Loss: 5.0753\n",
      "  Val Loss: 5.8030\n",
      "  Val Perplexity: 331.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=5.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104:\n",
      "  Train Loss: 5.0623\n",
      "  Val Loss: 5.8014\n",
      "  Val Perplexity: 330.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=5.0495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105:\n",
      "  Train Loss: 5.0495\n",
      "  Val Loss: 5.8080\n",
      "  Val Perplexity: 332.96\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=5.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106:\n",
      "  Train Loss: 5.0374\n",
      "  Val Loss: 5.8018\n",
      "  Val Perplexity: 330.91\n",
      "  📊 Best model still at Epoch 104 (Val Loss: 5.8014)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=5.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107:\n",
      "  Train Loss: 5.0251\n",
      "  Val Loss: 5.7969\n",
      "  Val Perplexity: 329.27\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=5.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108:\n",
      "  Train Loss: 5.0142\n",
      "  Val Loss: 5.7983\n",
      "  Val Perplexity: 329.75\n",
      "  📊 Best model still at Epoch 107 (Val Loss: 5.7969)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=5.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109:\n",
      "  Train Loss: 5.0020\n",
      "  Val Loss: 5.7950\n",
      "  Val Perplexity: 328.65\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110:\n",
      "  Train Loss: 4.9903\n",
      "  Val Loss: 5.7906\n",
      "  Val Perplexity: 327.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.9797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111:\n",
      "  Train Loss: 4.9797\n",
      "  Val Loss: 5.7909\n",
      "  Val Perplexity: 327.29\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112:\n",
      "  Train Loss: 4.9691\n",
      "  Val Loss: 5.7951\n",
      "  Val Perplexity: 328.68\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.9580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113:\n",
      "  Train Loss: 4.9580\n",
      "  Val Loss: 5.7962\n",
      "  Val Perplexity: 329.06\n",
      "  📊 Best model still at Epoch 110 (Val Loss: 5.7906)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.9469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114:\n",
      "  Train Loss: 4.9469\n",
      "  Val Loss: 5.7853\n",
      "  Val Perplexity: 325.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/300: 100%|██████████| 573/573 [00:21<00:00, 26.23it/s, loss=4.9363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115:\n",
      "  Train Loss: 4.9363\n",
      "  Val Loss: 5.7926\n",
      "  Val Perplexity: 327.85\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.9268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116:\n",
      "  Train Loss: 4.9268\n",
      "  Val Loss: 5.7856\n",
      "  Val Perplexity: 325.58\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.9171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117:\n",
      "  Train Loss: 4.9171\n",
      "  Val Loss: 5.7922\n",
      "  Val Perplexity: 327.74\n",
      "  📊 Best model still at Epoch 114 (Val Loss: 5.7853)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.9066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118:\n",
      "  Train Loss: 4.9066\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.8967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119:\n",
      "  Train Loss: 4.8967\n",
      "  Val Loss: 5.7872\n",
      "  Val Perplexity: 326.09\n",
      "  📊 Best model still at Epoch 118 (Val Loss: 5.7850)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120:\n",
      "  Train Loss: 4.8883\n",
      "  Val Loss: 5.7828\n",
      "  Val Perplexity: 324.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/300: 100%|██████████| 573/573 [00:21<00:00, 26.15it/s, loss=4.8788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121:\n",
      "  Train Loss: 4.8788\n",
      "  Val Loss: 5.7842\n",
      "  Val Perplexity: 325.11\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.8692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122:\n",
      "  Train Loss: 4.8692\n",
      "  Val Loss: 5.7858\n",
      "  Val Perplexity: 325.65\n",
      "  📊 Best model still at Epoch 120 (Val Loss: 5.7828)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.8605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123:\n",
      "  Train Loss: 4.8605\n",
      "  Val Loss: 5.7808\n",
      "  Val Perplexity: 324.02\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.8523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124:\n",
      "  Train Loss: 4.8523\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.27\n",
      "  📊 Best model still at Epoch 123 (Val Loss: 5.7808)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/300: 100%|██████████| 573/573 [00:21<00:00, 26.12it/s, loss=4.8435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125:\n",
      "  Train Loss: 4.8435\n",
      "  Val Loss: 5.7782\n",
      "  Val Perplexity: 323.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.8349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126:\n",
      "  Train Loss: 4.8349\n",
      "  Val Loss: 5.7851\n",
      "  Val Perplexity: 325.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.8264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127:\n",
      "  Train Loss: 4.8264\n",
      "  Val Loss: 5.7815\n",
      "  Val Perplexity: 324.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.8189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128:\n",
      "  Train Loss: 4.8189\n",
      "  Val Loss: 5.7861\n",
      "  Val Perplexity: 325.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.8105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129:\n",
      "  Train Loss: 4.8105\n",
      "  Val Loss: 5.7850\n",
      "  Val Perplexity: 325.38\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/300: 100%|██████████| 573/573 [00:21<00:00, 26.08it/s, loss=4.8032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130:\n",
      "  Train Loss: 4.8032\n",
      "  Val Loss: 5.7830\n",
      "  Val Perplexity: 324.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/300: 100%|██████████| 573/573 [00:21<00:00, 26.16it/s, loss=4.7954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131:\n",
      "  Train Loss: 4.7954\n",
      "  Val Loss: 5.7816\n",
      "  Val Perplexity: 324.29\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/300: 100%|██████████| 573/573 [00:21<00:00, 26.30it/s, loss=4.7874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132:\n",
      "  Train Loss: 4.7874\n",
      "  Val Loss: 5.7833\n",
      "  Val Perplexity: 324.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.7810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133:\n",
      "  Train Loss: 4.7810\n",
      "  Val Loss: 5.7840\n",
      "  Val Perplexity: 325.07\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134:\n",
      "  Train Loss: 4.7739\n",
      "  Val Loss: 5.7885\n",
      "  Val Perplexity: 326.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.7667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135:\n",
      "  Train Loss: 4.7667\n",
      "  Val Loss: 5.7910\n",
      "  Val Perplexity: 327.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136:\n",
      "  Train Loss: 4.7597\n",
      "  Val Loss: 5.7893\n",
      "  Val Perplexity: 326.79\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/300: 100%|██████████| 573/573 [00:21<00:00, 26.22it/s, loss=4.7533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137:\n",
      "  Train Loss: 4.7533\n",
      "  Val Loss: 5.7864\n",
      "  Val Perplexity: 325.85\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.7473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138:\n",
      "  Train Loss: 4.7473\n",
      "  Val Loss: 5.7855\n",
      "  Val Perplexity: 325.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139:\n",
      "  Train Loss: 4.7406\n",
      "  Val Loss: 5.7886\n",
      "  Val Perplexity: 326.54\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/300: 100%|██████████| 573/573 [00:22<00:00, 25.83it/s, loss=4.7337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140:\n",
      "  Train Loss: 4.7337\n",
      "  Val Loss: 5.7882\n",
      "  Val Perplexity: 326.43\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/300: 100%|██████████| 573/573 [00:22<00:00, 25.94it/s, loss=4.7287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141:\n",
      "  Train Loss: 4.7287\n",
      "  Val Loss: 5.7875\n",
      "  Val Perplexity: 326.19\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.7231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142:\n",
      "  Train Loss: 4.7231\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.7168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143:\n",
      "  Train Loss: 4.7168\n",
      "  Val Loss: 5.7946\n",
      "  Val Perplexity: 328.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/300: 100%|██████████| 573/573 [00:21<00:00, 26.20it/s, loss=4.7111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144:\n",
      "  Train Loss: 4.7111\n",
      "  Val Loss: 5.7941\n",
      "  Val Perplexity: 328.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.7053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145:\n",
      "  Train Loss: 4.7053\n",
      "  Val Loss: 5.7961\n",
      "  Val Perplexity: 329.02\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/300: 100%|██████████| 573/573 [00:21<00:00, 26.09it/s, loss=4.7015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146:\n",
      "  Train Loss: 4.7015\n",
      "  Val Loss: 5.7959\n",
      "  Val Perplexity: 328.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/300: 100%|██████████| 573/573 [00:22<00:00, 25.93it/s, loss=4.6960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147:\n",
      "  Train Loss: 4.6960\n",
      "  Val Loss: 5.7968\n",
      "  Val Perplexity: 329.23\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/300: 100%|██████████| 573/573 [00:21<00:00, 26.27it/s, loss=4.6905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148:\n",
      "  Train Loss: 4.6905\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.84\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.6853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149:\n",
      "  Train Loss: 4.6853\n",
      "  Val Loss: 5.8067\n",
      "  Val Perplexity: 332.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150:\n",
      "  Train Loss: 4.6807\n",
      "  Val Loss: 5.8016\n",
      "  Val Perplexity: 330.82\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.6765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151:\n",
      "  Train Loss: 4.6765\n",
      "  Val Loss: 5.8083\n",
      "  Val Perplexity: 333.06\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/300: 100%|██████████| 573/573 [00:21<00:00, 26.06it/s, loss=4.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152:\n",
      "  Train Loss: 4.6715\n",
      "  Val Loss: 5.8106\n",
      "  Val Perplexity: 333.81\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.6671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153:\n",
      "  Train Loss: 4.6671\n",
      "  Val Loss: 5.8089\n",
      "  Val Perplexity: 333.27\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154:\n",
      "  Train Loss: 4.6625\n",
      "  Val Loss: 5.8132\n",
      "  Val Perplexity: 334.67\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.6580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155:\n",
      "  Train Loss: 4.6580\n",
      "  Val Loss: 5.8153\n",
      "  Val Perplexity: 335.40\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156:\n",
      "  Train Loss: 4.6547\n",
      "  Val Loss: 5.8108\n",
      "  Val Perplexity: 333.88\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/300: 100%|██████████| 573/573 [00:22<00:00, 25.81it/s, loss=4.6494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157:\n",
      "  Train Loss: 4.6494\n",
      "  Val Loss: 5.8130\n",
      "  Val Perplexity: 334.63\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/300: 100%|██████████| 573/573 [00:21<00:00, 26.07it/s, loss=4.6463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158:\n",
      "  Train Loss: 4.6463\n",
      "  Val Loss: 5.8192\n",
      "  Val Perplexity: 336.72\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/300: 100%|██████████| 573/573 [00:22<00:00, 25.86it/s, loss=4.6418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159:\n",
      "  Train Loss: 4.6418\n",
      "  Val Loss: 5.8141\n",
      "  Val Perplexity: 335.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160:\n",
      "  Train Loss: 4.6382\n",
      "  Val Loss: 5.8186\n",
      "  Val Perplexity: 336.52\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.6342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161:\n",
      "  Train Loss: 4.6342\n",
      "  Val Loss: 5.8263\n",
      "  Val Perplexity: 339.10\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/300: 100%|██████████| 573/573 [00:22<00:00, 26.04it/s, loss=4.6309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162:\n",
      "  Train Loss: 4.6309\n",
      "  Val Loss: 5.8299\n",
      "  Val Perplexity: 340.32\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/300: 100%|██████████| 573/573 [00:22<00:00, 26.01it/s, loss=4.6272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163:\n",
      "  Train Loss: 4.6272\n",
      "  Val Loss: 5.8290\n",
      "  Val Perplexity: 340.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/300: 100%|██████████| 573/573 [00:21<00:00, 26.05it/s, loss=4.6243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164:\n",
      "  Train Loss: 4.6243\n",
      "  Val Loss: 5.8307\n",
      "  Val Perplexity: 340.58\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.6207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165:\n",
      "  Train Loss: 4.6207\n",
      "  Val Loss: 5.8317\n",
      "  Val Perplexity: 340.94\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166:\n",
      "  Train Loss: 4.6168\n",
      "  Val Loss: 5.8319\n",
      "  Val Perplexity: 341.00\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/300: 100%|██████████| 573/573 [00:22<00:00, 25.92it/s, loss=4.6140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167:\n",
      "  Train Loss: 4.6140\n",
      "  Val Loss: 5.8300\n",
      "  Val Perplexity: 340.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/300: 100%|██████████| 573/573 [00:22<00:00, 25.61it/s, loss=4.6106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168:\n",
      "  Train Loss: 4.6106\n",
      "  Val Loss: 5.8423\n",
      "  Val Perplexity: 344.56\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/300: 100%|██████████| 573/573 [00:22<00:00, 26.03it/s, loss=4.6070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169:\n",
      "  Train Loss: 4.6070\n",
      "  Val Loss: 5.8412\n",
      "  Val Perplexity: 344.20\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/300: 100%|██████████| 573/573 [00:22<00:00, 25.74it/s, loss=4.6043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170:\n",
      "  Train Loss: 4.6043\n",
      "  Val Loss: 5.8384\n",
      "  Val Perplexity: 343.24\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/300: 100%|██████████| 573/573 [00:22<00:00, 25.99it/s, loss=4.6021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171:\n",
      "  Train Loss: 4.6021\n",
      "  Val Loss: 5.8431\n",
      "  Val Perplexity: 344.83\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/300: 100%|██████████| 573/573 [00:22<00:00, 25.96it/s, loss=4.5984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172:\n",
      "  Train Loss: 4.5984\n",
      "  Val Loss: 5.8439\n",
      "  Val Perplexity: 345.13\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/300: 100%|██████████| 573/573 [00:22<00:00, 25.91it/s, loss=4.5964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173:\n",
      "  Train Loss: 4.5964\n",
      "  Val Loss: 5.8477\n",
      "  Val Perplexity: 346.42\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/300: 100%|██████████| 573/573 [00:21<00:00, 26.14it/s, loss=4.5928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174:\n",
      "  Train Loss: 4.5928\n",
      "  Val Loss: 5.8537\n",
      "  Val Perplexity: 348.53\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175:\n",
      "  Train Loss: 4.5904\n",
      "  Val Loss: 5.8522\n",
      "  Val Perplexity: 348.01\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/300: 100%|██████████| 573/573 [00:22<00:00, 25.85it/s, loss=4.5878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176:\n",
      "  Train Loss: 4.5878\n",
      "  Val Loss: 5.8608\n",
      "  Val Perplexity: 350.99\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/300: 100%|██████████| 573/573 [00:21<00:00, 26.17it/s, loss=4.5843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177:\n",
      "  Train Loss: 4.5843\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.57\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/300: 100%|██████████| 573/573 [00:22<00:00, 26.00it/s, loss=4.5822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178:\n",
      "  Train Loss: 4.5822\n",
      "  Val Loss: 5.8606\n",
      "  Val Perplexity: 350.92\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/300: 100%|██████████| 573/573 [00:22<00:00, 25.98it/s, loss=4.5791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179:\n",
      "  Train Loss: 4.5791\n",
      "  Val Loss: 5.8596\n",
      "  Val Perplexity: 350.59\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/300: 100%|██████████| 573/573 [00:21<00:00, 26.11it/s, loss=4.5771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180:\n",
      "  Train Loss: 4.5771\n",
      "  Val Loss: 5.8684\n",
      "  Val Perplexity: 353.69\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/300: 100%|██████████| 573/573 [00:21<00:00, 26.25it/s, loss=4.5752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181:\n",
      "  Train Loss: 4.5752\n",
      "  Val Loss: 5.8683\n",
      "  Val Perplexity: 353.64\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/300: 100%|██████████| 573/573 [00:21<00:00, 26.13it/s, loss=4.5727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182:\n",
      "  Train Loss: 4.5727\n",
      "  Val Loss: 5.8679\n",
      "  Val Perplexity: 353.51\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/300: 100%|██████████| 573/573 [00:21<00:00, 26.21it/s, loss=4.5705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183:\n",
      "  Train Loss: 4.5705\n",
      "  Val Loss: 5.8615\n",
      "  Val Perplexity: 351.25\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/300: 100%|██████████| 573/573 [00:22<00:00, 26.02it/s, loss=4.5688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184:\n",
      "  Train Loss: 4.5688\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.36\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185:\n",
      "  Train Loss: 4.5664\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.37\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/300: 100%|██████████| 573/573 [00:21<00:00, 26.10it/s, loss=4.5638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186:\n",
      "  Train Loss: 4.5638\n",
      "  Val Loss: 5.8703\n",
      "  Val Perplexity: 354.34\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/300: 100%|██████████| 573/573 [00:21<00:00, 26.18it/s, loss=4.5614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187:\n",
      "  Train Loss: 4.5614\n",
      "  Val Loss: 5.8798\n",
      "  Val Perplexity: 357.75\n",
      "  📊 Best model still at Epoch 125 (Val Loss: 5.7782)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/300:  27%|██▋       | 156/573 [00:05<00:15, 26.18it/s, loss=4.5186]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRatio (ours/optimal): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_params/optimal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Train for 300 epochs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m best_val_loss, best_epoch = \u001b[43mtrain_model_properly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmha_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMHA TRAINING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain_model_properly\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Backward with gradient clipping\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\u001b[39;00m\n\u001b[32m     93\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# Prevents exploding gradients\u001b[39;00m\n\u001b[32m     96\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Better Training with RIGHT MODEL SIZE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================\n",
    "# CLEAR GPU AND START FRESH\n",
    "# ============================================\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# BETTER HYPERPARAMETERS (Less Overfitting)\n",
    "# ============================================\n",
    "\n",
    "# Create datasets with LONGER sequences\n",
    "train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "# ============================================\n",
    "# BETTER TRAINING FUNCTION (WITH BEST MODEL SAVING!)\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ============================================\n",
    "        # TRAINING PHASE\n",
    "        # ============================================\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            # NOTE: Gradient clipping helps prevent exploding gradients, especially in deep networks if not done we can have unstable training\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # ============================================\n",
    "        # VALIDATION PHASE\n",
    "        # ============================================\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # SAVE BEST MODEL (THE KEY FIX!)\n",
    "        # ============================================\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "            \n",
    "            print(f\"  ✅ Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            print(f\"  📊 Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return best_val_loss, best_epoch\n",
    "\n",
    "# ============================================\n",
    "# TRAIN MHA MODEL WITH OPTIMAL SIZE (1.8M params)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MHA MODEL - CHINCHILLA OPTIMIZED SIZE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# mha_model = LanguageModel(\n",
    "#     vocab_size=50257,\n",
    "#     max_seq_len=512,\n",
    "#     d_model=192,        # Reduced from 512 → 192\n",
    "#     num_heads=4,        # Reduced from 8 → 4\n",
    "#     d_ff=768,           # Reduced from 2048 → 768\n",
    "#     num_layers=4,       # Reduced from 6 → 4\n",
    "#     dropout=0.2         # Kept at 0.2\n",
    "# ).to(device)\n",
    "mha_model = LanguageModel(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=512,\n",
    "    d_model=64,        # SMALLER!\n",
    "    num_heads=4,\n",
    "    d_ff=256,          # SMALLER!\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "optimizer = optim.AdamW(mha_model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_params = sum(p.numel() for p in mha_model.parameters())\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Model Size: {num_params/1e6:.2f}M (vs 70.7M before)\")\n",
    "print(f\"Size Reduction: {70.7/(num_params/1e6):.1f}x smaller\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\\n\")\n",
    "\n",
    "# Chinchilla check\n",
    "tokens = 2_347_038\n",
    "optimal_params = tokens / 20\n",
    "print(f\"Dataset tokens: {tokens:,}\")\n",
    "print(f\"Chinchilla optimal params: {optimal_params:,.0f}\")\n",
    "print(f\"Our model params: {num_params:,}\")\n",
    "print(f\"Ratio (ours/optimal): {num_params/optimal_params:.1f}x\")\n",
    "\n",
    "\n",
    "# Train for 300 epochs\n",
    "best_val_loss, best_epoch = train_model_properly(\n",
    "    mha_model, train_loader, val_loader, criterion, optimizer, device, num_epochs=300\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA TRAINING COMPLETE!\")\n",
    "print(f\"Best model saved at Epoch {best_epoch}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04cfc5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING MHA MODEL GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of\n",
      "Output: The history of The New Age , the New York and W.com ranked in Ireland , with the 2012 . The United Nations and one @-@ year , in Canada ranked Billboard Hot 100 in the third consecutive weeks\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 000 and federal federal federal federal federal government , the Government departments , and the government .= = State = Post = = = = =Although the government Government Act of of the executive Department is the\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the most significant in the same time of the name of a small species , including the name , from the age of 1611th century . Another new genera was the Middle Middle Trinitys in the English\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In mathematics,\",\n",
    "    \"The cat sat on the\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING MHA MODEL GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text_proper(mha_model, tokenizer, prompt, max_length=40, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5bd7",
   "metadata": {},
   "source": [
    "# Full Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2301e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets transformers hf_transfer matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU Memory: 0.86 GB\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Creating datasets...\n",
      "Tokenizing dataset...\n",
      "Total tokens: 2,347,038\n",
      "Tokenizing dataset...\n",
      "Total tokens: 242,643\n",
      "Train batches: 573\n",
      "Val batches: 60\n",
      "\n",
      "Creating model...\n",
      "Parameters: 869,153\n",
      "GPU Memory: 0.86 GB\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 573/573 [00:18<00:00, 31.40it/s, loss=13.6341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "  Train Loss: 13.6341\n",
      "  Val Loss: 10.1061\n",
      "  Val Perplexity: 24491.82\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████| 573/573 [00:18<00:00, 31.46it/s, loss=9.5446] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "  Train Loss: 9.5446\n",
      "  Val Loss: 8.6099\n",
      "  Val Perplexity: 5485.81\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=8.5138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "  Train Loss: 8.5138\n",
      "  Val Loss: 7.9880\n",
      "  Val Perplexity: 2945.36\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=8.0134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "  Train Loss: 8.0134\n",
      "  Val Loss: 7.7309\n",
      "  Val Perplexity: 2277.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████| 573/573 [00:18<00:00, 31.73it/s, loss=7.7935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "  Train Loss: 7.7935\n",
      "  Val Loss: 7.6305\n",
      "  Val Perplexity: 2060.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████| 573/573 [00:17<00:00, 31.89it/s, loss=7.6808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "  Train Loss: 7.6808\n",
      "  Val Loss: 7.5810\n",
      "  Val Perplexity: 1960.64\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████| 573/573 [00:18<00:00, 31.36it/s, loss=7.6122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "  Train Loss: 7.6122\n",
      "  Val Loss: 7.5494\n",
      "  Val Perplexity: 1899.53\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████| 573/573 [00:18<00:00, 31.37it/s, loss=7.5633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "  Train Loss: 7.5633\n",
      "  Val Loss: 7.5240\n",
      "  Val Perplexity: 1851.97\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=7.5231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "  Train Loss: 7.5231\n",
      "  Val Loss: 7.4997\n",
      "  Val Perplexity: 1807.45\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=7.4867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "  Train Loss: 7.4867\n",
      "  Val Loss: 7.4755\n",
      "  Val Perplexity: 1764.24\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 573/573 [00:18<00:00, 31.50it/s, loss=7.4549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11:\n",
      "  Train Loss: 7.4549\n",
      "  Val Loss: 7.4510\n",
      "  Val Perplexity: 1721.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|██████████| 573/573 [00:18<00:00, 31.31it/s, loss=7.4236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:\n",
      "  Train Loss: 7.4236\n",
      "  Val Loss: 7.4196\n",
      "  Val Perplexity: 1668.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=7.3861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:\n",
      "  Train Loss: 7.3861\n",
      "  Val Loss: 7.3770\n",
      "  Val Perplexity: 1598.78\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=7.3479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:\n",
      "  Train Loss: 7.3479\n",
      "  Val Loss: 7.3429\n",
      "  Val Perplexity: 1545.15\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=7.3144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "  Train Loss: 7.3144\n",
      "  Val Loss: 7.3138\n",
      "  Val Perplexity: 1500.94\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/300: 100%|██████████| 573/573 [00:18<00:00, 31.31it/s, loss=7.2865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:\n",
      "  Train Loss: 7.2865\n",
      "  Val Loss: 7.2916\n",
      "  Val Perplexity: 1467.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/300: 100%|██████████| 573/573 [00:18<00:00, 31.06it/s, loss=7.2625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:\n",
      "  Train Loss: 7.2625\n",
      "  Val Loss: 7.2720\n",
      "  Val Perplexity: 1439.43\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=7.2410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:\n",
      "  Train Loss: 7.2410\n",
      "  Val Loss: 7.2526\n",
      "  Val Perplexity: 1411.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/300: 100%|██████████| 573/573 [00:18<00:00, 31.27it/s, loss=7.2202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:\n",
      "  Train Loss: 7.2202\n",
      "  Val Loss: 7.2331\n",
      "  Val Perplexity: 1384.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=7.2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Train Loss: 7.2000\n",
      "  Val Loss: 7.2162\n",
      "  Val Perplexity: 1361.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/300: 100%|██████████| 573/573 [00:18<00:00, 31.58it/s, loss=7.1822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:\n",
      "  Train Loss: 7.1822\n",
      "  Val Loss: 7.2027\n",
      "  Val Perplexity: 1343.11\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/300: 100%|██████████| 573/573 [00:18<00:00, 31.58it/s, loss=7.1659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:\n",
      "  Train Loss: 7.1659\n",
      "  Val Loss: 7.1889\n",
      "  Val Perplexity: 1324.71\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/300: 100%|██████████| 573/573 [00:18<00:00, 31.62it/s, loss=7.1509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23:\n",
      "  Train Loss: 7.1509\n",
      "  Val Loss: 7.1767\n",
      "  Val Perplexity: 1308.63\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/300: 100%|██████████| 573/573 [00:18<00:00, 31.15it/s, loss=7.1368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:\n",
      "  Train Loss: 7.1368\n",
      "  Val Loss: 7.1655\n",
      "  Val Perplexity: 1294.07\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/300: 100%|██████████| 573/573 [00:18<00:00, 31.55it/s, loss=7.1238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:\n",
      "  Train Loss: 7.1238\n",
      "  Val Loss: 7.1542\n",
      "  Val Perplexity: 1279.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=7.1106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:\n",
      "  Train Loss: 7.1106\n",
      "  Val Loss: 7.1431\n",
      "  Val Perplexity: 1265.38\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=7.0983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:\n",
      "  Train Loss: 7.0983\n",
      "  Val Loss: 7.1321\n",
      "  Val Perplexity: 1251.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/300: 100%|██████████| 573/573 [00:18<00:00, 31.35it/s, loss=7.0864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:\n",
      "  Train Loss: 7.0864\n",
      "  Val Loss: 7.1219\n",
      "  Val Perplexity: 1238.79\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/300: 100%|██████████| 573/573 [00:18<00:00, 31.15it/s, loss=7.0744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:\n",
      "  Train Loss: 7.0744\n",
      "  Val Loss: 7.1108\n",
      "  Val Perplexity: 1225.15\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=7.0628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:\n",
      "  Train Loss: 7.0628\n",
      "  Val Loss: 7.1007\n",
      "  Val Perplexity: 1212.78\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/300: 100%|██████████| 573/573 [00:18<00:00, 31.62it/s, loss=7.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:\n",
      "  Train Loss: 7.0516\n",
      "  Val Loss: 7.0900\n",
      "  Val Perplexity: 1199.91\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=7.0397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:\n",
      "  Train Loss: 7.0397\n",
      "  Val Loss: 7.0782\n",
      "  Val Perplexity: 1185.78\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/300: 100%|██████████| 573/573 [00:18<00:00, 31.33it/s, loss=7.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:\n",
      "  Train Loss: 7.0282\n",
      "  Val Loss: 7.0681\n",
      "  Val Perplexity: 1173.96\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/300: 100%|██████████| 573/573 [00:17<00:00, 31.92it/s, loss=7.0166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:\n",
      "  Train Loss: 7.0166\n",
      "  Val Loss: 7.0566\n",
      "  Val Perplexity: 1160.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/300: 100%|██████████| 573/573 [00:18<00:00, 31.55it/s, loss=7.0049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:\n",
      "  Train Loss: 7.0049\n",
      "  Val Loss: 7.0452\n",
      "  Val Perplexity: 1147.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/300: 100%|██████████| 573/573 [00:18<00:00, 31.59it/s, loss=6.9924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:\n",
      "  Train Loss: 6.9924\n",
      "  Val Loss: 7.0327\n",
      "  Val Perplexity: 1133.10\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=6.9803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:\n",
      "  Train Loss: 6.9803\n",
      "  Val Loss: 7.0207\n",
      "  Val Perplexity: 1119.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/300: 100%|██████████| 573/573 [00:18<00:00, 31.73it/s, loss=6.9674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:\n",
      "  Train Loss: 6.9674\n",
      "  Val Loss: 7.0092\n",
      "  Val Perplexity: 1106.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/300: 100%|██████████| 573/573 [00:18<00:00, 31.19it/s, loss=6.9544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:\n",
      "  Train Loss: 6.9544\n",
      "  Val Loss: 6.9957\n",
      "  Val Perplexity: 1091.90\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/300: 100%|██████████| 573/573 [00:18<00:00, 31.45it/s, loss=6.9412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:\n",
      "  Train Loss: 6.9412\n",
      "  Val Loss: 6.9814\n",
      "  Val Perplexity: 1076.46\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/300: 100%|██████████| 573/573 [00:18<00:00, 31.57it/s, loss=6.9273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:\n",
      "  Train Loss: 6.9273\n",
      "  Val Loss: 6.9681\n",
      "  Val Perplexity: 1062.22\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/300: 100%|██████████| 573/573 [00:18<00:00, 31.52it/s, loss=6.9127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:\n",
      "  Train Loss: 6.9127\n",
      "  Val Loss: 6.9535\n",
      "  Val Perplexity: 1046.79\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=6.8973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:\n",
      "  Train Loss: 6.8973\n",
      "  Val Loss: 6.9394\n",
      "  Val Perplexity: 1032.14\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/300: 100%|██████████| 573/573 [00:18<00:00, 31.09it/s, loss=6.8814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:\n",
      "  Train Loss: 6.8814\n",
      "  Val Loss: 6.9226\n",
      "  Val Perplexity: 1014.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=6.8646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:\n",
      "  Train Loss: 6.8646\n",
      "  Val Loss: 6.9064\n",
      "  Val Perplexity: 998.61\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=6.8477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:\n",
      "  Train Loss: 6.8477\n",
      "  Val Loss: 6.8901\n",
      "  Val Perplexity: 982.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/300: 100%|██████████| 573/573 [00:18<00:00, 31.28it/s, loss=6.8293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:\n",
      "  Train Loss: 6.8293\n",
      "  Val Loss: 6.8723\n",
      "  Val Perplexity: 965.17\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/300: 100%|██████████| 573/573 [00:18<00:00, 31.32it/s, loss=6.8101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:\n",
      "  Train Loss: 6.8101\n",
      "  Val Loss: 6.8537\n",
      "  Val Perplexity: 947.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=6.7899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49:\n",
      "  Train Loss: 6.7899\n",
      "  Val Loss: 6.8357\n",
      "  Val Perplexity: 930.46\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/300: 100%|██████████| 573/573 [00:18<00:00, 31.83it/s, loss=6.7697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50:\n",
      "  Train Loss: 6.7697\n",
      "  Val Loss: 6.8156\n",
      "  Val Perplexity: 911.97\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/300: 100%|██████████| 573/573 [00:18<00:00, 31.42it/s, loss=6.7487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51:\n",
      "  Train Loss: 6.7487\n",
      "  Val Loss: 6.7962\n",
      "  Val Perplexity: 894.48\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/300: 100%|██████████| 573/573 [00:18<00:00, 31.10it/s, loss=6.7272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52:\n",
      "  Train Loss: 6.7272\n",
      "  Val Loss: 6.7774\n",
      "  Val Perplexity: 877.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/300: 100%|██████████| 573/573 [00:18<00:00, 31.54it/s, loss=6.7058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53:\n",
      "  Train Loss: 6.7058\n",
      "  Val Loss: 6.7585\n",
      "  Val Perplexity: 861.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=6.6841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54:\n",
      "  Train Loss: 6.6841\n",
      "  Val Loss: 6.7397\n",
      "  Val Perplexity: 845.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/300: 100%|██████████| 573/573 [00:18<00:00, 31.56it/s, loss=6.6623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55:\n",
      "  Train Loss: 6.6623\n",
      "  Val Loss: 6.7217\n",
      "  Val Perplexity: 830.24\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=6.6411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56:\n",
      "  Train Loss: 6.6411\n",
      "  Val Loss: 6.7035\n",
      "  Val Perplexity: 815.28\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/300: 100%|██████████| 573/573 [00:18<00:00, 31.33it/s, loss=6.6202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57:\n",
      "  Train Loss: 6.6202\n",
      "  Val Loss: 6.6856\n",
      "  Val Perplexity: 800.81\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=6.6003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58:\n",
      "  Train Loss: 6.6003\n",
      "  Val Loss: 6.6688\n",
      "  Val Perplexity: 787.45\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=6.5804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59:\n",
      "  Train Loss: 6.5804\n",
      "  Val Loss: 6.6523\n",
      "  Val Perplexity: 774.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=6.5604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60:\n",
      "  Train Loss: 6.5604\n",
      "  Val Loss: 6.6385\n",
      "  Val Perplexity: 763.98\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=6.5416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61:\n",
      "  Train Loss: 6.5416\n",
      "  Val Loss: 6.6223\n",
      "  Val Perplexity: 751.67\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/300: 100%|██████████| 573/573 [00:18<00:00, 31.26it/s, loss=6.5223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62:\n",
      "  Train Loss: 6.5223\n",
      "  Val Loss: 6.6087\n",
      "  Val Perplexity: 741.53\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=6.5040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63:\n",
      "  Train Loss: 6.5040\n",
      "  Val Loss: 6.5945\n",
      "  Val Perplexity: 731.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=6.4863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64:\n",
      "  Train Loss: 6.4863\n",
      "  Val Loss: 6.5799\n",
      "  Val Perplexity: 720.44\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=6.4692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65:\n",
      "  Train Loss: 6.4692\n",
      "  Val Loss: 6.5686\n",
      "  Val Perplexity: 712.38\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=6.4515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66:\n",
      "  Train Loss: 6.4515\n",
      "  Val Loss: 6.5544\n",
      "  Val Perplexity: 702.33\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/300: 100%|██████████| 573/573 [00:18<00:00, 31.75it/s, loss=6.4351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67:\n",
      "  Train Loss: 6.4351\n",
      "  Val Loss: 6.5424\n",
      "  Val Perplexity: 693.98\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=6.4185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68:\n",
      "  Train Loss: 6.4185\n",
      "  Val Loss: 6.5329\n",
      "  Val Perplexity: 687.40\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/300: 100%|██████████| 573/573 [00:18<00:00, 31.79it/s, loss=6.4026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69:\n",
      "  Train Loss: 6.4026\n",
      "  Val Loss: 6.5207\n",
      "  Val Perplexity: 679.03\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=6.3861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70:\n",
      "  Train Loss: 6.3861\n",
      "  Val Loss: 6.5090\n",
      "  Val Perplexity: 671.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=6.3703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71:\n",
      "  Train Loss: 6.3703\n",
      "  Val Loss: 6.4975\n",
      "  Val Perplexity: 663.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=6.3546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72:\n",
      "  Train Loss: 6.3546\n",
      "  Val Loss: 6.4870\n",
      "  Val Perplexity: 656.53\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=6.3392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73:\n",
      "  Train Loss: 6.3392\n",
      "  Val Loss: 6.4760\n",
      "  Val Perplexity: 649.39\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/300: 100%|██████████| 573/573 [00:18<00:00, 31.62it/s, loss=6.3231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74:\n",
      "  Train Loss: 6.3231\n",
      "  Val Loss: 6.4655\n",
      "  Val Perplexity: 642.56\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=6.3088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75:\n",
      "  Train Loss: 6.3088\n",
      "  Val Loss: 6.4578\n",
      "  Val Perplexity: 637.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=6.2942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76:\n",
      "  Train Loss: 6.2942\n",
      "  Val Loss: 6.4469\n",
      "  Val Perplexity: 630.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=6.2786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77:\n",
      "  Train Loss: 6.2786\n",
      "  Val Loss: 6.4385\n",
      "  Val Perplexity: 625.45\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=6.2643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78:\n",
      "  Train Loss: 6.2643\n",
      "  Val Loss: 6.4261\n",
      "  Val Perplexity: 617.75\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/300: 100%|██████████| 573/573 [00:18<00:00, 31.58it/s, loss=6.2503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79:\n",
      "  Train Loss: 6.2503\n",
      "  Val Loss: 6.4187\n",
      "  Val Perplexity: 613.19\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/300: 100%|██████████| 573/573 [00:18<00:00, 31.60it/s, loss=6.2363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80:\n",
      "  Train Loss: 6.2363\n",
      "  Val Loss: 6.4105\n",
      "  Val Perplexity: 608.17\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/300: 100%|██████████| 573/573 [00:17<00:00, 31.91it/s, loss=6.2232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81:\n",
      "  Train Loss: 6.2232\n",
      "  Val Loss: 6.4021\n",
      "  Val Perplexity: 603.14\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/300: 100%|██████████| 573/573 [00:18<00:00, 31.75it/s, loss=6.2091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82:\n",
      "  Train Loss: 6.2091\n",
      "  Val Loss: 6.3937\n",
      "  Val Perplexity: 598.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=6.1960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83:\n",
      "  Train Loss: 6.1960\n",
      "  Val Loss: 6.3837\n",
      "  Val Perplexity: 592.14\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=6.1834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84:\n",
      "  Train Loss: 6.1834\n",
      "  Val Loss: 6.3763\n",
      "  Val Perplexity: 587.76\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/300: 100%|██████████| 573/573 [00:18<00:00, 31.54it/s, loss=6.1703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85:\n",
      "  Train Loss: 6.1703\n",
      "  Val Loss: 6.3690\n",
      "  Val Perplexity: 583.45\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=6.1577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86:\n",
      "  Train Loss: 6.1577\n",
      "  Val Loss: 6.3606\n",
      "  Val Perplexity: 578.62\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/300: 100%|██████████| 573/573 [00:18<00:00, 31.59it/s, loss=6.1454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87:\n",
      "  Train Loss: 6.1454\n",
      "  Val Loss: 6.3517\n",
      "  Val Perplexity: 573.45\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/300: 100%|██████████| 573/573 [00:17<00:00, 31.94it/s, loss=6.1337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88:\n",
      "  Train Loss: 6.1337\n",
      "  Val Loss: 6.3487\n",
      "  Val Perplexity: 571.78\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/300: 100%|██████████| 573/573 [00:17<00:00, 31.96it/s, loss=6.1220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89:\n",
      "  Train Loss: 6.1220\n",
      "  Val Loss: 6.3402\n",
      "  Val Perplexity: 566.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=6.1108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90:\n",
      "  Train Loss: 6.1108\n",
      "  Val Loss: 6.3331\n",
      "  Val Perplexity: 562.93\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=6.0993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91:\n",
      "  Train Loss: 6.0993\n",
      "  Val Loss: 6.3286\n",
      "  Val Perplexity: 560.37\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=6.0892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92:\n",
      "  Train Loss: 6.0892\n",
      "  Val Loss: 6.3217\n",
      "  Val Perplexity: 556.51\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/300: 100%|██████████| 573/573 [00:18<00:00, 31.80it/s, loss=6.0789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93:\n",
      "  Train Loss: 6.0789\n",
      "  Val Loss: 6.3177\n",
      "  Val Perplexity: 554.31\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=6.0682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94:\n",
      "  Train Loss: 6.0682\n",
      "  Val Loss: 6.3076\n",
      "  Val Perplexity: 548.74\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/300: 100%|██████████| 573/573 [00:18<00:00, 31.73it/s, loss=6.0585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95:\n",
      "  Train Loss: 6.0585\n",
      "  Val Loss: 6.3042\n",
      "  Val Perplexity: 546.86\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=6.0489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96:\n",
      "  Train Loss: 6.0489\n",
      "  Val Loss: 6.2984\n",
      "  Val Perplexity: 543.70\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=6.0391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97:\n",
      "  Train Loss: 6.0391\n",
      "  Val Loss: 6.2955\n",
      "  Val Perplexity: 542.11\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=6.0302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98:\n",
      "  Train Loss: 6.0302\n",
      "  Val Loss: 6.2880\n",
      "  Val Perplexity: 538.08\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=6.0213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99:\n",
      "  Train Loss: 6.0213\n",
      "  Val Loss: 6.2853\n",
      "  Val Perplexity: 536.60\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/300: 100%|██████████| 573/573 [00:18<00:00, 31.73it/s, loss=6.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100:\n",
      "  Train Loss: 6.0124\n",
      "  Val Loss: 6.2801\n",
      "  Val Perplexity: 533.85\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=6.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101:\n",
      "  Train Loss: 6.0039\n",
      "  Val Loss: 6.2749\n",
      "  Val Perplexity: 531.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/300: 100%|██████████| 573/573 [00:18<00:00, 31.57it/s, loss=5.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102:\n",
      "  Train Loss: 5.9960\n",
      "  Val Loss: 6.2706\n",
      "  Val Perplexity: 528.79\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=5.9875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103:\n",
      "  Train Loss: 5.9875\n",
      "  Val Loss: 6.2662\n",
      "  Val Perplexity: 526.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/300: 100%|██████████| 573/573 [00:17<00:00, 31.85it/s, loss=5.9801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 104:\n",
      "  Train Loss: 5.9801\n",
      "  Val Loss: 6.2620\n",
      "  Val Perplexity: 524.29\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/300: 100%|██████████| 573/573 [00:17<00:00, 31.85it/s, loss=5.9726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105:\n",
      "  Train Loss: 5.9726\n",
      "  Val Loss: 6.2552\n",
      "  Val Perplexity: 520.70\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/300: 100%|██████████| 573/573 [00:18<00:00, 31.65it/s, loss=5.9650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106:\n",
      "  Train Loss: 5.9650\n",
      "  Val Loss: 6.2555\n",
      "  Val Perplexity: 520.87\n",
      "  📊 Best model still at Epoch 105 (Val Loss: 6.2552)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=5.9576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107:\n",
      "  Train Loss: 5.9576\n",
      "  Val Loss: 6.2519\n",
      "  Val Perplexity: 519.01\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.9509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108:\n",
      "  Train Loss: 5.9509\n",
      "  Val Loss: 6.2451\n",
      "  Val Perplexity: 515.47\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=5.9433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109:\n",
      "  Train Loss: 5.9433\n",
      "  Val Loss: 6.2424\n",
      "  Val Perplexity: 514.07\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=5.9372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 110:\n",
      "  Train Loss: 5.9372\n",
      "  Val Loss: 6.2388\n",
      "  Val Perplexity: 512.26\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=5.9306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111:\n",
      "  Train Loss: 5.9306\n",
      "  Val Loss: 6.2361\n",
      "  Val Perplexity: 510.85\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=5.9247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 112:\n",
      "  Train Loss: 5.9247\n",
      "  Val Loss: 6.2317\n",
      "  Val Perplexity: 508.64\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=5.9184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113:\n",
      "  Train Loss: 5.9184\n",
      "  Val Loss: 6.2288\n",
      "  Val Perplexity: 507.13\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.9122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 114:\n",
      "  Train Loss: 5.9122\n",
      "  Val Loss: 6.2272\n",
      "  Val Perplexity: 506.34\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=5.9062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 115:\n",
      "  Train Loss: 5.9062\n",
      "  Val Loss: 6.2257\n",
      "  Val Perplexity: 505.56\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=5.9010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116:\n",
      "  Train Loss: 5.9010\n",
      "  Val Loss: 6.2227\n",
      "  Val Perplexity: 504.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=5.8954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117:\n",
      "  Train Loss: 5.8954\n",
      "  Val Loss: 6.2189\n",
      "  Val Perplexity: 502.14\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.8901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 118:\n",
      "  Train Loss: 5.8901\n",
      "  Val Loss: 6.2144\n",
      "  Val Perplexity: 499.92\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=5.8847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 119:\n",
      "  Train Loss: 5.8847\n",
      "  Val Loss: 6.2103\n",
      "  Val Perplexity: 497.84\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.8796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 120:\n",
      "  Train Loss: 5.8796\n",
      "  Val Loss: 6.2069\n",
      "  Val Perplexity: 496.14\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=5.8748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121:\n",
      "  Train Loss: 5.8748\n",
      "  Val Loss: 6.2038\n",
      "  Val Perplexity: 494.63\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/300: 100%|██████████| 573/573 [00:18<00:00, 31.58it/s, loss=5.8700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 122:\n",
      "  Train Loss: 5.8700\n",
      "  Val Loss: 6.2039\n",
      "  Val Perplexity: 494.69\n",
      "  📊 Best model still at Epoch 121 (Val Loss: 6.2038)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=5.8654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 123:\n",
      "  Train Loss: 5.8654\n",
      "  Val Loss: 6.2031\n",
      "  Val Perplexity: 494.26\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/300: 100%|██████████| 573/573 [00:18<00:00, 31.60it/s, loss=5.8612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 124:\n",
      "  Train Loss: 5.8612\n",
      "  Val Loss: 6.2016\n",
      "  Val Perplexity: 493.52\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.8558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 125:\n",
      "  Train Loss: 5.8558\n",
      "  Val Loss: 6.1969\n",
      "  Val Perplexity: 491.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/300: 100%|██████████| 573/573 [00:17<00:00, 31.90it/s, loss=5.8522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 126:\n",
      "  Train Loss: 5.8522\n",
      "  Val Loss: 6.1955\n",
      "  Val Perplexity: 490.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/300: 100%|██████████| 573/573 [00:17<00:00, 31.93it/s, loss=5.8477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 127:\n",
      "  Train Loss: 5.8477\n",
      "  Val Loss: 6.1941\n",
      "  Val Perplexity: 489.87\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=5.8440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 128:\n",
      "  Train Loss: 5.8440\n",
      "  Val Loss: 6.1912\n",
      "  Val Perplexity: 488.41\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/300: 100%|██████████| 573/573 [00:18<00:00, 31.66it/s, loss=5.8401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 129:\n",
      "  Train Loss: 5.8401\n",
      "  Val Loss: 6.1881\n",
      "  Val Perplexity: 486.90\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/300: 100%|██████████| 573/573 [00:17<00:00, 31.90it/s, loss=5.8359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130:\n",
      "  Train Loss: 5.8359\n",
      "  Val Loss: 6.1896\n",
      "  Val Perplexity: 487.66\n",
      "  📊 Best model still at Epoch 129 (Val Loss: 6.1881)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/300: 100%|██████████| 573/573 [00:18<00:00, 31.80it/s, loss=5.8324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131:\n",
      "  Train Loss: 5.8324\n",
      "  Val Loss: 6.1844\n",
      "  Val Perplexity: 485.12\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=5.8295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132:\n",
      "  Train Loss: 5.8295\n",
      "  Val Loss: 6.1817\n",
      "  Val Perplexity: 483.81\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=5.8258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133:\n",
      "  Train Loss: 5.8258\n",
      "  Val Loss: 6.1814\n",
      "  Val Perplexity: 483.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=5.8228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134:\n",
      "  Train Loss: 5.8228\n",
      "  Val Loss: 6.1791\n",
      "  Val Perplexity: 482.55\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/300: 100%|██████████| 573/573 [00:17<00:00, 31.83it/s, loss=5.8198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 135:\n",
      "  Train Loss: 5.8198\n",
      "  Val Loss: 6.1783\n",
      "  Val Perplexity: 482.17\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=5.8161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 136:\n",
      "  Train Loss: 5.8161\n",
      "  Val Loss: 6.1770\n",
      "  Val Perplexity: 481.53\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=5.8138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 137:\n",
      "  Train Loss: 5.8138\n",
      "  Val Loss: 6.1759\n",
      "  Val Perplexity: 481.00\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.8104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 138:\n",
      "  Train Loss: 5.8104\n",
      "  Val Loss: 6.1732\n",
      "  Val Perplexity: 479.72\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=5.8082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 139:\n",
      "  Train Loss: 5.8082\n",
      "  Val Loss: 6.1721\n",
      "  Val Perplexity: 479.20\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/300: 100%|██████████| 573/573 [00:17<00:00, 31.90it/s, loss=5.8055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 140:\n",
      "  Train Loss: 5.8055\n",
      "  Val Loss: 6.1703\n",
      "  Val Perplexity: 478.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=5.8030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 141:\n",
      "  Train Loss: 5.8030\n",
      "  Val Loss: 6.1684\n",
      "  Val Perplexity: 477.42\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=5.8012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 142:\n",
      "  Train Loss: 5.8012\n",
      "  Val Loss: 6.1658\n",
      "  Val Perplexity: 476.20\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/300: 100%|██████████| 573/573 [00:17<00:00, 31.97it/s, loss=5.7985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 143:\n",
      "  Train Loss: 5.7985\n",
      "  Val Loss: 6.1663\n",
      "  Val Perplexity: 476.44\n",
      "  📊 Best model still at Epoch 142 (Val Loss: 6.1658)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/300: 100%|██████████| 573/573 [00:18<00:00, 31.80it/s, loss=5.7960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 144:\n",
      "  Train Loss: 5.7960\n",
      "  Val Loss: 6.1638\n",
      "  Val Perplexity: 475.23\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/300: 100%|██████████| 573/573 [00:18<00:00, 31.57it/s, loss=5.7937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145:\n",
      "  Train Loss: 5.7937\n",
      "  Val Loss: 6.1650\n",
      "  Val Perplexity: 475.80\n",
      "  📊 Best model still at Epoch 144 (Val Loss: 6.1638)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=5.7918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146:\n",
      "  Train Loss: 5.7918\n",
      "  Val Loss: 6.1615\n",
      "  Val Perplexity: 474.12\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=5.7901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147:\n",
      "  Train Loss: 5.7901\n",
      "  Val Loss: 6.1642\n",
      "  Val Perplexity: 475.42\n",
      "  📊 Best model still at Epoch 146 (Val Loss: 6.1615)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/300: 100%|██████████| 573/573 [00:18<00:00, 31.79it/s, loss=5.7874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148:\n",
      "  Train Loss: 5.7874\n",
      "  Val Loss: 6.1587\n",
      "  Val Perplexity: 472.79\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/300: 100%|██████████| 573/573 [00:18<00:00, 31.55it/s, loss=5.7863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 149:\n",
      "  Train Loss: 5.7863\n",
      "  Val Loss: 6.1571\n",
      "  Val Perplexity: 472.04\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/300: 100%|██████████| 573/573 [00:18<00:00, 31.79it/s, loss=5.7848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 150:\n",
      "  Train Loss: 5.7848\n",
      "  Val Loss: 6.1557\n",
      "  Val Perplexity: 471.40\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/300: 100%|██████████| 573/573 [00:18<00:00, 31.45it/s, loss=5.7826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 151:\n",
      "  Train Loss: 5.7826\n",
      "  Val Loss: 6.1569\n",
      "  Val Perplexity: 471.96\n",
      "  📊 Best model still at Epoch 150 (Val Loss: 6.1557)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/300: 100%|██████████| 573/573 [00:18<00:00, 31.59it/s, loss=5.7809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152:\n",
      "  Train Loss: 5.7809\n",
      "  Val Loss: 6.1559\n",
      "  Val Perplexity: 471.50\n",
      "  📊 Best model still at Epoch 150 (Val Loss: 6.1557)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/300: 100%|██████████| 573/573 [00:18<00:00, 31.57it/s, loss=5.7793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153:\n",
      "  Train Loss: 5.7793\n",
      "  Val Loss: 6.1538\n",
      "  Val Perplexity: 470.49\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.7779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 154:\n",
      "  Train Loss: 5.7779\n",
      "  Val Loss: 6.1569\n",
      "  Val Perplexity: 471.98\n",
      "  📊 Best model still at Epoch 153 (Val Loss: 6.1538)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/300: 100%|██████████| 573/573 [00:17<00:00, 31.87it/s, loss=5.7767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 155:\n",
      "  Train Loss: 5.7767\n",
      "  Val Loss: 6.1528\n",
      "  Val Perplexity: 470.05\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156:\n",
      "  Train Loss: 5.7750\n",
      "  Val Loss: 6.1540\n",
      "  Val Perplexity: 470.59\n",
      "  📊 Best model still at Epoch 155 (Val Loss: 6.1528)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/300: 100%|██████████| 573/573 [00:18<00:00, 31.58it/s, loss=5.7738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 157:\n",
      "  Train Loss: 5.7738\n",
      "  Val Loss: 6.1512\n",
      "  Val Perplexity: 469.27\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/300: 100%|██████████| 573/573 [00:18<00:00, 31.61it/s, loss=5.7725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158:\n",
      "  Train Loss: 5.7725\n",
      "  Val Loss: 6.1524\n",
      "  Val Perplexity: 469.85\n",
      "  📊 Best model still at Epoch 157 (Val Loss: 6.1512)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/300: 100%|██████████| 573/573 [00:17<00:00, 31.83it/s, loss=5.7711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159:\n",
      "  Train Loss: 5.7711\n",
      "  Val Loss: 6.1502\n",
      "  Val Perplexity: 468.80\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/300: 100%|██████████| 573/573 [00:17<00:00, 31.89it/s, loss=5.7702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160:\n",
      "  Train Loss: 5.7702\n",
      "  Val Loss: 6.1503\n",
      "  Val Perplexity: 468.86\n",
      "  📊 Best model still at Epoch 159 (Val Loss: 6.1502)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/300: 100%|██████████| 573/573 [00:17<00:00, 31.97it/s, loss=5.7684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 161:\n",
      "  Train Loss: 5.7684\n",
      "  Val Loss: 6.1499\n",
      "  Val Perplexity: 468.66\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/300: 100%|██████████| 573/573 [00:17<00:00, 31.87it/s, loss=5.7674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162:\n",
      "  Train Loss: 5.7674\n",
      "  Val Loss: 6.1500\n",
      "  Val Perplexity: 468.72\n",
      "  📊 Best model still at Epoch 161 (Val Loss: 6.1499)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/300: 100%|██████████| 573/573 [00:17<00:00, 31.95it/s, loss=5.7667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163:\n",
      "  Train Loss: 5.7667\n",
      "  Val Loss: 6.1499\n",
      "  Val Perplexity: 468.65\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/300: 100%|██████████| 573/573 [00:17<00:00, 31.92it/s, loss=5.7650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 164:\n",
      "  Train Loss: 5.7650\n",
      "  Val Loss: 6.1468\n",
      "  Val Perplexity: 467.21\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/300: 100%|██████████| 573/573 [00:17<00:00, 31.92it/s, loss=5.7641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 165:\n",
      "  Train Loss: 5.7641\n",
      "  Val Loss: 6.1459\n",
      "  Val Perplexity: 466.82\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=5.7623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166:\n",
      "  Train Loss: 5.7623\n",
      "  Val Loss: 6.1461\n",
      "  Val Perplexity: 466.89\n",
      "  📊 Best model still at Epoch 165 (Val Loss: 6.1459)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/300: 100%|██████████| 573/573 [00:18<00:00, 31.83it/s, loss=5.7615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 167:\n",
      "  Train Loss: 5.7615\n",
      "  Val Loss: 6.1466\n",
      "  Val Perplexity: 467.13\n",
      "  📊 Best model still at Epoch 165 (Val Loss: 6.1459)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=5.7606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168:\n",
      "  Train Loss: 5.7606\n",
      "  Val Loss: 6.1470\n",
      "  Val Perplexity: 467.32\n",
      "  📊 Best model still at Epoch 165 (Val Loss: 6.1459)\n",
      "  ⏳ Epochs without improvement: 3/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=5.7603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 169:\n",
      "  Train Loss: 5.7603\n",
      "  Val Loss: 6.1475\n",
      "  Val Perplexity: 467.56\n",
      "  📊 Best model still at Epoch 165 (Val Loss: 6.1459)\n",
      "  ⏳ Epochs without improvement: 4/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/300: 100%|██████████| 573/573 [00:18<00:00, 31.82it/s, loss=5.7590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 170:\n",
      "  Train Loss: 5.7590\n",
      "  Val Loss: 6.1476\n",
      "  Val Perplexity: 467.60\n",
      "  📊 Best model still at Epoch 165 (Val Loss: 6.1459)\n",
      "  ⏳ Epochs without improvement: 5/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/300: 100%|██████████| 573/573 [00:18<00:00, 31.66it/s, loss=5.7577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171:\n",
      "  Train Loss: 5.7577\n",
      "  Val Loss: 6.1435\n",
      "  Val Perplexity: 465.68\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.7575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 172:\n",
      "  Train Loss: 5.7575\n",
      "  Val Loss: 6.1462\n",
      "  Val Perplexity: 466.93\n",
      "  📊 Best model still at Epoch 171 (Val Loss: 6.1435)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=5.7566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173:\n",
      "  Train Loss: 5.7566\n",
      "  Val Loss: 6.1462\n",
      "  Val Perplexity: 466.94\n",
      "  📊 Best model still at Epoch 171 (Val Loss: 6.1435)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/300: 100%|██████████| 573/573 [00:18<00:00, 31.83it/s, loss=5.7555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 174:\n",
      "  Train Loss: 5.7555\n",
      "  Val Loss: 6.1432\n",
      "  Val Perplexity: 465.54\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=5.7549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175:\n",
      "  Train Loss: 5.7549\n",
      "  Val Loss: 6.1454\n",
      "  Val Perplexity: 466.57\n",
      "  📊 Best model still at Epoch 174 (Val Loss: 6.1432)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=5.7541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 176:\n",
      "  Train Loss: 5.7541\n",
      "  Val Loss: 6.1431\n",
      "  Val Perplexity: 465.50\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/300: 100%|██████████| 573/573 [00:18<00:00, 31.65it/s, loss=5.7533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 177:\n",
      "  Train Loss: 5.7533\n",
      "  Val Loss: 6.1444\n",
      "  Val Perplexity: 466.09\n",
      "  📊 Best model still at Epoch 176 (Val Loss: 6.1431)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.7526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178:\n",
      "  Train Loss: 5.7526\n",
      "  Val Loss: 6.1449\n",
      "  Val Perplexity: 466.35\n",
      "  📊 Best model still at Epoch 176 (Val Loss: 6.1431)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179:\n",
      "  Train Loss: 5.7512\n",
      "  Val Loss: 6.1428\n",
      "  Val Perplexity: 465.35\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=5.7507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 180:\n",
      "  Train Loss: 5.7507\n",
      "  Val Loss: 6.1441\n",
      "  Val Perplexity: 465.98\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=5.7503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 181:\n",
      "  Train Loss: 5.7503\n",
      "  Val Loss: 6.1431\n",
      "  Val Perplexity: 465.50\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/300: 100%|██████████| 573/573 [00:17<00:00, 31.91it/s, loss=5.7489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182:\n",
      "  Train Loss: 5.7489\n",
      "  Val Loss: 6.1450\n",
      "  Val Perplexity: 466.39\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 3/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/300: 100%|██████████| 573/573 [00:17<00:00, 31.92it/s, loss=5.7485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 183:\n",
      "  Train Loss: 5.7485\n",
      "  Val Loss: 6.1435\n",
      "  Val Perplexity: 465.69\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 4/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=5.7479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184:\n",
      "  Train Loss: 5.7479\n",
      "  Val Loss: 6.1439\n",
      "  Val Perplexity: 465.88\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 5/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=5.7470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 185:\n",
      "  Train Loss: 5.7470\n",
      "  Val Loss: 6.1429\n",
      "  Val Perplexity: 465.42\n",
      "  📊 Best model still at Epoch 179 (Val Loss: 6.1428)\n",
      "  ⏳ Epochs without improvement: 6/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 186:\n",
      "  Train Loss: 5.7467\n",
      "  Val Loss: 6.1427\n",
      "  Val Perplexity: 465.32\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/300: 100%|██████████| 573/573 [00:18<00:00, 31.62it/s, loss=5.7463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 187:\n",
      "  Train Loss: 5.7463\n",
      "  Val Loss: 6.1422\n",
      "  Val Perplexity: 465.09\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/300: 100%|██████████| 573/573 [00:18<00:00, 31.73it/s, loss=5.7451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 188:\n",
      "  Train Loss: 5.7451\n",
      "  Val Loss: 6.1432\n",
      "  Val Perplexity: 465.53\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.7447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 189:\n",
      "  Train Loss: 5.7447\n",
      "  Val Loss: 6.1437\n",
      "  Val Perplexity: 465.78\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=5.7444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 190:\n",
      "  Train Loss: 5.7444\n",
      "  Val Loss: 6.1430\n",
      "  Val Perplexity: 465.43\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 3/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 191:\n",
      "  Train Loss: 5.7431\n",
      "  Val Loss: 6.1427\n",
      "  Val Perplexity: 465.30\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 4/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/300: 100%|██████████| 573/573 [00:17<00:00, 31.88it/s, loss=5.7433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 192:\n",
      "  Train Loss: 5.7433\n",
      "  Val Loss: 6.1423\n",
      "  Val Perplexity: 465.12\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 5/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 193:\n",
      "  Train Loss: 5.7417\n",
      "  Val Loss: 6.1428\n",
      "  Val Perplexity: 465.36\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 6/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194/300: 100%|██████████| 573/573 [00:18<00:00, 31.69it/s, loss=5.7415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 194:\n",
      "  Train Loss: 5.7415\n",
      "  Val Loss: 6.1429\n",
      "  Val Perplexity: 465.42\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 7/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195/300: 100%|██████████| 573/573 [00:18<00:00, 31.63it/s, loss=5.7412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 195:\n",
      "  Train Loss: 5.7412\n",
      "  Val Loss: 6.1441\n",
      "  Val Perplexity: 465.98\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 8/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/300: 100%|██████████| 573/573 [00:18<00:00, 31.48it/s, loss=5.7409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 196:\n",
      "  Train Loss: 5.7409\n",
      "  Val Loss: 6.1439\n",
      "  Val Perplexity: 465.87\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 9/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/300: 100%|██████████| 573/573 [00:17<00:00, 31.89it/s, loss=5.7396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 197:\n",
      "  Train Loss: 5.7396\n",
      "  Val Loss: 6.1431\n",
      "  Val Perplexity: 465.52\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 10/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198/300: 100%|██████████| 573/573 [00:17<00:00, 31.84it/s, loss=5.7395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 198:\n",
      "  Train Loss: 5.7395\n",
      "  Val Loss: 6.1446\n",
      "  Val Perplexity: 466.21\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 11/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199/300: 100%|██████████| 573/573 [00:18<00:00, 31.60it/s, loss=5.7387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 199:\n",
      "  Train Loss: 5.7387\n",
      "  Val Loss: 6.1432\n",
      "  Val Perplexity: 465.56\n",
      "  📊 Best model still at Epoch 187 (Val Loss: 6.1422)\n",
      "  ⏳ Epochs without improvement: 12/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200/300: 100%|██████████| 573/573 [00:17<00:00, 31.89it/s, loss=5.7380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 200:\n",
      "  Train Loss: 5.7380\n",
      "  Val Loss: 6.1416\n",
      "  Val Perplexity: 464.79\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201/300: 100%|██████████| 573/573 [00:18<00:00, 31.67it/s, loss=5.7381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 201:\n",
      "  Train Loss: 5.7381\n",
      "  Val Loss: 6.1422\n",
      "  Val Perplexity: 465.09\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202/300: 100%|██████████| 573/573 [00:17<00:00, 31.88it/s, loss=5.7375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 202:\n",
      "  Train Loss: 5.7375\n",
      "  Val Loss: 6.1451\n",
      "  Val Perplexity: 466.42\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 203/300: 100%|██████████| 573/573 [00:18<00:00, 31.75it/s, loss=5.7370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 203:\n",
      "  Train Loss: 5.7370\n",
      "  Val Loss: 6.1471\n",
      "  Val Perplexity: 467.34\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 3/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 204/300: 100%|██████████| 573/573 [00:18<00:00, 31.66it/s, loss=5.7367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 204:\n",
      "  Train Loss: 5.7367\n",
      "  Val Loss: 6.1443\n",
      "  Val Perplexity: 466.04\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 4/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=5.7361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 205:\n",
      "  Train Loss: 5.7361\n",
      "  Val Loss: 6.1429\n",
      "  Val Perplexity: 465.41\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 5/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 206/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 206:\n",
      "  Train Loss: 5.7354\n",
      "  Val Loss: 6.1435\n",
      "  Val Perplexity: 465.69\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 6/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 207/300: 100%|██████████| 573/573 [00:18<00:00, 31.80it/s, loss=5.7345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 207:\n",
      "  Train Loss: 5.7345\n",
      "  Val Loss: 6.1468\n",
      "  Val Perplexity: 467.23\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 7/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 208/300: 100%|██████████| 573/573 [00:18<00:00, 31.81it/s, loss=5.7349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 208:\n",
      "  Train Loss: 5.7349\n",
      "  Val Loss: 6.1477\n",
      "  Val Perplexity: 467.63\n",
      "  📊 Best model still at Epoch 200 (Val Loss: 6.1416)\n",
      "  ⏳ Epochs without improvement: 8/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 209/300: 100%|██████████| 573/573 [00:18<00:00, 31.77it/s, loss=5.7344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 209:\n",
      "  Train Loss: 5.7344\n",
      "  Val Loss: 6.1413\n",
      "  Val Perplexity: 464.64\n",
      "  ✅ Best model so far! Saved to 'mha_model_best.pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 210/300: 100%|██████████| 573/573 [00:18<00:00, 31.76it/s, loss=5.7336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 210:\n",
      "  Train Loss: 5.7336\n",
      "  Val Loss: 6.1447\n",
      "  Val Perplexity: 466.23\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 1/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 211/300: 100%|██████████| 573/573 [00:18<00:00, 31.79it/s, loss=5.7336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 211:\n",
      "  Train Loss: 5.7336\n",
      "  Val Loss: 6.1444\n",
      "  Val Perplexity: 466.09\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 2/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212/300: 100%|██████████| 573/573 [00:18<00:00, 31.83it/s, loss=5.7330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 212:\n",
      "  Train Loss: 5.7330\n",
      "  Val Loss: 6.1460\n",
      "  Val Perplexity: 466.87\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 3/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 213/300: 100%|██████████| 573/573 [00:18<00:00, 31.74it/s, loss=5.7324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 213:\n",
      "  Train Loss: 5.7324\n",
      "  Val Loss: 6.1460\n",
      "  Val Perplexity: 466.85\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 4/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 214/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.7320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 214:\n",
      "  Train Loss: 5.7320\n",
      "  Val Loss: 6.1431\n",
      "  Val Perplexity: 465.51\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 5/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 215/300: 100%|██████████| 573/573 [00:17<00:00, 31.96it/s, loss=5.7317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 215:\n",
      "  Train Loss: 5.7317\n",
      "  Val Loss: 6.1453\n",
      "  Val Perplexity: 466.51\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 6/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 216/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=5.7313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 216:\n",
      "  Train Loss: 5.7313\n",
      "  Val Loss: 6.1446\n",
      "  Val Perplexity: 466.19\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 7/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 217/300: 100%|██████████| 573/573 [00:17<00:00, 31.94it/s, loss=5.7304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 217:\n",
      "  Train Loss: 5.7304\n",
      "  Val Loss: 6.1447\n",
      "  Val Perplexity: 466.26\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 8/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 218/300: 100%|██████████| 573/573 [00:18<00:00, 31.72it/s, loss=5.7303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 218:\n",
      "  Train Loss: 5.7303\n",
      "  Val Loss: 6.1450\n",
      "  Val Perplexity: 466.39\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 9/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 219/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=5.7300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 219:\n",
      "  Train Loss: 5.7300\n",
      "  Val Loss: 6.1468\n",
      "  Val Perplexity: 467.22\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 10/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 220/300: 100%|██████████| 573/573 [00:17<00:00, 31.89it/s, loss=5.7296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 220:\n",
      "  Train Loss: 5.7296\n",
      "  Val Loss: 6.1459\n",
      "  Val Perplexity: 466.78\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 11/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 221/300: 100%|██████████| 573/573 [00:17<00:00, 31.92it/s, loss=5.7290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 221:\n",
      "  Train Loss: 5.7290\n",
      "  Val Loss: 6.1450\n",
      "  Val Perplexity: 466.40\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 12/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222/300: 100%|██████████| 573/573 [00:18<00:00, 31.70it/s, loss=5.7291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 222:\n",
      "  Train Loss: 5.7291\n",
      "  Val Loss: 6.1478\n",
      "  Val Perplexity: 467.69\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 13/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 223/300: 100%|██████████| 573/573 [00:18<00:00, 31.75it/s, loss=5.7287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 223:\n",
      "  Train Loss: 5.7287\n",
      "  Val Loss: 6.1467\n",
      "  Val Perplexity: 467.16\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 14/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 224/300: 100%|██████████| 573/573 [00:18<00:00, 31.65it/s, loss=5.7279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 224:\n",
      "  Train Loss: 5.7279\n",
      "  Val Loss: 6.1462\n",
      "  Val Perplexity: 466.96\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 15/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225/300: 100%|██████████| 573/573 [00:18<00:00, 31.75it/s, loss=5.7278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 225:\n",
      "  Train Loss: 5.7278\n",
      "  Val Loss: 6.1476\n",
      "  Val Perplexity: 467.61\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 16/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 226/300: 100%|██████████| 573/573 [00:18<00:00, 31.64it/s, loss=5.7275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 226:\n",
      "  Train Loss: 5.7275\n",
      "  Val Loss: 6.1478\n",
      "  Val Perplexity: 467.69\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 17/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 227/300: 100%|██████████| 573/573 [00:18<00:00, 31.71it/s, loss=5.7274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 227:\n",
      "  Train Loss: 5.7274\n",
      "  Val Loss: 6.1469\n",
      "  Val Perplexity: 467.27\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 18/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 228/300: 100%|██████████| 573/573 [00:18<00:00, 31.78it/s, loss=5.7272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 228:\n",
      "  Train Loss: 5.7272\n",
      "  Val Loss: 6.1468\n",
      "  Val Perplexity: 467.24\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 19/20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 229/300: 100%|██████████| 573/573 [00:18<00:00, 31.68it/s, loss=5.7263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 229:\n",
      "  Train Loss: 5.7263\n",
      "  Val Loss: 6.1492\n",
      "  Val Perplexity: 468.35\n",
      "  📊 Best model still at Epoch 209 (Val Loss: 6.1413)\n",
      "  ⏳ Epochs without improvement: 20/20\n",
      "\n",
      "🛑 Early stopping triggered! No improvement for 20 consecutive epochs.\n",
      "   Best model was at Epoch 209 with Val Loss: 6.1413\n",
      "======================================================================\n",
      "Training Complete!\n",
      "Best Epoch: 209\n",
      "Best Val Loss: 6.1413\n",
      "Best Val Perplexity: 464.64\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PLOTTING TRAINING METRICS\n",
      "======================================================================\n",
      "\n",
      "📊 Training metrics plot saved to: training_metrics.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING GENERATION\n",
      "======================================================================\n",
      "\n",
      "Prompt: The history of India is \n",
      "Output: The history of India is ˈ ( 1 @-@ a @-@ 000 @-@ 500 and the high on the early 19 , ) – 0 / Hip @-@ 15 @-@ 40 ; as well and\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: In mathematics,\n",
      "Output: In mathematics,@ 6 .= , and both music , but to order , the film called he was considered their work on the album in the the book has an acclaim . While a \" was one player was released\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Prompt: The cat sat on the\n",
      "Output: The cat sat on the stion to a woman , . The book is a new the mother are they is an first 's , the the episode and \" . A is not \" The \" with \" as an woman is\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Causal masking\n",
    "        batch_size, num_heads, seq_len, _ = scores.shape\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        mask_to_block = (causal_mask == 0)\n",
    "        mask_to_block = mask_to_block.unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask_to_block, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        tokens = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos = self.pos_embed(positions)\n",
    "\n",
    "        return tokens + pos\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = Embeddings(vocab_size, max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.lm_head.weight = self.embeddings.token_embed.weight\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embeddings(token_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ============================================\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "\n",
    "        for example in data:\n",
    "            text = example['text'].strip()\n",
    "            if len(text) > 0:\n",
    "                tokens = tokenizer.encode(text)\n",
    "                all_tokens.extend(tokens)\n",
    "\n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.tokens):,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.max_length\n",
    "        end = start + self.max_length\n",
    "\n",
    "        input_ids = self.tokens[start:end]\n",
    "        target_ids = self.tokens[start+1:end+1]\n",
    "\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity.item()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model_properly(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30, patience=20):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Track metrics history for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "\n",
    "    # Early stopping tracking\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, target_ids in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets_flat = target_ids.view(batch_size * seq_len)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            # Backward with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{total_loss/num_batches:.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_perplexity = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "        # Save best model and track early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, 'mha_model_best.pt')\n",
    "\n",
    "            print(f\"  Best model so far! Saved to 'mha_model_best.pt'\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"  Best model still at Epoch {best_epoch} (Val Loss: {best_val_loss:.4f})\")\n",
    "            print(f\"  Epochs without improvement: {epochs_without_improvement}/{patience}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping triggered! No improvement for {patience} consecutive epochs.\")\n",
    "            print(f\"   Best model was at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val Perplexity: {torch.exp(torch.tensor(best_val_loss)):.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text_proper(model, tokenizer, prompt, max_length=50, top_k=50, device='cuda'):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Top-k sampling\n",
    "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices[next_token_idx]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def plot_metrics(metrics_history, save_path='training_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot all training metrics including train loss, val loss, and val perplexity.\n",
    "\n",
    "    Args:\n",
    "        metrics_history: Dictionary containing training metrics\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    train_losses = metrics_history['train_losses']\n",
    "    val_losses = metrics_history['val_losses']\n",
    "    val_perplexities = metrics_history['val_perplexities']\n",
    "    best_epoch = metrics_history['best_epoch']\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training Metrics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[0, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Validation Loss\n",
    "    axes[0, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[0, 1].scatter([best_epoch], [metrics_history['best_val_loss']],\n",
    "                       color='r', s=100, zorder=5, label='Best Val Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Validation Loss over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Validation Perplexity\n",
    "    axes[1, 0].plot(epochs, val_perplexities, 'purple', linewidth=2, label='Val Perplexity')\n",
    "    axes[1, 0].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Validation Perplexity over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Train vs Validation Loss Comparison\n",
    "    axes[1, 1].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "    axes[1, 1].plot(epochs, val_losses, 'g-', linewidth=2, label='Val Loss')\n",
    "    axes[1, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Train vs Validation Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nTraining metrics plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_dataset = WikiTextDataset(dataset['train'], tokenizer, max_length=512)\n",
    "    val_dataset = WikiTextDataset(dataset['validation'], tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = LanguageModel(\n",
    "        vocab_size=50257,\n",
    "        max_seq_len=512,\n",
    "        d_model=16,\n",
    "        num_heads=2,\n",
    "        d_ff=64,\n",
    "        num_layers=2,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    metrics_history = train_model_properly(\n",
    "        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=2\n",
    "    )\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PLOTTING TRAINING METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    plot_metrics(metrics_history, save_path='training_metrics.png')\n",
    "\n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompts = [\n",
    "        \"The history of India is \",\n",
    "        \"In mathematics,\",\n",
    "        \"The cat sat on the\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        generated = generate_text_proper(model, tokenizer, prompt, max_length=40, device=device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Output: {generated}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ec5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
