{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ae69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers hf_transfer accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc195b",
   "metadata": {},
   "source": [
    "# Basic Library Support\n",
    "\n",
    "# NOTE: transformers<4.37.0 is required for qwen 2.5 series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144084c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "4.57.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a1ddc",
   "metadata": {},
   "source": [
    "# Download the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593e3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "#Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#Loading the model for a next token prediction task ideally suited for autocomplete functionality\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331d171",
   "metadata": {},
   "source": [
    "# Run Basic Inference to verify everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2760e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de84fb9",
   "metadata": {},
   "source": [
    "# Seeing the Top 5 Logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d6d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID  12095 | ' Paris' | 31.05%\n",
      "Token ID  32671 | ' ______' | 10.06%\n",
      "Token ID  30743 | ' ____' | 6.10%\n",
      "Token ID   1304 | ' __' | 5.40%\n",
      "Token ID    510 | ':\n",
      "' | 5.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "next_token_logits = outputs.logits[0, -1, :]  # logits for the last position\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "top5 = torch.topk(probs, 5)\n",
    "for prob, token_id in zip(top5.values, top5.indices):\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    print(f\"Token ID {token_id.item():6} | '{token_text}' | {prob.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad7d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ICD-10 code for Type 2 Diabetes is:\n",
      "A. 400.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The ICD-10 code for Type 2 Diabetes is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a4e8d",
   "metadata": {},
   "source": [
    "# Task: Can Qwen2.5-0.5B learn to write medical documentation through continued pretraining?\n",
    "\n",
    "Use Case: Medical Documentation Assistant for Low-Resource Settings\n",
    "- Doctors in rural clinics\n",
    "- Community health workers  \n",
    "- Offline capability (no internet needed)\n",
    "\n",
    "Dataset: PMC-Patients (167k patient summaries from PubMed Central)\n",
    "\n",
    "What the model should LEARN (not memorize):\n",
    "- Clinical writing patterns (\"Patient presents with...\", \"On examination...\")\n",
    "- Medical vocabulary in context\n",
    "- How to structure patient notes\n",
    "- How symptoms, findings, and diagnoses connect\n",
    "\n",
    "Before training: Generic autocomplete, doesn't know medical patterns\n",
    "After training: Completes clinical notes like a doctor would write\n",
    "\n",
    "Why 0.5B model:\n",
    "- Runs on laptop (~1GB memory)\n",
    "- Works offline\n",
    "- Fast inference\n",
    "- Suitable for low-resource deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e194cec",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba8f529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3894fed20249fe88468f6c2f52bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fe72b56618468b81fbe4e12754a4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augmented_notes_30K.jsonl:   0%|          | 0.00/372M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b58897e5a0400894e2836bc081b208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'note', 'full_note', 'conversation', 'summary'],\n",
      "        num_rows: 30000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This one has clinical notes and works reliably\n",
    "dataset = load_dataset(\"AGBonnet/augmented-clinical-notes\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c1a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a sixteen year-old girl, presented to our Outpatient department with the complaints of discomfort in the neck and lower back as well as restriction of body movements. She was not able to maintain an erect posture and would tend to fall on either side while standing up from a sitting position. She would keep her head turned to the right and upwards due to the sustained contraction of the neck muscles. There was a sideways bending of the back in the lumbar region. To counter the abnormal positioning of the back and neck, she would keep her limbs in a specific position to allow her body weight to be supported. Due to the restrictions with the body movements at the neck and in the lumbar region, she would require assistance in standing and walking. She would require her parents to help her with daily chores, including all activities of self-care.\n",
      "She had been experiencing these difficulties for the past four months since when she was introduced to olanzapine tablets for the control of her exacerbated mental illness. This was not her first experience with this drug over the past seven years since she had been diagnosed with bipolar affective disorder. Her first episode of the affective disorder was that of mania at the age of eleven which was managed with the use of olanzapine tablets in 2.5–10 mg doses per day at different times. The patient developed pain and discomfort in her neck within the second week of being put on tablet olanzapine at a dose of 5 mg per day. This was associated with a sustained and abnormal contraction of the neck muscles that would pull her head to the right in an upward direction. These features had persisted for the first three years of her illness with a varying intensity, distress, and dysfunction which would tend to correlate with the dose of olanzapine. Apart from a brief period of around three weeks when she was given tablet trihexyphenidyl 4 mg per day for rigidity in her upper limbs, she was not prescribed any other psychotropic medication. The rigidity showed good response to this medication which was subsequently\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e8bcdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notes: 30000\n",
      "Average length: 2050 characters\n",
      "Shortest: 1689\n",
      "Longest: 2445\n"
     ]
    }
   ],
   "source": [
    "notes = dataset['train']['note']\n",
    "lengths = [len(n) for n in notes]\n",
    "print(f\"Total notes: {len(notes)}\")\n",
    "print(f\"Average length: {sum(lengths)//len(lengths)} characters\")\n",
    "print(f\"Shortest: {min(lengths)}\")\n",
    "print(f\"Longest: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069ec49",
   "metadata": {},
   "source": [
    "# Before Continued Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1734fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The patient was started on tablet trihexyphenidyl\n",
      "Output: The patient was started on tablet trihexyphenidyl (Artane) 10 mg twice daily for 10 days. The patient was then\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Diagnosis: Bipolar affective disorder, current episode\n",
      "Output: Diagnosis: Bipolar affective disorder, current episode of depression, or other mood disorder.\n",
      "Treatment: Antidepressants, mood stabilizers, and/or\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: On examination, the patient showed signs of dystonia with\n",
      "Output: On examination, the patient showed signs of dystonia with a positive Babinski sign. The most likely diagnosis is\n",
      "A. Parkinson's disease\n",
      "B.\n",
      "--------------------------------------------------\n",
      "Prompt: The drug was tapered gradually and replaced with\n",
      "Output: The drug was tapered gradually and replaced with a new drug, which was effective in 90% of patients. The new drug was effective\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The patient was started on tablet trihexyphenidyl\",\n",
    "    \"Diagnosis: Bipolar affective disorder, current episode\",\n",
    "    \"On examination, the patient showed signs of dystonia with\",\n",
    "    \"The drug was tapered gradually and replaced with\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458721f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This note has 417 tokens\n",
      "Note length in characters: 2082\n"
     ]
    }
   ],
   "source": [
    "# Check how many tokens our notes actually have\n",
    "sample_note = dataset['train'][0]['note']\n",
    "tokens = tokenizer(sample_note)\n",
    "print(f\"This note has {len(tokens['input_ids'])} tokens\")\n",
    "print(f\"Note length in characters: {len(sample_note)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac47c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1093.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min tokens: 379\n",
      "Max tokens: 516\n",
      "Average tokens: 441\n",
      "Notes over 512 tokens: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# How many tokens does each note have?\n",
    "from tqdm import tqdm\n",
    "\n",
    "token_counts = []\n",
    "for note in tqdm(dataset['train']['note'][:1000]):  # Check first 1000\n",
    "    tokens = tokenizer(note)\n",
    "    token_counts.append(len(tokens['input_ids']))\n",
    "\n",
    "print(f\"Min tokens: {min(token_counts)}\")\n",
    "print(f\"Max tokens: {max(token_counts)}\")\n",
    "print(f\"Average tokens: {sum(token_counts)//len(token_counts)}\")\n",
    "print(f\"Notes over 512 tokens: {sum(1 for t in token_counts if t > 512)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fc27f",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c8a407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2472ac35db5b4f0089208d3831e28c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 30000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(examples):\n",
    "    return tokenizer(\n",
    "        examples['note'],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset['train'].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ae078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
